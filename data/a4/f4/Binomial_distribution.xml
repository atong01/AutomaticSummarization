<article title='Binomial_distribution'><paragraph><template><target>Redirect</target><arg>Binomial model</arg><arg>the binomial model in options pricing</arg><arg>Binomial options pricing model</arg></template></paragraph><preblock><preline><template><target>see also</target><arg>Negative binomial distribution</arg></template></preline></preblock><paragraph><template><target>Probability distribution</target><arg name="name"><space/>binomial
<space/></arg><arg name="type"><space/>mass
<space/></arg><arg name="pdf_image"><space/>[[File:Binomial distribution pmf.svg|300px|Probability mass function for the binomial distribution]]
<space/></arg><arg name="cdf_image"><space/>[[File:Binomial distribution cdf.svg|300px|Cumulative distribution function for the binomial distribution]]
<space/></arg><arg name="notation"><space/>''B''(''n'',&thinsp;''p'')
<space/></arg><arg name="parameters"><space/>''n'' ∈ [[Natural numbers|'''N'''<sub>0</sub>]] — number of trials<br />''p'' ∈ [0,1] — success probability in each trial
<space/></arg><arg name="support"><space/>''k'' ∈ {&thinsp;0, …, ''n''&thinsp;} — number of successes
<space/></arg><arg name="pdf"><space/><math>\textstyle {n \choose k}\, p^k (1-p)^{n-k}</math>
<space/></arg><arg name="cdf"><space/><math>\textstyle I_{1-p}(n - k, 1 + k)</math>
<space/></arg><arg name="mean"><space/><math>np</math>
<space/></arg><arg name="median"><space/><math>\lfloor np \rfloor</math><space/>or<space/><math>\lceil np \rceil</math>
<space/></arg><arg name="mode"><space/><math>\lfloor (n + 1)p \rfloor</math><space/>or<space/><math>\lfloor (n + 1)p \rfloor - 1</math>
<space/></arg><arg name="variance"><space/><math>np(1 - p)</math>
<space/></arg><arg name="skewness"><space/><math>\frac{1-2p}{\sqrt{np(1-p)}}</math>
<space/></arg><arg name="kurtosis"><space/><math>\frac{1-6p(1-p)}{np(1-p)}</math>
<space/></arg><arg name="entropy"><space/><math>\frac12 \log_2 \big( 2\pi e\, np(1-p) \big) + O \left( \frac{1}{n} \right)</math><br/><space/>in [[Shannon (unit)|shannons]]. For [[nat (unit)|nats]], use the natural log, and omit the factor of<space/><math>e</math><space/>in the log.
<space/></arg><arg name="mgf"><space/><math>(1-p + pe^t)^n \!</math>
<space/></arg><arg name="char"><space/><math>(1-p + pe^{it})^n \!</math>
<space/></arg><arg name="pgf"><space/><math>G(z)<space/></arg><arg name="fisher"><space/><math><space/>g_n(p)<space/></arg></template><link><target>File:Pascal's triangle; binomial distribution.svg</target><part>thumb</part><part>340px</part><part>Binomial distribution for<space/><extension extension_name='math'>p=0.5</extension><xhtml:br></xhtml:br>with ''n'' and ''k'' as in<space/><link><target>Pascal's triangle</target></link><xhtml:br></xhtml:br><xhtml:br></xhtml:br>The probability that a ball in a<space/><link><target>Bean machine</target><part>Galton box</part></link><space/>with 8 layers (''n'' = 8) ends up in the central bin (''k'' = 4) is<space/><extension extension_name='math'>70/256</extension>.</part></link>In<space/><link><target>probability theory</target></link><space/>and<space/><link><target>statistics</target></link>, the<space/><bold>binomial distribution</bold><space/>with parameters<space/><italics>n</italics><space/>and<space/><italics>p</italics><space/>is the<space/><link><target>discrete probability distribution</target></link><space/>of the number of successes in a sequence of<space/><italics>n</italics><space/><link><target>statistical independence</target><part>independent</part></link><space/>yes/no experiments, each of which yields success with<space/><link><target>probability</target></link><space/><italics>p</italics>.A success/failure experiment is also called a Bernoulli experiment or<space/><link><target>Bernoulli trial</target></link>; when<space/><italics>n</italics><space/>= 1, the binomial distribution is a<space/><link><target>Bernoulli distribution</target></link>. The binomial distribution is the basis for the popular<space/><link><target>binomial test</target></link><space/>of<space/><link><target>statistical significance</target></link>.</paragraph><paragraph>The binomial distribution is frequently used to model the number of successes in a sample of size<space/><italics>n</italics><space/>drawn<space/><link><target>Sampling (statistics)#Replacement of selected units</target><part>with replacement</part></link><space/>from a population of size<space/><italics>N.</italics><space/>If the sampling is carried out without replacement, the draws are not independent and so the resulting distribution is a<space/><link><target>hypergeometric distribution</target></link>, not a binomial one. However, for<space/><italics>N</italics><space/>much larger than<space/><italics>n</italics>, the binomial distribution is a good approximation, and widely used.</paragraph><heading level='2'>Specification</heading><heading level='3'>Probability mass function</heading><paragraph>In general, if the random variable<space/><italics>X</italics><space/>follows the binomial distribution with parameters<space/><italics>n</italics><space/>and<space/><italics>p</italics>, we write<space/><italics>X</italics>&amp;nbsp;~&amp;nbsp;B(<italics>n</italics>,&amp;nbsp;<italics>p</italics>). The probability of getting exactly<space/><italics>k</italics><space/>successes in<space/><italics>n</italics><space/>trials is given by the<space/><link><target>probability mass function</target></link>:</paragraph><list type='ident'><listitem><extension extension_name='math'><space/>f(k;n,p) = \Pr(X = k) = \binom n k p^k(1-p)^{n-k}</extension></listitem></list><paragraph>for<space/><italics>k</italics>&amp;nbsp;=&amp;nbsp;0,&amp;nbsp;1,&amp;nbsp;2,&amp;nbsp;...,&amp;nbsp;<italics>n</italics>, where</paragraph><list type='ident'><listitem><extension extension_name='math'>\binom n k =\frac{n!}{k!(n-k)!}</extension></listitem></list><paragraph>is the<space/><link><target>binomial coefficient</target></link>, hence the name of the distribution. The formula can be understood as follows: we want exactly<space/><italics>k</italics><space/>successes (<italics>p</italics><xhtml:sup><italics>k</italics></xhtml:sup>) and<space/><italics>n</italics>&amp;nbsp;&amp;nbsp;<italics>k</italics><space/>failures (1&amp;nbsp;&amp;nbsp;<italics>p</italics>)<xhtml:sup><italics>n</italics>&amp;nbsp;&amp;nbsp;<italics>k</italics></xhtml:sup>. However, the<space/><italics>k</italics><space/>successes can occur anywhere among the<space/><italics>n</italics><space/>trials, and there are<space/><extension extension_name='math'>{n\choose k}</extension><space/>different ways of distributing<space/><italics>k</italics><space/>successes in a sequence of<space/><italics>n</italics><space/>trials.</paragraph><paragraph>In creating reference tables for binomial distribution probability, usually the table is filled in up to<space/><italics>n</italics>/2 values. This is because for<space/><italics>k</italics>&amp;nbsp;&gt;&amp;nbsp;<italics>n</italics>/2, the probability can be calculated by its complement as</paragraph><list type='ident'><listitem><extension extension_name='math'>f(k,n,p)=f(n-k,n,1-p).<space/></extension></listitem></list><paragraph>Looking at the expression<space/><italics></italics>(<italics>k</italics>,&amp;nbsp;<italics>n</italics>,&amp;nbsp;<italics>p</italics>) as a function of<space/><italics>k</italics>, there is a<space/><italics>k</italics><space/>value that maximizes it. This<space/><italics>k</italics><space/>value can be found by calculating</paragraph><list type='ident'><listitem><extension extension_name='math'><space/>\frac{f(k+1,n,p)}{f(k,n,p)}=\frac{(n-k)p}{(k+1)(1-p)}<space/></extension></listitem></list><paragraph>and comparing it to 1. There is always an integer<space/><italics>M</italics><space/>that satisfies</paragraph><list type='ident'><listitem>&lt;math&gt;(n+1)p-1 \leq M &lt; (n+1)p.&lt;/math&gt;</listitem></list><paragraph><italics></italics>(<italics>k</italics>,&amp;nbsp;<italics>n</italics>,&amp;nbsp;<italics>p</italics>) is monotone increasing for<space/><italics>k</italics>&amp;nbsp;&lt;&amp;nbsp;<italics>M</italics><space/>and monotone decreasing for<space/><italics>k</italics>&amp;nbsp;&gt;&amp;nbsp;<italics>M</italics>, with the exception of the case where (<italics>n</italics>&amp;nbsp;+&amp;nbsp;1)<italics>p</italics><space/>is an integer. In this case, there are two values for which<space/><italics></italics><space/>is maximal: (<italics>n</italics>&amp;nbsp;+&amp;nbsp;1)<italics>p</italics><space/>and (<italics>n</italics>&amp;nbsp;+&amp;nbsp;1)<italics>p</italics>&amp;nbsp;&amp;nbsp;1.<space/><italics>M</italics><space/>is the<space/><italics>most probable</italics><space/>(<italics>most likely</italics>) outcome of the Bernoulli trials and is called the<space/><link><target>Mode (statistics)</target><part>mode</part></link>. Note that the probability of it occurring can be fairly small.</paragraph><paragraph>The following<space/><link><target>recurrence relation</target></link><space/>holds:</paragraph><list type='ident'><listitem><extension extension_name='math'>\left\{\begin{array}{l}
p (n-k) \operatorname{Prob}(k)+(k+1) (p-1)
 \operatorname{Prob}(k+1)=0, \\[10pt]
\operatorname{Prob}(0)=(1-p)^n
\end{array}\right\}</extension></listitem></list><heading level='3'>Cumulative distribution function</heading><paragraph>The<space/><link><target>cumulative distribution function</target></link><space/>can be expressed as:</paragraph><list type='ident'><listitem><extension extension_name='math'>F(k;n,p) = \Pr(X \le k) = \sum_{i=0}^{\lfloor k \rfloor} {n\choose i}p^i(1-p)^{n-i}</extension></listitem></list><paragraph>where<space/><extension extension_name='math'>\scriptstyle \lfloor k\rfloor\,</extension><space/>is the &quot;floor&quot; under<space/><italics>k</italics>, i.e. the<space/><link><target>greatest integer</target></link><space/>less than or equal to<space/><italics>k</italics>.</paragraph><paragraph>It can also be represented in terms of the<space/><link><target>regularized incomplete beta function</target></link>, as follows:<extension extension_name='ref'><template><target>cite book</target><arg name="last">Wadsworth</arg><arg name="first">G. P.</arg><arg name="title">Introduction to probability and random variables</arg><arg name="year">1960</arg><arg name="publisher">McGraw-Hill New York</arg><arg name="location">USA</arg><arg name="page">52</arg></template></extension></paragraph><list type='ident'><listitem><extension extension_name='math'>\begin{align}
F(k;n,p) &amp; = \Pr(X \le k) \\
&amp;= I_{1-p}(n-k, k+1) \\
&amp; = (n-k) {n \choose k} \int_0^{1-p} t^{n-k-1} (1-t)^k \, dt.
\end{align}</extension></listitem></list><paragraph>Some closed-form bounds for the cumulative distribution function are given<space/><link><target>#Tail Bounds</target><part>below</part></link>.</paragraph><heading level='2'>Example</heading><paragraph>Suppose a<space/><link><target>fair coin</target><part>biased coin</part></link><space/>comes up heads with probability 0.3 when tossed. What is the probability of achieving 0, 1,..., 6 heads after six tosses?</paragraph><list type='ident'><listitem><extension extension_name='math'>\Pr(0\text{ heads}) = f(0) = \Pr(X = 0) = {6\choose 0}0.3^0 (1-0.3)^{6-0} \approx 0.1176<space/></extension></listitem><listitem><extension extension_name='math'>\Pr(1\text{ heads}) = f(1) = \Pr(X = 1) = {6\choose 1}0.3^1 (1-0.3)^{6-1} \approx 0.3025<space/></extension></listitem><listitem><extension extension_name='math'>\Pr(2\text{ heads}) = f(2) = \Pr(X = 2) = {6\choose 2}0.3^2 (1-0.3)^{6-2} \approx 0.3241<space/></extension></listitem><listitem><extension extension_name='math'>\Pr(3\text{ heads}) = f(3) = \Pr(X = 3) = {6\choose 3}0.3^3 (1-0.3)^{6-3} \approx 0.1852</extension></listitem><listitem><extension extension_name='math'>\Pr(4\text{ heads}) = f(4) = \Pr(X = 4) = {6\choose 4}0.3^4 (1-0.3)^{6-4} \approx 0.0595</extension></listitem><listitem><extension extension_name='math'>\Pr(5\text{ heads}) = f(5) = \Pr(X = 5) = {6\choose 5}0.3^5 (1-0.3)^{6-5} \approx 0.0102<space/></extension></listitem><listitem><extension extension_name='math'>\Pr(6\text{ heads}) = f(6) = \Pr(X = 6) = {6\choose 6}0.3^6 (1-0.3)^{6-6} \approx 0.0007</extension><extension extension_name='ref'>Hamilton Institute.<space/><link type='external' href='http://www.hamilton.ie/ollie/EE304/Binom.pdf'>&quot;The Binomial Distribution&quot;</link><space/>October 20, 2010.</extension></listitem></list><heading level='2'>Mean and variance</heading><paragraph>If<space/><italics>X</italics><space/>~<space/><italics>B</italics>(<italics>n</italics>,<space/><italics>p</italics>), that is,<space/><italics>X</italics><space/>is a binomially distributed random variable, n being the total number of experiments and p the probability of each experiment yielding a successful result, then the<space/><link><target>expected value</target></link><space/>of<space/><italics>X</italics><space/>is:<extension extension_name='ref'>See<space/><link type='external' href='https://proofwiki.org/wiki/Expectation_of_Binomial_Distribution'>Proof Wiki</link></extension></paragraph><list type='ident'><listitem><extension extension_name='math'><space/>\operatorname{E}[X] = np ,<space/></extension></listitem></list><paragraph><italics>(For example, if n=100, and p=1/4, then the average number of successful results will be 25)</italics></paragraph><paragraph>The<space/><link><target>variance</target></link><space/>is:</paragraph><list type='ident'><listitem><extension extension_name='math'><space/>\operatorname{Var}[X] = np(1 - p).</extension></listitem></list><heading level='2'>Mode and median</heading><paragraph>Usually the<space/><link><target>mode (statistics)</target><part>mode</part></link><space/>of a binomial<space/><italics>B</italics>(<italics>n</italics>,&amp;thinsp;<italics>p</italics>) distribution is equal to<space/><extension extension_name='math'>\lfloor (n+1)p\rfloor</extension>, where<space/><extension extension_name='math'>\lfloor\cdot\rfloor</extension><space/>is the<space/><link><target>floor function</target></link>. However when (<italics>n</italics>&amp;nbsp;+&amp;nbsp;1)<italics>p</italics><space/>is an integer and<space/><italics>p</italics><space/>is neither 0 nor 1, then the distribution has two modes: (<italics>n</italics>&amp;nbsp;+&amp;nbsp;1)<italics>p</italics><space/>and (<italics>n</italics>&amp;nbsp;+&amp;nbsp;1)<italics>p</italics>&amp;nbsp;&amp;nbsp;1. When<space/><italics>p</italics><space/>is equal to 0 or 1, the mode will be 0 and<space/><italics>n</italics><space/>correspondingly. These cases can be summarized as follows:</paragraph><list type='ident'><listitem><extension extension_name='math'>\text{mode} =
 \begin{cases}
 \lfloor (n+1)\,p\rfloor &amp; \text{if }(n+1)p\text{ is 0 or a noninteger}, \\
 (n+1)\,p\ \text{ and }\ (n+1)\,p - 1 &amp;\text{if }(n+1)p\in\{1,\dots,n\}, \\
 n &amp; \text{if }(n+1)p = n + 1.
 \end{cases}</extension></listitem></list><paragraph>In general, there is no single formula to find the<space/><link><target>median</target></link><space/>for a binomial distribution, and it may even be non-unique. However several special results have been established:</paragraph><list type='bullet'><listitem>If<space/><italics>np</italics><space/>is an integer, then the mean, median, and mode coincide and equal<space/><italics>np</italics>.<extension extension_name='ref'><template><target>cite journal</target><arg name="last">Neumann</arg><arg name="first">P.</arg><arg name="year">1966</arg><arg name="title">Über den Median der Binomial- and Poissonverteilung</arg><arg name="journal">Wissenschaftliche Zeitschrift der Technischen Universität Dresden</arg><arg name="volume">19</arg><arg name="pages">29–33</arg><arg name="language">German</arg></template></extension><extension extension_name='ref'>Lord, Nick. (July 2010). &quot;Binomial averages when the mean is an integer&quot;,<space/><link><target>The Mathematical Gazette</target></link><space/>94, 331-332.</extension></listitem><listitem>Any median<space/><italics>m</italics><space/>must lie within the interval<space/><italics>np</italics>&amp;nbsp;&amp;nbsp;<italics>m</italics>&amp;nbsp;&amp;nbsp;<italics>np</italics>.<extension extension_name='ref' name="KaasBuhrman"><template><target>cite journal</target><arg name="first1">R.</arg><arg name="last1">Kaas</arg><arg name="first2">J.M.</arg><arg name="last2">Buhrman</arg><arg name="title">Mean, Median and Mode in Binomial Distributions</arg><arg name="journal">Statistica Neerlandica</arg><arg name="year">1980</arg><arg name="volume">34</arg><arg name="issue">1</arg><arg name="pages">13–18</arg><arg name="doi">10.1111/j.1467-9574.1980.tb00681.x</arg></template></extension></listitem><listitem>A median<space/><italics>m</italics><space/>cannot lie too far away from the mean:<space/><template><target>nowrap</target><arg>&#124;''m'' − ''np''&#124; ≤ min{&thinsp;ln 2, max{''p'', 1 − ''p''}&thinsp;</arg></template>}.<extension extension_name='ref' name="Hamza"><template><target>Cite journal</target><arg name="last1"><space/>Hamza<space/></arg><arg name="first1"><space/>K. 
</arg><arg name="doi"><space/>10.1016/0167-7152(94)00090-U 
</arg><arg name="title"><space/>The smallest uniform upper bound on the distance between the mean and the median of the binomial and Poisson distributions 
</arg><arg name="journal"><space/>Statistics & Probability Letters 
</arg><arg name="volume"><space/>23 
</arg><arg name="pages"><space/>21–25 
</arg><arg name="year"><space/>1995 
</arg><arg name="pmid"><space/>
</arg><arg name="pmc"><space/>
</arg></template></extension></listitem><listitem>The median is unique and equal to<space/><italics>m</italics>&amp;nbsp;=&amp;nbsp;<link><target>Rounding</target><part>round</part></link>(<italics>np</italics>) in cases when either<space/><template><target>nowrap</target><arg>''p'' ≤ 1 − ln 2</arg></template><space/>or<space/><template><target>nowrap</target><arg>''p'' ≥ ln 2</arg></template><space/>or |<italics>m</italics>&amp;nbsp;&amp;nbsp;<italics>np</italics>|&amp;nbsp;&amp;nbsp;min{<italics>p</italics>,&amp;nbsp;1&amp;nbsp;&amp;nbsp;<italics>p</italics>} (except for the case when<space/><italics>p</italics>&amp;nbsp;=&amp;nbsp; and<space/><italics>n</italics><space/>is odd).<extension extension_name='ref' name="KaasBuhrman"></extension><extension extension_name='ref' name="Hamza"></extension></listitem><listitem>When<space/><italics>p</italics>&amp;nbsp;=&amp;nbsp;1/2 and<space/><italics>n</italics><space/>is odd, any number<space/><italics>m</italics><space/>in the interval (<italics>n</italics>&amp;nbsp;&amp;nbsp;1)&amp;nbsp;&amp;nbsp;<italics>m</italics>&amp;nbsp;&amp;nbsp;(<italics>n</italics>&amp;nbsp;+&amp;nbsp;1) is a median of the binomial distribution. If<space/><italics>p</italics>&amp;nbsp;=&amp;nbsp;1/2 and<space/><italics>n</italics><space/>is even, then<space/><italics>m</italics>&amp;nbsp;=&amp;nbsp;<italics>n</italics>/2 is the unique median.</listitem></list><heading level='2'>Covariance between two binomials</heading><paragraph>If two binomially distributed random variables<space/><italics>X</italics><space/>and<space/><italics>Y</italics><space/>are observed together, estimating their covariance can be useful. Using the definition of<space/><link><target>covariance</target></link>, in the case<space/><italics>n</italics>&amp;nbsp;=&amp;nbsp;1 (thus being<space/><link><target>Bernoulli trial</target><trail>s</trail></link>) we have</paragraph><list type='ident'><listitem><extension extension_name='math'>\operatorname{Cov}(X, Y) = \operatorname{E}(XY) - \mu_X \mu_Y.</extension></listitem></list><paragraph>The first term is non-zero only when both<space/><italics>X</italics><space/>and<space/><italics>Y</italics><space/>are one, and<space/><italics></italics><xhtml:sub><italics>X</italics></xhtml:sub><space/>and<space/><italics></italics><xhtml:sub><italics>Y</italics></xhtml:sub><space/>are equal to the two probabilities. Defining<space/><italics>p</italics><xhtml:sub><italics>B</italics></xhtml:sub><space/>as the probability of both happening at the same time, this gives</paragraph><list type='ident'><listitem><extension extension_name='math'>\operatorname{Cov}(X, Y) = p_B - p_X p_Y,</extension></listitem></list><paragraph>and for<space/><italics>n</italics><space/>independent pairwise trials</paragraph><list type='ident'><listitem><extension extension_name='math'>\operatorname{Cov}(X, Y)_n = n ( p_B - p_X p_Y ).</extension></listitem></list><paragraph>If<space/><italics>X</italics><space/>and<space/><italics>Y</italics><space/>are the same variable, this reduces to the variance formula given above.</paragraph><heading level='2'>Related distributions</heading><heading level='3'>Sums of binomials</heading><paragraph>If<space/><italics>X</italics>&amp;nbsp;~&amp;nbsp;B(<italics>n</italics>,&amp;nbsp;<italics>p</italics>) and<space/><italics>Y</italics>&amp;nbsp;~&amp;nbsp;B(<italics>m</italics>,&amp;nbsp;<italics>p</italics>) are independent binomial variables with the same probability<space/><italics>p</italics>, then<space/><italics>X</italics>&amp;nbsp;+&amp;nbsp;<italics>Y</italics><space/>is again a binomial variable; its distribution is<template><target>citation needed</target><arg name="date">May 2012</arg></template><space/><italics>X+Y</italics>&amp;nbsp;~&amp;nbsp;B(<italics>n+m</italics>,&amp;nbsp;<italics>p</italics>).</paragraph><paragraph>However, if<space/><italics>X</italics><space/>and<space/><italics>Y</italics><space/>do not have the same probability<space/><italics>p</italics>, then the variance of the sum will be<space/><link><target>Binomial sum variance inequality</target><part>smaller than the variance of a binomial variable</part></link><space/>distributed as<space/><extension extension_name='math'>B(n+m, \bar{p}).\,</extension></paragraph><heading level='3'>Conditional binomials</heading><paragraph>If<space/><italics>X</italics>&amp;nbsp;~&amp;nbsp;B(<italics>n</italics>,&amp;nbsp;<italics>p</italics>) and, conditional on<space/><italics>X</italics>,<space/><italics>Y</italics>&amp;nbsp;~&amp;nbsp;B(<italics>X</italics>,&amp;nbsp;<italics>q</italics>), then<space/><italics>Y</italics><space/>is a simple binomial variable with distribution<template><target>citation needed</target><arg name="date">May 2012</arg></template></paragraph><list type='ident'><listitem><extension extension_name='math'>Y \sim B(n, pq).</extension></listitem></list><paragraph>For example imagine throwing<space/><italics>n</italics><space/>balls to a basket<space/><italics>U<xhtml:sub>X</xhtml:sub></italics><space/>and taking the balls that hit and throwing them to another basket<space/><italics>U<xhtml:sub>Y</xhtml:sub></italics>. If<space/><italics>p</italics><space/>is the probability to hit<space/><italics>U<xhtml:sub>X</xhtml:sub></italics><space/>then<space/><italics>X</italics>&amp;nbsp;~&amp;nbsp;B(<italics>n</italics>,&amp;nbsp;<italics>p</italics>) is the number of balls that hit<space/><italics>U<xhtml:sub>X</xhtml:sub></italics>. If<space/><italics>q</italics><space/>is the probability to hit<space/><italics>U<xhtml:sub>Y</xhtml:sub></italics><space/>then the number of balls that hit<space/><italics>U<xhtml:sub>Y</xhtml:sub></italics><space/>is<space/><italics>Y</italics>&amp;nbsp;~&amp;nbsp;B(<italics>X</italics>,&amp;nbsp;<italics>q</italics>) and therefore<space/><italics>Y</italics>&amp;nbsp;~&amp;nbsp;B(<italics>n</italics>,&amp;nbsp;<italics>pq</italics>).</paragraph><heading level='3'>Bernoulli distribution</heading><paragraph>The<space/><link><target>Bernoulli distribution</target></link><space/>is a special case of the binomial distribution, where<space/><italics>n</italics>&amp;nbsp;=&amp;nbsp;1. Symbolically,<space/><italics>X</italics>&amp;nbsp;~&amp;nbsp;B(1,&amp;nbsp;<italics>p</italics>) has the same meaning as<space/><italics>X</italics>&amp;nbsp;~&amp;nbsp;Bern(<italics>p</italics>). Conversely, any binomial distribution, B(<italics>n</italics>,&amp;nbsp;<italics>p</italics>), is the distribution of the sum of<space/><italics>n</italics><space/><link><target>Bernoulli trials</target></link>, Bern(<italics>p</italics>), each with the same probability<space/><italics>p</italics>.<template><target>citation needed</target><arg name="date">May 2012</arg></template></paragraph><heading level='3'>Poisson binomial distribution</heading><paragraph>The binomial distribution is a special case of the<space/><link><target>Poisson binomial distribution</target></link>, which is a sum of<space/><italics>n</italics><space/>independent non-identical<space/><link><target>Bernoulli trials</target></link><space/>Bern(<italics>p<xhtml:sub>i</xhtml:sub></italics>).<extension extension_name='ref'><template><target>Cite journal</target><arg name="volume"><space/>3
</arg><arg name="issue"><space/>2
</arg><arg name="pages"><space/>295–312
</arg><arg name="last"><space/>Wang
</arg><arg name="first"><space/>Y. H.
</arg><arg name="title"><space/>On the number of successes in independent trials
</arg><arg name="journal"><space/>Statistica Sinica
</arg><arg name="year"><space/>1993
</arg><arg name="url"><space/>http://www3.stat.sinica.edu.tw/statistica/oldpdf/A3n23.pdf
</arg></template></extension><space/>If<space/><italics>X</italics><space/>has the Poisson binomial distribution with<space/><italics>p<xhtml:sub>1</xhtml:sub></italics>&amp;nbsp;=&amp;nbsp;&amp;nbsp;=&amp;nbsp;<italics>p<xhtml:sub>n</xhtml:sub></italics>&amp;nbsp;=<italics>p</italics><space/>then<space/><italics>X</italics>&amp;nbsp;~&amp;nbsp;B(<italics>n</italics>,&amp;nbsp;<italics>p</italics>).</paragraph><heading level='3'>Normal approximation</heading><paragraph><link><target>File:Binomial Distribution.svg</target><part>right</part><part>250px</part><part>thumb</part><part>Binomial<space/><link><target>probability mass function</target></link><space/>and normal<space/><link><target>probability density function</target></link><space/>approximation for ''n''&amp;nbsp;=&amp;nbsp;6 and ''p''&amp;nbsp;=&amp;nbsp;0.5</part></link></paragraph><paragraph>If<space/><italics>n</italics><space/>is large enough, then the skew of the distribution is not too great. In this case a reasonable approximation to B(<italics>n</italics>,&amp;nbsp;<italics>p</italics>) is given by the<space/><link><target>normal distribution</target></link></paragraph><list type='ident'><listitem><extension extension_name='math'><space/>\mathcal{N}(np,\, np(1-p)),</extension></listitem></list><paragraph>and this basic approximation can be improved in a simple way by using a suitable<space/><link><target>continuity correction</target></link>.The basic approximation generally improves as<space/><italics>n</italics><space/>increases (at least 20) and is better when<space/><italics>p</italics><space/>is not near to 0 or 1.<extension extension_name='ref' name="bhh"><template><target>cite book</target><arg name="title">Statistics for experimenters</arg><arg name="author">Box, Hunter and Hunter</arg><arg name="publisher">Wiley</arg><arg name="year">1978</arg><arg name="page">130</arg></template></extension><space/>Various<space/><link><target>Rule of thumb</target><part>rules of thumb</part></link><space/>may be used to decide whether<space/><italics>n</italics><space/>is large enough, and<space/><italics>p</italics><space/>is far enough from the extremes of zero or one:</paragraph><list type='bullet'><listitem>One rule is that both<space/><italics>x=np</italics><space/>and<space/><italics>n</italics>(1&amp;nbsp;&amp;nbsp;<italics>p</italics>) must be greater than&amp;nbsp;5. However, the specific number varies from source to source, and depends on how good an approximation one wants; some sources give 10 which gives virtually the same results as the following rule for large<space/><italics>n</italics><space/>until<space/><italics>n</italics><space/>is very large (ex:<space/><italics>x=11, n=7752</italics>).</listitem></list><list type='bullet'><listitem>A second rule<extension extension_name='ref' name="bhh"></extension><space/>is that for<space/><template><target>nowrap</target><arg>''n'' ><space/>5</arg></template><space/>the normal approximation is adequate if</listitem></list><list type='ident'><listitem><list type='ident'><listitem>&lt;math&gt;\left | \left (\frac{1}{\sqrt{n}} \right ) \left (\sqrt{\frac{1-p}{p}}-\sqrt{\frac{p}{1-p}} \right ) \right |&lt;0.3&lt;/math&gt;</listitem></list></listitem></list><list type='bullet'><listitem>Another commonly used rule holds that the normal approximation is appropriate only if everything within 3 standard deviations of its mean is within the range of possible values,<template><target>Citation needed</target><arg name="date">August 2011</arg></template><space/>that is if</listitem></list><list type='ident'><listitem><list type='ident'><listitem><extension extension_name='math'>\mu \pm 3 \sigma = np \pm 3 \sqrt{np(1-p)} \in [0,n].</extension></listitem></list></listitem></list><paragraph>The following is an example of applying a<space/><link><target>continuity correction</target></link>. Suppose one wishes to calculate Pr(<italics>X</italics>&amp;nbsp;&amp;nbsp;8) for a binomial random variable<space/><italics>X</italics>. If<space/><italics>Y</italics><space/>has a distribution given by the normal approximation, then Pr(<italics>X</italics>&amp;nbsp;&amp;nbsp;8) is approximated by Pr(<italics>Y</italics>&amp;nbsp;&amp;nbsp;8.5). The addition of 0.5 is the continuity correction; the uncorrected normal approximation gives considerably less accurate results.</paragraph><paragraph>This approximation, known as<space/><link><target>de MoivreLaplace theorem</target></link>, is a huge time-saver when undertaking calculations by hand (exact calculations with large<space/><italics>n</italics><space/>are very onerous); historically, it was the first use of the normal distribution, introduced in<space/><link><target>Abraham de Moivre</target></link>'s book<space/><italics><link><target>The Doctrine of Chances</target></link></italics><space/>in 1738. Nowadays, it can be seen as a consequence of the<space/><link><target>central limit theorem</target></link><space/>since B(<italics>n</italics>,&amp;nbsp;<italics>p</italics>) is a sum of<space/><italics>n</italics><space/>independent, identically distributed<space/><link><target>Bernoulli distribution</target><part>Bernoulli variables</part></link><space/>with parameter&amp;nbsp;<italics>p</italics>. This fact is the basis of a<space/><link><target>hypothesis test</target></link>, a &quot;proportion z-test&quot;, for the value of<space/><italics>p</italics><space/>using<space/><italics>x/n</italics>, the sample proportion and estimator of<space/><italics>p</italics>, in a<space/><link><target>common test statistics</target><part>common test statistic</part></link>.<extension extension_name='ref'><link><target>NIST</target></link>/<link><target>SEMATECH</target></link>,<space/><link type='external' href='http://www.itl.nist.gov/div898/handbook/prc/section2/prc24.htm'>&quot;7.2.4. Does the proportion of defectives meet requirements?&quot;</link><space/><italics>e-Handbook of Statistical Methods.</italics></extension></paragraph><paragraph>For example, suppose one randomly samples<space/><italics>n</italics><space/>people out of a large population and ask them whether they agree with a certain statement. The proportion of people who agree will of course depend on the sample. If groups of<space/><italics>n</italics><space/>people were sampled repeatedly and truly randomly, the proportions would follow an approximate normal distribution with mean equal to the true proportion<space/><italics>p</italics><space/>of agreement in the population and with standard deviation &amp;nbsp;=&amp;nbsp;(<italics>p</italics>(1&amp;nbsp;&amp;nbsp;<italics>p</italics>)/<italics>n</italics>)<xhtml:sup>1/2</xhtml:sup>.</paragraph><heading level='3'>Poisson approximation</heading><paragraph>The binomial distribution converges towards the<space/><link><target>Poisson distribution</target></link><space/>as the number of trials goes to infinity while the product<space/><italics>np</italics><space/>remains fixed. Therefore the Poisson distribution with parameter<space/><italics></italics><space/>=<space/><italics>np</italics><space/>can be used as an approximation to B(<italics>n</italics>,<space/><italics>p</italics>) of the binomial distribution if<space/><italics>n</italics><space/>is sufficiently large and<space/><italics>p</italics><space/>is sufficiently small. According to two rules of thumb, this approximation is good if<space/><italics>n</italics>&amp;nbsp;&amp;nbsp;20 and<space/><italics>p</italics>&amp;nbsp;&amp;nbsp;0.05, or if<space/><italics>n</italics>&amp;nbsp;&amp;nbsp;100 and<space/><italics>np</italics>&amp;nbsp;&amp;nbsp;10.<extension extension_name='ref' name='nist'><link><target>NIST</target></link>/<link><target>SEMATECH</target></link>,<space/><link type='external' href='http://www.itl.nist.gov/div898/handbook/pmc/section3/pmc331.htm'>&quot;6.3.3.1. Counts Control Charts&quot;</link>,<space/><italics>e-Handbook of Statistical Methods.</italics></extension></paragraph><heading level='3'>Limiting distributions</heading><list type='bullet'><listitem><italics><link><target>Poisson limit theorem</target></link></italics>: As<space/><italics>n</italics><space/>approaches and<space/><italics>p</italics><space/>approaches 0 while<space/><italics>np</italics><space/>remains fixed at &amp;nbsp;&gt;&amp;nbsp;0 or at least<space/><italics>np</italics><space/>approaches &amp;nbsp;&gt;&amp;nbsp;0, then the Binomial(<italics>n</italics>,&amp;nbsp;<italics>p</italics>) distribution approaches the<space/><link><target>Poisson distribution</target></link><space/>with<space/><link><target>expected value</target></link><space/>.<extension extension_name='ref' name='nist'></extension></listitem></list><list type='bullet'><listitem><italics><link><target>de MoivreLaplace theorem</target></link></italics>: As<space/><italics>n</italics><space/>approaches while<space/><italics>p</italics><space/>remains fixed, the distribution of</listitem></list><list type='ident'><listitem><list type='ident'><listitem><extension extension_name='math'>\frac{X-np}{\sqrt{np(1-p)}}</extension></listitem></list></listitem></list><list type='ident'><listitem>approaches the<space/><link><target>normal distribution</target></link><space/>with expected value&amp;nbsp;0 and<space/><link><target>variance</target></link>&amp;nbsp;1.<template><target>citation needed</target><arg name="date">May 2012</arg></template><space/>This result is sometimes loosely stated by saying that the distribution of<space/><italics>X</italics><space/>is<space/><link><target>Asymptotic normality</target><part>asymptotically normal</part></link><space/>with expected value&amp;nbsp;<italics>np</italics><space/>and<space/><link><target>variance</target></link>&amp;nbsp;<italics>np</italics>(1&amp;nbsp;&amp;nbsp;<italics>p</italics>). This result is a specific case of the<space/><link><target>central limit theorem</target></link>.</listitem></list><heading level='3'>Beta distribution</heading><paragraph><link><target>Beta distribution</target><trail>s</trail></link><space/>provide a family of<space/><link><target>conjugate prior distribution</target><part>conjugate prior probability distribution</part><trail>s</trail></link><space/>for binomial distributions in<space/><link><target>Bayesian inference</target></link>. The domain of the beta distribution can be viewed as a probability, and in fact the beta distribution is often used to describe the distribution of a probability value<space/><italics>p</italics>:<extension extension_name='ref' name="MacKay"><template><target>cite book</target><arg name="last">MacKay</arg><arg name="first">David</arg><arg name="title"><space/>Information Theory, Inference and Learning Algorithms</arg><arg name="year">2003</arg><arg name="publisher">Cambridge University Press; First Edition<space/></arg><arg name="isbn">978-0521642989</arg></template></extension><space/></paragraph><list type='ident'><listitem><extension extension_name='math'>P(p;\alpha,\beta) = \frac{p^{\alpha-1}(1-p)^{\beta-1}}{\mathrm{B}(\alpha,\beta)}</extension>.</listitem></list><heading level='2'>Confidence intervals</heading><paragraph><template><target>main</target><arg>Binomial proportion confidence interval</arg></template></paragraph><paragraph>Even for quite large values of<space/><italics>n</italics>, the actual distribution of the mean is significantly nonnormal.<extension extension_name='ref' name="Brown2001"><template><target>Citation</target><arg name="first1">Lawrence D.<space/></arg><arg name="last1">Brown<space/></arg><arg name="first2">T. Tony<space/></arg><arg name="last2">Cai<space/></arg><arg name="first3">Anirban<space/></arg><arg name="last3">DasGupta<space/></arg><arg name="year">2001<space/></arg><arg name="title">Interval Estimation for a Binomial Proportion<space/></arg><arg name="url">http://www-stat.wharton.upenn.edu/~tcai/paper/html/Binomial-StatSci.html<space/></arg><arg name="journal">Statistical Science<space/></arg><arg name="volume">16<space/></arg><arg name="issue">2<space/></arg><arg name="pages">101–133<space/></arg><arg name="accessdate">2015-01-05<space/></arg><arg name="doi">10.1214/ss/1009213286</arg></template></extension><space/>Because of this problem several methods to estimate confidence intervals have been proposed.</paragraph><paragraph>Let<space/><italics>n</italics><xhtml:sub>1</xhtml:sub><space/>be the number of successes out of<space/><italics>n</italics>, the total number of trials, and let<space/></paragraph><list type='ident'><listitem><extension extension_name='math'><space/>\hat{p} = \frac{n_1}{n}</extension></listitem></list><paragraph>be the proportion of successes. Let<space/><italics>z</italics><xhtml:sub>/2</xhtml:sub><space/>be the 100(1 /2)th percentile of the<space/><link><target>standard normal distribution</target></link>.</paragraph><list type='bullet'><listitem>Wald method</listitem></list><list type='ident'><listitem><list type='ident'><listitem><extension extension_name='math'><space/>\hat{p} \pm z_{\frac{\alpha}{2}} \sqrt{ \frac{ \hat{p} ( 1 -\hat{p} )}{ n } } .</extension></listitem></list></listitem></list><list type='ident'><listitem>A<space/><link><target>continuity correction</target></link><space/>of 0.5/<italics>n</italics><space/>may be added.<template><target>clarify</target><arg name="date">July 2012</arg></template></listitem></list><list type='bullet'><listitem>Agresti-Coull method<extension extension_name='ref' name="Agresti1988"><template><target>Citation</target><arg name="last1">Agresti<space/></arg><arg name="first1">Alan<space/></arg><arg name="last2">Coull<space/></arg><arg name="first2">Brent A.<space/></arg><arg name="date">May 1998<space/></arg><arg name="title">Approximate is better than 'exact' for interval estimation of binomial proportions<space/></arg><arg name="url">http://www.stat.ufl.edu/~aa/articles/agresti_coull_1998.pdf<space/></arg><arg name="journal">The American Statistician<space/></arg><arg name="volume">52<space/></arg><arg name="issue">2<space/></arg><arg name="pages">119–126<space/></arg><arg name="accessdate">2015-01-05<space/></arg><arg name="doi">10.2307/2685469</arg></template></extension></listitem></list><list type='ident'><listitem><list type='ident'><listitem><extension extension_name='math'><space/>\tilde{p} \pm z_{\frac{\alpha}{2}} \sqrt{ \frac{ \tilde{p} ( 1 - \tilde{p} )}{ n + z_{\frac{\alpha}{2}}^2 } } .</extension></listitem></list></listitem></list><list type='ident'><listitem>Here the estimate of<space/><italics>p</italics><space/>is modified to</listitem></list><list type='ident'><listitem><list type='ident'><listitem><extension extension_name='math'><space/>\tilde{p}= \frac{ n_1 + \frac{1}{2} z_{\frac{\alpha}{2}}^2}{ n + z_{\frac{\alpha}{2}}^2 }<space/></extension></listitem></list></listitem></list><list type='bullet'><listitem>ArcSine method<extension extension_name='ref' name="Pires00">Pires MA<space/><link type='external' href='http://www.math.ist.utl.pt/~apires/PDFs/AP_COMPSTAT02.pdf'>Confidence intervals for a binomial proportion: comparison of methods and software evaluation.</link></extension></listitem></list><list type='ident'><listitem><list type='ident'><listitem><extension extension_name='math'>\sin^2 \left (\arcsin \left ( \sqrt{ \hat{p} } \right ) \pm \frac{ z }{ 2 \sqrt{ n } } \right )<space/></extension></listitem></list></listitem></list><list type='bullet'><listitem>Wilson (score) method<extension extension_name='ref' name="Wilson1927"><template><target>Citation</target><arg name="last">Wilson<space/></arg><arg name="first">Edwin B.<space/></arg><arg name="date">June 1927<space/></arg><arg name="title">Probable inference, the law of succession, and statistical inference<space/></arg><arg name="url">http://psych.stanford.edu/~jlm/pdfs/Wison27SingleProportion.pdf<space/></arg><arg name="journal">J. American Statistical Association<space/></arg><arg name="volume">22<space/></arg><arg name="issue">158<space/></arg><arg name="pages">209–212<space/></arg><arg name="accessdate">2015-01-05<space/></arg><arg name="doi">10.2307/2276774</arg></template></extension></listitem></list><list type='ident'><listitem><list type='ident'><listitem><extension extension_name='math'><space/>\frac{\hat{p} + \frac{1}{2n} z_{1-\frac{\alpha}{2}}^2 \pm \frac{1}{2n} z_{1-\frac{\alpha}{2}} \sqrt{4n\hat{p}(1 - \hat{p})+ z_{1-\frac{\alpha}{2}}^2}} {1+ \frac{1}{n} z_{1-\frac{\alpha}{2}}^2}.</extension></listitem></list></listitem></list><paragraph>The exact (<link><target>Binomial_proportion_confidence_interval#Clopper-Pearson_interval</target><part>Clopper-Pearson</part></link>) method is the most conservative.<extension extension_name='ref' name="Brown2001"></extension><space/>The Wald method although commonly recommended in the text books is the most biased.<template><target>clarify</target><arg name="reason">what sense of bias is this</arg><arg name="date">July 2012</arg></template></paragraph><heading level='2'>Generating binomial random variates</heading><paragraph>Methods for<space/><link><target>random number generation</target></link><space/>where the<space/><link><target>marginal distribution</target></link><space/>is a binomial distribution are well-established.<extension extension_name='ref'>Devroye, Luc (1986)<space/><italics>Non-Uniform Random Variate Generation</italics>, New York: Springer-Verlag. (See especially<space/><link type='external' href='http://luc.devroye.org/chapter_ten.pdf'>Chapter X, Discrete Univariate Distributions</link>)</extension><extension extension_name='ref'><template><target>Cite journal</target><arg name="pages"><space/>216–222 
</arg><arg name="year"><space/>1988 
</arg><arg name="doi"><space/>10.1145/42372.42381
</arg><arg name="last2"><space/>Schmeiser</arg><arg name="first1"><space/>V. 
</arg><arg name="volume"><space/>31</arg><arg name="first2"><space/>B. W. 
</arg><arg name="journal"><space/>Communications of the ACM 
</arg><arg name="title"><space/>Binomial random variate generation
</arg><arg name="last1"><space/>Kachitvichyanukul</arg><arg name="issue"><space/>2
</arg></template></extension></paragraph><paragraph>One way to generate random samples from a binomial distribution is to use an inversion algorithm. To do so, one must calculate the probability that P(X=k) for all values<space/><italics>k</italics><space/>from 0 through<space/><italics>n</italics>. (These probabilities should sum to a value close to one, in order to encompass the entire sample space.) Then by using a<space/><link><target>linear congruential generator</target></link><space/>to generate samples uniform between 0 and 1, one can transform the calculated samples U[0,1] into discrete numbers by using the probabilities calculated in step one.</paragraph><heading level='2'>Tail Bounds</heading><paragraph>For<space/><italics>k</italics><space/><italics>np</italics>,<space/><link><target>Chernoff bound</target><part>upper bounds</part></link><space/>for the lower tail of the distribution function can be derived. In particular,<space/><link><target>Hoeffding's inequality</target></link><space/>yields the bound</paragraph><list type='ident'><listitem><extension extension_name='math'><space/>F(k;n,p) \leq \exp\left(-2 \frac{(np-k)^2}{n}\right), \!</extension></listitem></list><paragraph>and<space/><link><target>Chernoff's inequality</target></link><space/>can be used to derive the bound</paragraph><list type='ident'><listitem><extension extension_name='math'><space/>F(k;n,p) \leq \exp\left(-\frac{1}{2\,p} \frac{(np-k)^2}{n}\right). \!</extension></listitem></list><paragraph>Moreover, these bounds are reasonably tight when<space/><italics>p = 1/2</italics>, since the following expression holds for all<space/><italics>k</italics><space/><italics>3n/8</italics><extension extension_name='ref'>Matouek, J, Vondrak, J:<space/><italics>The Probabilistic Method</italics><space/>(lecture notes)<space/><link type='external' href='http://kam.mff.cuni.cz/~matousek/prob-ln.ps.gz'></link>.</extension></paragraph><list type='ident'><listitem><extension extension_name='math'><space/>F(k;n,\tfrac{1}{2}) \geq \frac{1}{15} \exp\left(- \frac{16 (\frac{n}{2} - k)^2}{n}\right). \!</extension></listitem></list><paragraph>However, the bounds do not work well for extreme values of<space/><italics>p</italics>. In particular, as<space/><italics>p</italics><space/><extension extension_name='math'>\rightarrow</extension><space/><italics>1</italics>, value<space/><italics>F(k;n,p)</italics><space/>goes to zero (for fixed<space/><italics>k</italics>,<space/><italics>n</italics><space/>with<space/><italics>k&lt;n</italics>)while the upper bound above goes to a positive constant. In this case a better bound is given by<extension extension_name='ref' name="ag">R. Arratia and L. Gordon:<space/><italics>Tutorial on large deviations for the binomial distribution</italics>, Bulletin of Mathematical Biology 51(1) (1989), 125131<space/><link type='external' href='http://link.springer.com/article/10.1007%2FBF02458840'></link>.</extension></paragraph><list type='ident'><listitem>&lt;math&gt; F(k;n,p) \leq \exp\left(-nD\left(\frac{k}{n}\left|\right|p\right)\right) \quad\quad\mbox{if }0&lt;\frac{k}{n}&lt;p\!&lt;/math&gt;</listitem></list><paragraph>where<space/><italics>D(a|| p)</italics><space/>is the<space/><link><target>KullbackLeibler divergence</target><part>relative entropy</part></link><space/>between an<space/><italics>a</italics>-coin and a<space/><italics>p</italics>-coin (i.e. between the Bernoulli(a) and Bernoulli(p) distribution):</paragraph><list type='ident'><listitem><extension extension_name='math'><space/>D(a||p)=(a)\log\frac{a}{p}+(1-a)\log\frac{1-a}{1-p}. \!</extension></listitem></list><paragraph>Asymptotically, this bound is reasonably tight; see<extension extension_name='ref' name="ag"></extension><space/>for details. An equivalent formulation of the bound is</paragraph><list type='ident'><listitem>&lt;math&gt; \Pr(X \ge k) =F(n-k;n,1-p)\leq \exp\left(-nD\left(\frac{k}{n}\left|\right|p\right)\right) \quad\quad\mbox{if }p&lt;\frac{k}{n}&lt;1.\!&lt;/math&gt;</listitem></list><paragraph>Both these bounds are derived directly from the<space/><link><target>Chernoff bound</target></link>.It can also be shown that,</paragraph><list type='ident'><listitem>&lt;math&gt; \Pr(X \ge k) =F(n-k;n,1-p)\geq \frac{1}{(n+1)^2} \exp\left(-nD\left(\frac{k}{n}\left|\right|p\right)\right) \quad\quad\mbox{if }p&lt;\frac{k}{n}&lt;1.\!&lt;/math&gt;</listitem></list><paragraph>This is proved using the method of types (see for example chapter 12 of Elements of Information Theory by Cover and Thomas<space/><extension extension_name='ref' name="ct"><preblock><preline>T. Cover and J. Thomas, &quot;Elements of Information Theory, 2nd Edition&quot;, Wiley 2006</preline></preblock></extension>).</paragraph><heading level='2'>See also</heading><paragraph><template><target>Portal</target><arg>Statistics</arg></template></paragraph><list type='bullet'><listitem><link><target>Logistic regression</target></link></listitem><listitem><link><target>Multinomial distribution</target></link></listitem><listitem><link><target>Negative binomial distribution</target></link></listitem><listitem>Binomial measure, an example of a<space/><link><target>Multifractal system</target><part>multifractal</part></link><space/><link><target>measure (mathematics)</target><part>measure</part></link>.<extension extension_name='ref'>Mandelbrot, B. B., Fisher, A. J., &amp; Calvet, L. E. (1997). A multifractal model of asset returns.<space/><italics>3.2 The Binomial Measure is the Simplest Example of a Multifractal</italics></extension></listitem></list><heading level='2'>References</heading><paragraph><extension extension_name='references'></extension></paragraph><heading level='2'>External links</heading><list type='bullet'><listitem>Interactive graphic:<space/><link type='external' href='http://www.math.wm.edu/~leemis/chart/UDR/UDR.html'>Univariate Distribution Relationships</link></listitem><listitem><link type='external' href='http://www.fxsolver.com/browse/formulas/Binomial+distribution'>Binomial distribution formula calculator</link></listitem><listitem><link type='external' href='http://www.mbastats.net/Content/Basic_Prob/Binomial.html'>Binomial distribution calculator</link></listitem><listitem>Difference of two binomial variables:<space/><link type='external' href='http://math.stackexchange.com/questions/1065487/difference-between-two-independent-binomial-random-variables-with-equal-success'>X-Y</link><space/>or<space/><link type='external' href='http://math.stackexchange.com/questions/562119/difference-of-two-binomial-random-variables'>|X-Y|</link></listitem></list><paragraph><template><target>ProbDistributions</target><arg>discrete-finite</arg></template><template><target>Common univariate probability distributions</target></template></paragraph><paragraph><template><target>DEFAULTSORT:Binomial Distribution</target></template><link><target>Category:Discrete distributions</target></link><link><target>Category:Factorial and binomial topics</target></link><link><target>Category:Distributions with conjugate priors</target></link><link><target>Category:Exponential family distributions</target></link><link><target>Category:Probability distributions</target></link></paragraph></article>