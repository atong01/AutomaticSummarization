<article title='Amdahl%27s_law'><paragraph><link><target>Image:AmdahlsLaw.svg</target><part>thumb</part><part>407x407px</part><part>The speedup of a program using multiple processors in parallel computing is limited by the sequential fraction of the program. For example, if 95% of the program can be parallelized, the theoretical maximum speedup using parallel computing would be 20 as shown in the diagram.</part></link></paragraph><paragraph><bold>Amdahl's law</bold>, also known as<space/><bold>Amdahl's argument</bold>,<extension extension_name='ref'><template><target>harv</target><arg>Rodgers</arg><arg>1985</arg><arg name="p">226</arg></template></extension><space/>is used to find the maximum expected improvement to an overall system when only part of the system is improved. It is often used in<space/><link><target>parallel computing</target></link><space/>to predict the theoretical maximum<space/><link><target>speedup</target></link><space/>using multiple processors. The law is named after<space/><link><target>Computer architecture</target><part>computer architect</part></link><space/><link><target>Gene Amdahl</target></link>, and was presented at the<space/><link><target>American Federation of Information Processing Societies</target><part>AFIPS</part></link><space/>Spring Joint Computer Conference in 1967.</paragraph><paragraph>The speedup of a program using multiple processors in parallel computing is limited by the time needed for the sequential fraction of the program. For example, if a program needs 20 hours using a single processor core, and a particular portion of the program which takes one hour to execute cannot be parallelized, while the remaining 19 hours (95%) of execution time can be parallelized, then regardless of how many processors are devoted to a parallelized execution of this program, the minimum execution time cannot be less than that critical one hour. Hence, the theoretical speedup is limited to at most 20.</paragraph><heading level='2'>Definition</heading><paragraph>Given:</paragraph><list type='bullet'><listitem><extension extension_name='math'>n \in \mathbb{N}</extension>, the number of<space/><link><target>thread of execution</target><part>threads of execution</part></link>,</listitem><listitem><extension extension_name='math'>B\in [0, 1]</extension>, the fraction of the algorithm that is strictly serial,</listitem></list><paragraph>The time<space/><extension extension_name='math'>T \left(n \right)</extension><space/>an algorithm takes to finish when being executed on<space/><extension extension_name='math'>n</extension><space/>thread(s) of execution corresponds to:</paragraph><paragraph><extension extension_name='math'>T(n) = T(1) \left(B + \frac{1}{n}\left(1 - B\right)\right)</extension></paragraph><paragraph>Therefore, the theoretical speedup<space/><extension extension_name='math'>S(n)</extension><space/>that can be had by executing a given algorithm on a system capable of executing<space/><extension extension_name='math'>n</extension><space/>threads of execution is:</paragraph><paragraph><extension extension_name='math'>S(n) = \frac{ T\left(1\right)}{T\left(n\right)} = \frac{T\left(1\right)}{T\left(1\right)\left(B + \frac{1}{n}\left(1 - B\right)\right) } = \frac{1}{B + \frac{1}{n}\left(1-B\right)}</extension></paragraph><heading level='2'>Description</heading><paragraph>Amdahl's law is a model for the expected speedup and the relationship between parallelized implementations of an algorithm and its sequential implementations, under the assumption that the problem size remains the same when parallelized. For example, if for a given problem size a parallelized implementation of an algorithm can run 12% of the algorithm's operations arbitrarily quickly (while the remaining 88% of the operations are not parallelizable), Amdahl's law states that the maximum speedup of the parallelized version is<space/><template><target>nowrap</target><arg name="1">1/(1 – 0.12)<space/></arg></template><space/>times as fast as the non-parallelized implementation.</paragraph><paragraph>More technically, the law is concerned with the speedup achievable from an improvement to a computation that affects a proportion<space/><italics>P</italics><space/>of that computation where the improvement has a speedup of<space/><italics>S</italics>. (For example, if 30% of the computation may be the subject of a speed up,<space/><italics>P</italics><space/>will be 0.3; if the improvement makes the portion affected twice as fast,<space/><italics>S</italics><space/>will be 2.) Amdahl's law states that the overall speedup of applying the improvement will be:</paragraph><list type='ident'><listitem><extension extension_name='math'>\frac{1}{(1 - P) + \frac{P}{S}} = \frac{1}{(1 - 0.3) + \frac{0.3}{2}} = 1.1765</extension></listitem></list><paragraph>To see how this formula was derived, assume that the running time of the old computation was 1, for some unit of time. The running time of the new computation will be the length of time the unimproved fraction takes (which is 1 &amp;minus;<space/><italics>P</italics>), plus the length of time the improved fraction takes. The length of time for the improved part of the computation is the length of the improved part's former running time divided by the speedup, making the length of time of the improved part<space/><italics>P</italics>/<italics>S</italics>. The final speedup is computed by dividing the old running time by the new running time, which is what the above formula does.</paragraph><paragraph>Here's another example. We are given a sequential task which is split into four consecutive parts: P1, P2, P3 and P4 with the percentages of runtime being 11%, 18%, 23% and 48% respectively. Then we are told that P1 is not sped up, so S1 = 1, while P2 is sped up 5, P3 is sped up 20, and P4 is sped up 1.6. By using the formulaP1/S1 + P2/S2 + P3/S3 + P4/S4, we find the new sequential running time is:</paragraph><list type='ident'><listitem><extension extension_name='math'>\frac{0.11}{1} + \frac{0.18}{5} + \frac{0.23}{20} + \frac{0.48}{1.6} = 0.4575.</extension></listitem></list><paragraph>or a little less than<space/><template><target>frac</target><arg>1</arg><arg>2</arg></template><space/>the original running time. Using the formula<space/><template><target>nowrap</target><arg>(P1/S1 + P2/S2 + P3/S3 + P4/S4)<sup>−1</sup></arg></template>, the overall speed boost is 1 / 0.4575 = 2.186, or a little more than double the original speed. Notice how the 20 and 5 speedup don't have much effect on the overall speed when P1 (11%) is not sped up, and P4 (48%) is sped up only 1.6 times.</paragraph><heading level='2'>Parallelization</heading><paragraph>In the case of parallelization, Amdahl's law states that if<space/><italics>P</italics><space/>is the proportion of a program that can be made parallel (i.e., benefit from parallelization), and (1<space/><italics>P</italics>) is the proportion that cannot be parallelized (remains serial), then the maximum speedup that can be achieved by using<space/><italics>N</italics><space/>processors is</paragraph><list type='ident'><listitem><extension extension_name='math'>S(N) = \frac{1}{(1-P) + \frac{P}{N}}</extension>.</listitem></list><paragraph>In the limit, as<space/><italics>N</italics><space/>tends to<space/><link><target>Extended real number line</target><part>infinity</part></link>, the maximum speedup tends to 1 / (1<space/><italics>P</italics>). In practice, performance to price ratio falls rapidly as<space/><italics>N</italics><space/>is increased once there is even a small component of (1<space/><italics>P</italics>).</paragraph><paragraph>As an example, if<space/><italics>P</italics><space/>is 90%, then (1<space/><italics>P</italics>) is 10%, and the problem can be sped up by a maximum of a factor of 10, no matter how large the value of<space/><italics>N</italics><space/>used. For this reason, parallel computing is only useful for either small numbers of<space/><link><target>central processing unit</target><part>processor</part><trail>s</trail></link>, or problems with very high values of<space/><italics>P</italics>: so-called<space/><link><target>embarrassingly parallel</target></link><space/>problems. A great part of the craft of<space/><link><target>parallel programming</target></link><space/>consists of attempting to reduce the component (1<space/><italics>P</italics>) to the smallest possible value.</paragraph><paragraph><italics>P</italics><space/>can be estimated by using the measured speedup (<italics>SU</italics>) on a specific number of processors (<italics>NP</italics>) using</paragraph><list type='ident'><listitem><extension extension_name='math'>P_\text{estimated} = \frac{\frac{1}{SU} - 1}{\frac{1}{NP} - 1}</extension>.</listitem></list><paragraph><italics>P</italics><space/>estimated in this way can then be used in Amdahl's law to predict speedup for a different number of processors.</paragraph><heading level='2'>Relation to law of diminishing returns</heading><paragraph>Amdahl's law is often conflated with the<space/><link><target>Diminishing returns</target><part>law of diminishing returns</part></link>, whereas only a special case of applying Amdahl's law demonstrates 'law of diminishing returns'. If one picks optimally (in terms of the achieved speed-up) what to improve, then one will see monotonically decreasing improvements as one improves. If, however, one picks non-optimally, after improving a sub-optimal component and moving on to improve a more optimal component, one can see an increase in return. Note that it is often rational to improve a system in an order that is &quot;non-optimal&quot; in this sense, given that some improvements are more difficult or consuming of development time than others.</paragraph><paragraph>Amdahl's law does represent the law of diminishing returns if you are considering what sort of return you get by adding more processors to a machine, if you are running a fixed-size computation that will use all available processors to their capacity. Each new processor you add to the system will add less usable power than the previous one. Each time you double the number of processors the speedup ratio will diminish, as the total throughput heads toward the limit of<space/><extension extension_name='math'>\scriptstyle 1 / (1 \,-\, P)</extension>.</paragraph><paragraph>This analysis neglects other potential bottlenecks such as<space/><link><target>memory bandwidth</target></link><space/>and I/O bandwidth, if they do not scale with the number of processors; however, taking into account such bottlenecks would tend to further demonstrate the diminishing returns of only adding processors.</paragraph><heading level='2'>Speedup in a sequential program</heading><paragraph><link><target>Image:Optimizing-different-parts.svg</target><part>thumb</part><part>400px</part><part>Assume that a task has two independent parts, A and B. B takes roughly 25% of the time of the whole computation. By working very hard, one may be able to make this part 5 times faster, but this only reduces the time for the whole computation by a little. In contrast, one may need to perform less work to make part A be twice as fast. This will make the computation much faster than by optimizing part B, even though B's speed-up is greater by ratio, (5 versus 2).</part></link>The maximum speedup in an improved sequential program, where some part was sped up<space/><extension extension_name='math'>p</extension><space/>times is limited by inequality</paragraph><list type='ident'><listitem><extension extension_name='math'>\text{maximum speedup } \le \frac{p}{1 + f \cdot (p - 1)}</extension></listitem></list><paragraph>where<space/><extension extension_name='math'>\scriptstyle f</extension><space/>(<extension extension_name='math'>\scriptstyle 0 \;&lt;\; f \;&lt;\; 1</extension>) is the fraction of time (before the improvement) spent in the part that was not improved. For example, (see picture on right):</paragraph><list type='bullet'><listitem>If part B is made five times faster (<extension extension_name='math'>\scriptstyle p \;=\; 5</extension>),<space/><extension extension_name='math'>\scriptstyle t_A \;=\; 3</extension>,<space/><extension extension_name='math'>\scriptstyle t_B \;=\; 1</extension>, and<space/><extension extension_name='math'>\scriptstyle f \;=\; t_A / (t_A \,+\, t_B) \;=\; 0.75</extension>, then<list type='ident'><listitem><extension extension_name='math'>\text{maximum speedup } \le \frac{5}{1 + 0.75 \cdot (5 - 1)} = 1.25</extension></listitem></list></listitem><listitem>If part A is made to run twice as fast (<extension extension_name='math'>\scriptstyle p \;=\; 2</extension>),<space/><extension extension_name='math'>\scriptstyle t_B \;=\; 1</extension>,<space/><extension extension_name='math'>\scriptstyle t_A \;=\; 3</extension>, and<space/><extension extension_name='math'>\scriptstyle f \;=\; t_B / (t_A \,+\, t_B) \;=\; 0.25</extension>, then<list type='ident'><listitem><extension extension_name='math'>\text{maximum speedup } \le \frac{2}{1 + 0.25 \cdot (2 - 1)} = 1.60</extension></listitem></list></listitem></list><paragraph>Therefore, making A twice as fast is better than making B five times faster. The percentage improvement in speed can be calculated as</paragraph><list type='ident'><listitem><extension extension_name='math'>\text{percentage improvement} = \left(1 - \frac{1}{\text{speedup factor}}\right) \cdot 100</extension></listitem></list><list type='bullet'><listitem>Improving part A by a factor of two will increase overall program speed by a factor of 1.6, which makes it 37.5% faster than the original computation.</listitem><listitem>However, improving part B by a factor of five, which presumably requires more effort, will only achieve an overall speedup factor of 1.25, which makes it 20% faster.</listitem></list><heading level='2'>Limitations</heading><paragraph>Amdahl's law only applies to cases where the problem size is fixed. In practice, as more computing resources become available, they tend to get used on larger problems (larger datasets), and the time spent in the parallelizable part often grows much faster than the inherently sequential work. In this case,<space/><link><target>Gustafson's law</target></link><space/>gives a more realistic assessment of parallel performance.<extension extension_name='ref' name="spp"><template><target>cite book</target><arg name="author1">Michael McCool<space/></arg><arg name="author2">James Reinders<space/></arg><arg name="author3">Arch Robison<space/></arg><arg name="title">Structured Parallel Programming: Patterns for Efficient Computation<space/></arg><arg name="publisher">Elsevier<space/></arg><arg name="year">2013<space/></arg><arg name="pages">61</arg></template></extension></paragraph><heading level='2'>See also</heading><list type='bullet'><listitem><link><target>Critical path method</target></link></listitem><listitem><link><target>KarpFlatt metric</target></link></listitem><listitem><link><target>Moore's law</target></link></listitem></list><heading level='2'>Notes</heading><paragraph><template><target>Reflist</target></template></paragraph><heading level='2'>References</heading><list type='bullet'><listitem><template><target>Cite journal</target><arg name="last"><space/>Rodgers<space/></arg><arg name="first"><space/>David P.<space/></arg><arg name="date">June 1985<space/></arg><arg name="url"><space/>http://portal.acm.org/citation.cfm?id</arg><arg name="title"><space/>Improvements in multiprocessor system design<space/></arg><arg name="journal"><space/>ACM SIGARCH Computer Architecture News archive<space/></arg><arg name="volume"><space/>13<space/></arg><arg name="issue"><space/>3<space/></arg><arg name="pages"><space/>225–231<space/></arg><arg name="issn"><space/>0163-5964<space/></arg><arg name="publisher"><space/>[[Association for Computing Machinery|ACM]]<space/></arg><arg name="location"><space/>New York, NY, USA<space/></arg><arg name="doi"><space/>10.1145/327070.327215</arg><arg name="ref">harv</arg><arg name="isbn">0-8186-0634-7</arg></template></listitem></list><heading level='2'>Further reading</heading><list type='bullet'><listitem><template><target>Cite journal</target><arg name="first"><space/>Gene M.<space/></arg><arg name="last"><space/>Amdahl<space/></arg><arg name="url"><space/>http://www-inst.eecs.berkeley.edu/~n252/paper/Amdahl.pdf</arg><arg name="doi">10.1145/1465482.1465560<space/></arg><arg name="title"><space/>Validity of the Single Processor Approach to Achieving Large-Scale Computing Capabilities<space/></arg><arg name="journal"><space/>AFIPS Conference Proceedings<space/></arg><arg name="issue"><space/>30<space/></arg><arg name="pages"><space/>483–485<space/></arg><arg name="year"><space/>1967</arg><arg name="ref">harv</arg></template></listitem></list><heading level='2'>External links</heading><paragraph><template><target>Commons category</target><arg>Amdahl's law</arg></template></paragraph><list type='bullet'><listitem><link type='external' href='http://www.futurechips.org/thoughts-for-researchers/parallel-programming-gene-amdahl-said.html'>Cases where Amdahl's law is inapplicable</link></listitem><listitem><link type='external' href='http://purl.umn.edu/104341'>Oral history interview with Gene M. Amdahl</link><space/><link><target>Charles Babbage Institute</target></link>, University of Minnesota. Amdahl discusses his graduate work at the University of Wisconsin and his design of<space/><link><target>Wisconsin Integrally Synchronized Computer</target><part>WISC</part></link>. Discusses his role in the design of several computers for IBM including the<space/><link><target>IBM Stretch</target><part>STRETCH</part></link>,<space/><link><target>IBM 701</target></link>, and<space/><link><target>IBM 704</target></link>. He discusses his work with<space/><link><target>Nathaniel Rochester (computer scientist)</target><part>Nathaniel Rochester</part></link><space/>and IBM's management of the design process. Mentions work with<space/><link><target>TRW Inc.</target><part>Ramo-Wooldridge</part></link>,<space/><link><target>Aeronutronic</target></link>, and<space/><link><target>Computer Sciences Corporation</target></link></listitem><listitem><link type='external' href='http://www.julianbrowne.com/article/viewer/amdahls-law'>A simple interactive Amdahl's Law calculator</link></listitem><listitem><link type='external' href='http://demonstrations.wolfram.com/AmdahlsLaw/'>&quot;Amdahl's Law&quot;</link><space/>by Joel F. Klein,<space/><link><target>Wolfram Demonstrations Project</target></link>, 2007.</listitem><listitem><link type='external' href='http://www.cs.wisc.edu/multifacet/amdahl/'>Amdahl's Law in the Multicore Era</link></listitem><listitem><link type='external' href='http://www.cilk.com/multicore-blog/bid/5365/What-the-is-Parallelism-Anyhow'>Blog Post: &quot;What the $#@! is Parallelism, Anyhow?&quot;</link></listitem><listitem><link type='external' href='http://www.multicorepacketprocessing.com/how-should-amdahl-law-drive-the-redesigns-of-socket-system-calls-for-an-os-on-a-multicore-cpu'>Amdahl's Law applied to OS system calls on multicore CPU</link></listitem><listitem><link type='external' href='https://www.cs.sfu.ca/~fedorova/papers/TurboBoostEvaluation.pdf'>Evaluation of the Intel Core i7 Turbo Boost feature</link>, by James Charles, Preet Jassi, Ananth Narayan S, Abbas Sadat and Alexandra Fedorova</listitem><listitem><link type='external' href='http://www.researchgate.net/publication/228569958_Calculation_of_the_acceleration_of_parallel_programs_as_a_function_of_the_number_of_threads'>Calculation of the acceleration of parallel programs as a function of the number of threads</link>, by George Popov, Valeri Mladenov and Nikos Mastorakis</listitem></list><paragraph><template><target>Computer laws</target></template><template><target>Parallel Computing</target></template></paragraph><paragraph><template><target>DEFAULTSORT:Amdahl's Law</target></template><link><target>Category:Analysis of parallel algorithms</target></link><link><target>Category:Programming rules of thumb</target></link></paragraph></article>