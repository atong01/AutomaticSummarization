<article title='Condition_number'><paragraph>In the field of<space/><link><target>numerical analysis</target></link>, the<space/><bold>condition number</bold><space/>of a function with respect to an argument measures how much the output value of the function can change for a small change in the input argument. This is used to measure how sensitive a function is to changes or errors in the input, and how much error in the output results from an error in the input. Very frequently, one is solving the inverse problem given<space/><extension extension_name='math'>f(x) = y,</extension><space/>one is solving for<space/><italics>x,</italics><space/>and thus the condition number of the (local) inverse must be used. In<space/><link><target>linear regression</target></link><space/>the condition number can be used as a diagnostic for<space/><link><target>multicollinearity</target></link>.</paragraph><paragraph>The condition number is an application of the derivative, and is formally defined as the value of the asymptotic worst-case relative change in output for a relative change in input. The &quot;function&quot; is the solution of a problem and the &quot;arguments&quot; are the data in the problem. The condition number is frequently applied to questions in linear algebra, in which case the derivative is straightforward but the error could be in many different directions, and is thus computed from the geometry of the matrix. More generally, condition numbers can be defined for non-linear functions in several variables.</paragraph><paragraph>A problem with a low condition number is said to be<space/><bold>well-conditioned</bold>, while a problem with a high condition number is said to be<space/><bold>ill-conditioned</bold>. The condition number is a property of the problem. Paired with the problem are any number of algorithms that can be used to solve the problem, that is, to calculate the solution. Some algorithms have a property called<space/><bold><link><target>Numerical stability</target><part>backward stability</part></link></bold>. In general, a backward stable algorithm can be expected to accurately solve well-conditioned problems. Numerical analysis textbooks give formulas for the condition numbers of problems and identify the backward stable algorithms.</paragraph><paragraph>As a rule of thumb, if the condition number<space/><extension extension_name='math'>\kappa(A) = 10^k</extension>, then you may lose up to<space/><extension extension_name='math'>k</extension><space/>digits of accuracy on top of what would be lost to the numerical method due to loss of precision from arithmetic methods.<extension extension_name='ref' name="Numerical Mathematics and Computing, by Cheney and Kincaid"><template><target>cite book</target><arg name="url">http://books.google.com/?id</arg><arg name="title"><space/>Numerical Mathematics and Computing</arg><arg name="last1"><space/>Cheney<space/></arg><arg name="last2"><space/>Kincaid</arg><arg name="isbn"><space/>978-0-495-11475-8</arg><arg name="date"><space/>2007-08-03</arg></template></extension><space/>However, the condition number does not give the exact value of the maximum inaccuracy that may occur in the algorithm. It generally just bounds it with an estimate (whose computed value depends on the choice of the norm to measure the inaccuracy).</paragraph><heading level='2'>Matrices</heading><paragraph>For example, the condition number associated with the<space/><link><target>linear equation</target></link><italics>Ax</italics>&amp;nbsp;=&amp;nbsp;<italics>b</italics><space/>gives a bound on how inaccurate the solution<space/><italics>x</italics><space/>will be after approximation. Note that this is before the effects of<space/><link><target>round-off error</target></link><space/>are taken into account; conditioning is a property of the matrix, not the<space/><link><target>algorithm</target></link><space/>or<space/><link><target>floating point</target></link><space/>accuracy of the computer used to solve the corresponding system. In particular, one should think of the condition number as being (very roughly) the rate at which the solution,<space/><italics>x</italics>, will change with respect to a change in<space/><italics>b</italics>. Thus, if the condition number is large, even a small error in<space/><italics>b</italics><space/>may cause a large error in<space/><italics>x</italics>. On the other hand, if the condition number is small then the error in<space/><italics>x</italics><space/>will not be much bigger than the error in<space/><italics>b</italics>.</paragraph><paragraph>The condition number is defined more precisely to be the maximum ratio of the<space/><link><target>relative error</target></link><space/>in<space/><italics>x</italics><space/>divided by the relative error in<space/><italics>b</italics>.</paragraph><paragraph>Let<space/><italics>e</italics><space/>be the error in<space/><italics>b</italics>. Assuming that<space/><italics>A</italics><space/>is a nonsingular matrix, the error in the solution<space/><italics>A</italics><xhtml:sup>1</xhtml:sup><italics>b</italics><space/>is<space/><italics>A</italics><xhtml:sup>1</xhtml:sup><italics>e</italics>. The ratio of the relative error in the solution to the relative error in<space/><italics>b</italics><space/>is</paragraph><list type='ident'><listitem><extension extension_name='math'><space/>\frac{ \left\Vert A^{-1} e \right\Vert / \left\Vert A^{-1} b \right\Vert }{ \left\Vert e \right\Vert / \left\Vert b \right\Vert } .</extension></listitem></list><paragraph>This is easily transformed to</paragraph><list type='ident'><listitem><extension extension_name='math'><space/>\left( \left\Vert A^{-1} e \right\Vert / \left\Vert e \right\Vert \right) \cdot \left( \left\Vert b \right\Vert / \left\Vert A^{-1} b \right\Vert \right) .</extension></listitem></list><paragraph>The maximum value (for nonzero<space/><italics>b</italics><space/>and<space/><italics>e</italics>) is easily seen to be the product of the two<space/><link><target>operator norm</target><trail>s</trail></link>:</paragraph><list type='ident'><listitem><extension extension_name='math'><space/>\kappa(A) = \left\Vert A^{-1} \right\Vert \cdot \left\Vert A \right\Vert .</extension></listitem></list><paragraph>The same definition is used for any consistent<space/><link><target>matrix norm</target><part>norm</part></link>,<template><target>clarify</target><arg name="date">October 2014</arg></template><space/>i.e. one that satisfies</paragraph><list type='ident'><listitem><extension extension_name='math'><space/>\kappa(A) \ge 1 .\,</extension></listitem></list><paragraph>When the condition number is exactly one (which can only happen if<space/><italics>A</italics><space/>is a<space/><link><target>Isometry#Linear isometry</target><part>linear isometry</part></link>), then a solution algorithm can find (in principle, meaning if the algorithm introduces no errors of its own) an approximation of the solution whose precision is no worse than that of the data.</paragraph><paragraph>However, it does not mean that the algorithm will converge rapidly to this solution, just that it won't diverge arbitrarily because of inaccuracy on the source data (backward error), provided that the forward error introduced by the algorithm does not diverge as well because of accumulating intermediate rounding errors.<template><target>clarify</target><arg name="date">October 2014</arg></template></paragraph><paragraph>The condition number may also be infinite, but this implies that the problem is<space/><link><target>well-posed problem</target><part>ill-posed</part></link><space/>(does not possess a unique, well-defined solution for each choice of data -- that is, the matrix is not invertible), and no algorithm can be expected to reliably find a solution.</paragraph><paragraph>Of course, the definition of the condition number depends on the choice of norm, as can be illustrated by two examples.</paragraph><paragraph>If<space/><extension extension_name='math'><space/>\left\| \cdot \right\|<space/></extension><space/>is the<space/><link><target>matrix norm</target><part>norm</part></link><space/>(usually noted as<space/><extension extension_name='math'><space/>\left\| \cdot \right\|_2<space/></extension>) defined in the square-summable<space/><link><target>sequence space</target></link><space/><link><target>Lp space</target><part><xhtml:sup>2</xhtml:sup></part></link><space/>(which matches the usual distance in a standard Euclidean space), then</paragraph><list type='ident'><listitem><extension extension_name='math'><space/>\kappa(A) = \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)} ,</extension></listitem></list><paragraph>where<space/><extension extension_name='math'><space/>\sigma_{\max}(A)</extension><space/>and<space/><extension extension_name='math'>\sigma_{\min}(A)<space/></extension><space/>are maximal and minimal<space/><link><target>singular value</target><trail>s</trail></link><space/>of<space/><extension extension_name='math'><space/>A<space/></extension><space/>respectively. Hence</paragraph><list type='bullet'><listitem>If<space/><extension extension_name='math'>A</extension><space/>is<space/><link><target>normal matrix</target><part>normal</part></link><space/>then<list type='ident'><listitem><extension extension_name='math'><space/>\kappa(A) = \left|\frac{\lambda_{\max}(A)}{\lambda_{\min}(A)}\right| ,</extension></listitem></list></listitem></list><paragraph>where<space/><extension extension_name='math'><space/>\lambda_{\max}(A)</extension><space/>and<space/><extension extension_name='math'>\lambda_{\min}(A)<space/></extension><space/>are maximal and minimal (by moduli)<space/><link><target>eigenvalue</target><trail>s</trail></link><space/>of<space/><extension extension_name='math'>A</extension><space/>respectively.</paragraph><list type='bullet'><listitem>If<space/><extension extension_name='math'>A</extension><space/>is<space/><link><target>unitary matrix</target><part>unitary</part></link><space/>then<list type='ident'><listitem><extension extension_name='math'><space/>\kappa(A) = 1 .\,</extension></listitem></list></listitem></list><paragraph>The condition number with respect to<space/><italics>L<xhtml:sup>2</xhtml:sup></italics><space/>arises so often in numerical<space/><link><target>linear algebra</target></link><space/>that it is given a name, the<space/><bold>condition number of a matrix</bold>.</paragraph><paragraph>If<space/><extension extension_name='math'><space/>\left\| \cdot \right\|<space/></extension><space/>is the<space/><link><target>matrix norm</target><part>norm</part></link><space/>(usually denoted by<space/><extension extension_name='math'><space/>\left\| \cdot \right\|_\infty<space/></extension>) defined in the<space/><link><target>sequence space</target></link><space/><link><target>Lp space</target><part><xhtml:sup></xhtml:sup></part></link><space/>of all<space/><link><target>Bounded operator</target><part>bounded</part></link><space/>sequences (which matches the maximum of distances measured on projections into the base subspaces), and<space/><extension extension_name='math'>A</extension><space/>is<space/><link><target>triangular matrix</target><part>lower triangular</part></link><space/>non-singular (i.e.,<space/><extension extension_name='math'><space/>\forall i, a_{ii} \ne 0 \,</extension>) then</paragraph><list type='ident'><listitem><extension extension_name='math'><space/>\kappa(A) \geq \frac{\max_i(|a_{ii}|)}{\min_i(|a_{ii}|)} .</extension></listitem></list><paragraph>The condition number computed with this norm is generally larger than the condition number computed with square-summable sequences, but it can be evaluated more easily (and this is often the only practicably computable condition number, when the problem to solve involves a<space/><italics>non-linear algebra</italics><template><target>what?</target><arg name="date">October 2014</arg></template>, for example when approximating irrational and transcendental functions or numbers with numerical methods.)</paragraph><paragraph>If the condition number is not too much larger than one (but it can still be a multiple of one), the matrix is well conditioned which means its inverse can be computed with good accuracy. If the condition number is very large, then the matrix is said to be ill-conditioned. Practically, such a matrix is almost singular, and the computation of its inverse, or solution of a linear system of equations is prone to large numerical errors. A matrix that is not invertible has condition number equal to infinity.</paragraph><heading level='2'>Non-linear</heading><paragraph>Condition numbers can also be defined for nonlinear functions, and can be computed using calculus. The condition number varies with the point; in some cases one can use the maximum (or supremum) condition number over the domain of the function or domain of the question as an overall condition number, while in other cases the condition number at a particular point is of more interest.</paragraph><heading level='3'>One variable</heading><paragraph><template><target>See also</target><arg>Significance arithmetic#Transcendental functions</arg></template></paragraph><paragraph>The condition number of a differentiable function<space/><italics>f</italics><space/>in one variable as a function is<space/><extension extension_name='math'>xf'/f.</extension><space/>Evaluated at a point<space/><italics>x</italics><space/>this is:</paragraph><list type='ident'><listitem><extension extension_name='math'>\frac{xf'(x)}{f(x)}.</extension></listitem></list><paragraph>Most elegantly, this can be understood as (the absolute value of) the ratio of the<space/><link><target>logarithmic derivative</target></link><space/>of<space/><italics>f,</italics><space/>which is<space/><extension extension_name='math'>(\log f)' = f'/f</extension><space/>and the logarithmic derivative of<space/><italics>x,</italics><space/>which is<space/><extension extension_name='math'>(\log x)' = x'/x = 1/x,</extension><space/>yielding a ratio of<space/><extension extension_name='math'>xf'/f.</extension><space/>This is because the logarithmic derivative is the infinitesimal rate of relative change in a function: it is the derivative<space/><extension extension_name='math'>f'</extension><space/>scaled by the value of<space/><italics>f.</italics><space/>Note that if a function has a zero at a point, its condition number at the point is infinite, as infinitesimal changes in the input can change the output from zero to positive or negative, yielding a ratio with zero in the denominator, hence infinite relative change.</paragraph><paragraph>More directly, given a small change<space/><extension extension_name='math'>\Delta x</extension><space/>in<space/><italics>x,</italics><space/>the relative change in<space/><italics>x</italics><space/>is<space/><extension extension_name='math'>[(x + \Delta x) - x]/x = (\Delta x)/x,</extension><space/>while the relative change in<space/><extension extension_name='math'>f(x)</extension><space/>is<space/><extension extension_name='math'>[f(x + \Delta x) - f(x)]/f(x).</extension><space/>Taking the ratio yields:</paragraph><list type='ident'><listitem><extension extension_name='math'>\frac{[f(x + \Delta x) - f(x)]/f(x)}{(\Delta x)/x}
= \frac{x}{f(x)}\frac{f(x + \Delta x) - f(x)}{(x + \Delta x) - x}.</extension></listitem></list><paragraph>The last term is the<space/><link><target>difference quotient</target></link><space/>(the slope of the secant line), and taking the limit yields the derivative.</paragraph><paragraph>Condition numbers of common<space/><link><target>elementary function</target><trail>s</trail></link><space/>are particularly important in computing<space/><link><target>significant figures</target></link>, and can be computed immediately from the derivative; see<space/><link><target>Significance arithmetic#Transcendental functions</target><part>significance arithmetic of transcendental functions</part></link>. A few important ones are given below:</paragraph><list type='bullet'><listitem>Exponential function<space/><extension extension_name='math'>e^x</extension>:<space/><extension extension_name='math'>x</extension></listitem><listitem>Natural logarithm function<space/><extension extension_name='math'>\ln(x)</extension>:<space/><extension extension_name='math'>\frac{1}{\ln(x)}</extension></listitem><listitem>Sine function<space/><extension extension_name='math'>\sin(x)</extension>:<space/><extension extension_name='math'>x\cot(x)</extension></listitem><listitem>Cosine function<space/><extension extension_name='math'>\cos(x)</extension>:<space/><extension extension_name='math'>x\tan(x)</extension></listitem><listitem>Tangent function<space/><extension extension_name='math'>\tan(x)</extension>:<space/><extension extension_name='math'>x(\tan(x)+\cot(x))</extension></listitem><listitem>Inverse sine function<space/><extension extension_name='math'>\arcsin(x)</extension>:<space/><extension extension_name='math'>\frac{x}{\sqrt{1-x^2}\arcsin(x)}</extension></listitem><listitem>Inverse cosine function<space/><extension extension_name='math'>\arccos(x)</extension>:<space/><extension extension_name='math'>\frac{x}{\sqrt{1-x^2}\arccos(x)}</extension></listitem><listitem>Inverse tangent function<space/><extension extension_name='math'>\arctan(x)</extension>:<space/><extension extension_name='math'>\frac{x}{(1+x^2)\arctan(x)}</extension></listitem></list><heading level='3'>Several variables</heading><paragraph>Condition numbers can be defined for any function<space/><italics></italics><space/>mapping its data from some<space/><link><target>function domain</target><part>domain</part></link><space/>(e.g. an<space/><italics>m</italics>-tuple of real numbers<space/><italics>x</italics>) into some<space/><link><target>codomain</target></link><space/>[e.g. an<space/><italics>n</italics>-tuple of real numbers<space/><italics></italics>(<italics>x</italics>)], where both the domain and codomain are<space/><link><target>Banach space</target><trail>s</trail></link>. They express how sensitive that function is to small changes (or small errors) in its arguments. This is crucial in assessing the sensitivity and potential accuracy difficulties of numerous computational problems, for example<space/><link><target>polynomial</target></link><space/><link><target>root finding</target></link><space/>or computing<space/><link><target>eigenvalue</target><trail>s</trail></link>.</paragraph><paragraph>The condition number of<space/><italics></italics><space/>at a point<space/><italics>x</italics><space/>(specifically, its<space/><bold>relative condition number</bold><extension extension_name='ref' name="TrefethenBau"></extension>) is then defined to be the maximum ratio of the fractional change in<space/><italics></italics>(<italics>x</italics>) to any fractional change in<space/><italics>x</italics>, in the limit where the change<space/><italics>x</italics><space/>in<space/><italics>x</italics><space/>becomes infinitesimally small:<extension extension_name='ref' name="TrefethenBau"><template><target>cite book</target><arg name="isbn"><space/>978-0-89871-361-9</arg><arg name="first1">L. N.</arg><arg name="last1"><space/>Trefethen<space/></arg><arg name="first2"><space/>D.<space/></arg><arg name="last2">Bau</arg><arg name="title">Numerical Linear Algebra</arg><arg name="publisher">SIAM</arg><arg name="year">1997</arg><arg name="url">http://books.google.com/?id</arg></template></extension></paragraph><list type='ident'><listitem><extension extension_name='math'>\lim_{ \varepsilon \to 0^+ }
 \sup_{ \Vert \delta x \Vert \leq \varepsilon }
 \left[ \frac{ \left\Vert f(x + \delta x) - f(x)\right\Vert }{ \Vert f(x) \Vert }
 / \frac{ \Vert \delta x \Vert }{ \Vert x \Vert }
 \right],</extension></listitem></list><paragraph>where<space/><extension extension_name='math'>\Vert \cdots \Vert</extension><space/>is a<space/><link><target>Norm (mathematics)</target><part>norm</part></link><space/>on the domain/codomain of<space/><italics></italics>(<italics>x</italics>).</paragraph><paragraph>If<space/><italics></italics><space/>is differentiable, this is equivalent to:<extension extension_name='ref' name="TrefethenBau"></extension></paragraph><list type='ident'><listitem><extension extension_name='math'>\frac{\Vert J (x)\Vert}{ \Vert f(x) \Vert / \Vert x \Vert},</extension></listitem></list><paragraph>where<space/><italics>J(x)</italics><space/>denotes the<space/><link><target>Jacobian matrix</target></link><space/>of<space/><link><target>partial derivative</target><trail>s</trail></link><space/>of<space/><italics></italics><space/>at<space/><italics>x</italics><space/>and<space/><extension extension_name='math'>\Vert J (x)\Vert</extension><space/>is the<space/><link><target>induced norm</target></link><space/>on the matrix.</paragraph><heading level='2'>See also</heading><list type='bullet'><listitem><link><target>Singular value</target></link></listitem><listitem><link><target>Ill-posed</target></link></listitem></list><heading level='2'>References</heading><paragraph><extension extension_name='references'></extension></paragraph><heading level='2'>External links</heading><list type='bullet'><listitem><link type='external' href='http://numericalmethods.eng.usf.edu/mws/gen/04sle/mws_gen_sle_spe_adequacy.pdf'>Condition Number of a Matrix</link><space/>at<space/><italics>Holistic Numerical Methods Institute</italics></listitem><listitem><template><target>planetmath reference</target><arg name="id">3480</arg><arg name="title">Matrix condition number</arg></template></listitem><listitem><link type='external' href='http://www.mathworks.in/help/techdoc/ref/cond.html'>MATLAB library function to determine condition number</link></listitem><listitem><link type='external' href='http://www.encyclopediaofmath.org/index.php/Condition_number'>Condition number Encyclopedia of Mathematics</link></listitem></list><paragraph><link><target>Category:Numerical analysis</target></link><link><target>Category:Matrices</target></link></paragraph></article>