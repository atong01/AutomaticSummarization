<article title='Kolmogorov_complexity'><paragraph><template><target>distinguish</target><arg>descriptive complexity theory</arg></template></paragraph><table style="float:right"><tablerow><tablecell><link><target>Image:Mandelpart2 red.png</target><part>300px</part><part>right</part><part>thumb</part><part>This image illustrates part of the<space/><link><target>Mandelbrot set</target></link><space/><link><target>fractal</target></link>. Simply storing the 24-bit color of each pixel in this image would require 1.62 million bits, but a small computer program can reproduce these 1.62 million bits using the definition of the Mandelbrot set and the coordinates of the corners of the image. Thus, the Kolmogorov complexity of the raw file encoding this bitmap is much less than 1.62 million bits in any pragmatic model of computation.</part></link></tablecell></tablerow></table><paragraph>In<space/><link><target>algorithmic information theory</target></link><space/>(a subfield of<space/><link><target>computer science</target></link><space/>and<space/><link><target>mathematics</target></link>), the<space/><bold>Kolmogorov complexity</bold><space/>(also known as<space/><bold>descriptive complexity</bold>,<space/><bold>Kolmogorov<link><target>Gregory Chaitin</target><part>Chaitin</part></link><space/>complexity</bold>,<space/><bold>algorithmic entropy</bold>, or<space/><bold>program-size complexity</bold>) of an object, such as a piece of text, is a measure of the<space/><link><target>computation</target><trail>al</trail></link><space/>resources needed to specify the object. It is named after<space/><link><target>Andrey Kolmogorov</target></link>, who first published on the subject in 1963.<extension extension_name='ref'><template><target>cite journal</target><arg name="authorlink">Andrey Kolmogorov</arg><arg name="first">Andrey</arg><arg name="last">Kolmogorov</arg><arg name="year">1963</arg><arg name="title">On Tables of Random Numbers</arg><arg name="journal">[[Sankhya (journal)|Sankhyā]] Ser. A.</arg><arg name="volume">25</arg><arg name="pages">369–375</arg><arg name="mr">178484</arg></template></extension><extension extension_name='ref'><template><target>cite journal</target><arg name="authorlink">Andrey Kolmogorov</arg><arg name="first">Andrey</arg><arg name="last">Kolmogorov</arg><arg name="year">1998</arg><arg name="title">On Tables of Random Numbers</arg><arg name="journal">Theoretical Computer Science</arg><arg name="volume">207</arg><arg name="issue">2</arg><arg name="pages">387–395</arg><arg name="doi">10.1016/S0304-3975(98)00075-9<space/></arg><arg name="mr">1643414</arg></template></extension></paragraph><paragraph>For example, consider the following two<space/><link><target>string (computer science)</target><part>strings</part></link><space/>of 32 lowercase letters and digits:</paragraph><paragraph><xhtml:pre>abababababababababababababababab</xhtml:pre><xhtml:pre>4c1j5b2p0cv4w1x8rx2y39umgw5q85s7</xhtml:pre></paragraph><paragraph>The first string has a short English-language description, namely &quot;<xhtml:tt>ab 16 times</xhtml:tt>&quot;, which consists of<space/><bold>11</bold><space/>characters. The second one has no obvious simple description (using the same character set) other than writing down the string itself, which has<space/><bold>32</bold><space/>characters.</paragraph><paragraph>More formally, the<space/><link><target>complexity</target></link><space/>of a string is the length of the shortest possible description of the string in some fixed<space/><link><target>Turing complete</target><part>universal</part></link><space/>description language (the sensitivity of complexity relative to the choice of description language is discussed below). It can be shown that the Kolmogorov complexity of any string cannot be more than a few bytes larger than the length of the string itself. Strings, like the<space/><italics>abab</italics><space/>example above, whose Kolmogorov complexity is small relative to the string's size are not considered to be complex.</paragraph><paragraph>The notion of Kolmogorov complexity can be used to state and prove impossibility results akin to<space/><link><target>Cantor's diagonal argument</target></link>,<space/><link><target>Gdel's incompleteness theorem</target></link>, and<space/><link><target>halting problem</target><part>Turing's halting problem</part></link>.</paragraph><heading level='2'>Definition</heading><paragraph>The Kolmogorov complexity can be defined for any mathematical object, but for simplicity the scope of this article is restricted to strings. We must first specify a description language for strings. Such a description language can be based on any computer programming language, such as<space/><link><target>Lisp programming language</target><part>Lisp</part></link>,<space/><link><target>Pascal (programming language)</target><part>Pascal</part></link>, or<space/><link><target>Java virtual machine</target></link><space/>bytecode. If<space/><bold>P</bold><space/>is a program which outputs a string<space/><italics>x</italics>, then<space/><bold>P</bold><space/>is a description of<space/><italics>x</italics>. The length of the description is just the length of<space/><bold>P</bold><space/>as a character string, multiplied by the number of bits in a character (e.g. 7 for<space/><link><target>ASCII</target></link>).</paragraph><paragraph>We could, alternatively, choose an encoding for<space/><link><target>Turing machine</target><trail>s</trail></link>, where an<space/><italics>encoding</italics><space/>is a function which associates to each Turing Machine<space/><bold>M</bold><space/>a bitstring &lt;<bold>M</bold>&gt;. If<space/><bold>M</bold><space/>is a Turing Machine which, on input<space/><italics>w</italics>, outputs string<space/><italics>x</italics>, then the concatenated string &lt;<bold>M</bold>&gt;<space/><italics>w</italics><space/>is a description of<space/><italics>x</italics>. For theoretical analysis, this approach is more suited for constructing detailed formal proofs and is generally preferred in the research literature. In this article, an informal approach is discussed.</paragraph><paragraph>Any string<space/><italics>s</italics><space/>has at least one description, namely the program:</paragraph><preblock><preline><space/><bold>function</bold><space/>GenerateFixedString()</preline><preline><space/><space/><space/><space/><bold>return</bold><space/><italics>s</italics></preline></preblock><paragraph>If a description of<space/><italics>s</italics>,<space/><italics>d</italics>(<italics>s</italics>), is of minimal length (i.e. it uses the fewest bits), it is called a<space/><bold>minimal description</bold><space/>of<space/><italics>s</italics>. Thus, the length of<space/><italics>d</italics>(<italics>s</italics>) (i.e. the number of bits in the description) is the<space/><bold>Kolmogorov complexity</bold><space/>of<space/><italics>s</italics>, written<space/><italics>K</italics>(<italics>s</italics>). Symbolically,</paragraph><list type='ident'><listitem><italics>K</italics>(<italics>s</italics>) = |<italics>d</italics>(<italics>s</italics>)|.</listitem></list><paragraph>The length of the shortest description will depend on the choice of description language; but the effect of changing languages is bounded (a result called the<space/><italics>invariance theorem</italics>).</paragraph><heading level='2'>Invariance theorem</heading><heading level='3'>Informal treatment</heading><paragraph>There are some description languages which are optimal, in the following sense: given any description of an object in a description language, I can use that description in my optimal description language with a constant overhead. The constant depends only on the languages involved, not on the description of the object, nor the object being described.</paragraph><paragraph>Here is an example of an optimal description language. A description will have two parts:</paragraph><list type='bullet'><listitem>The first part describes another description language.</listitem><listitem>The second part is a description of the object in that language.</listitem></list><paragraph>In more technical terms, the first part of a description is a computer program, with the second part being the input to that computer program which produces the object as output.</paragraph><paragraph><bold>The invariance theorem follows:</bold><space/>Given any description language<space/><italics>L</italics>, the optimal description language is at least as efficient as<space/><italics>L</italics>, with some constant overhead.</paragraph><paragraph><bold>Proof:</bold><space/>Any description<space/><italics>D</italics><space/>in<space/><italics>L</italics><space/>can be converted into a description in the optimal language by first describing<space/><italics>L</italics><space/>as a computer program<space/><italics>P</italics><space/>(part 1), and then using the original description<space/><italics>D</italics><space/>as input to that program (part 2). The total length of this new description<space/><italics>D</italics><space/>is (approximately):</paragraph><list type='ident'><listitem>|<italics>D</italics>| = |<italics>P</italics>| + |<italics>D</italics>|</listitem></list><paragraph>The length of<space/><italics>P</italics><space/>is a constant that doesn't depend on<space/><italics>D</italics>. So, there is at most a constant overhead, regardless of the object described. Therefore, the optimal language is universal<space/><link><target>up to</target></link><space/>this additive constant.</paragraph><heading level='3'>A more formal treatment</heading><paragraph><bold>Theorem</bold>: If<space/><italics>K</italics><xhtml:sub>1</xhtml:sub><space/>and<space/><italics>K</italics><xhtml:sub>2</xhtml:sub><space/>are the complexity functions relative to<space/><link><target>Turing complete</target></link><space/>description languages<space/><italics>L</italics><xhtml:sub>1</xhtml:sub><space/>and<space/><italics>L</italics><xhtml:sub>2</xhtml:sub>, then there is a constant<space/><italics>c</italics><space/>which depends only on the languages<space/><italics>L</italics><xhtml:sub>1</xhtml:sub><space/>and<space/><italics>L</italics><xhtml:sub>2</xhtml:sub><space/>chosen such that</paragraph><list type='ident'><listitem><italics>s</italics>. -<italics>c</italics><space/><italics>K</italics><xhtml:sub>1</xhtml:sub>(<italics>s</italics>) -<space/><italics>K</italics><xhtml:sub>2</xhtml:sub>(<italics>s</italics>)<space/><italics>c</italics>.</listitem></list><paragraph><bold>Proof</bold>: By symmetry, it suffices to prove that there is some constant<space/><italics>c</italics><space/>such that for all strings<space/><italics>s</italics></paragraph><list type='ident'><listitem><italics>K</italics><xhtml:sub>1</xhtml:sub>(<italics>s</italics>)<space/><italics>K</italics><xhtml:sub>2</xhtml:sub>(<italics>s</italics>) +<space/><italics>c</italics>.</listitem></list><paragraph>Now, suppose there is a program in the language<space/><italics>L</italics><xhtml:sub>1</xhtml:sub><space/>which acts as an<space/><link><target>interpreter (computing)</target><part>interpreter</part></link><space/>for<space/><italics>L</italics><xhtml:sub>2</xhtml:sub>:</paragraph><preblock><preline><space/><space/><bold>function</bold><space/>InterpretLanguage(<bold>string</bold><space/><italics>p</italics>)</preline></preblock><paragraph>where<space/><italics>p</italics><space/>is a program in<space/><italics>L</italics><xhtml:sub>2</xhtml:sub>. The interpreter is characterized by the following property:</paragraph><list type='ident'><listitem>Running<space/><xhtml:code>InterpretLanguage</xhtml:code><space/>on input<space/><italics>p</italics><space/>returns the result of running<space/><italics>p</italics>.</listitem></list><paragraph>Thus, if<space/><bold>P</bold><space/>is a program in<space/><italics>L</italics><xhtml:sub>2</xhtml:sub><space/>which is a minimal description of<space/><italics>s</italics>, then<space/><xhtml:code>InterpretLanguage</xhtml:code>(<bold>P</bold>) returns the string<space/><italics>s</italics>. The length of this description of<space/><italics>s</italics><space/>is the sum of</paragraph><list type='numbered'><listitem>The length of the program<space/><xhtml:code>InterpretLanguage</xhtml:code>, which we can take to be the constant<space/><italics>c</italics>.</listitem><listitem>The length of<space/><bold>P</bold><space/>which by definition is<space/><italics>K</italics><xhtml:sub>2</xhtml:sub>(<italics>s</italics>).</listitem></list><paragraph>This proves the desired upper bound.</paragraph><heading level='2'>History and context</heading><paragraph><link><target>Algorithmic information theory</target></link><space/>is the area of computer science that studies Kolmogorov complexity and other complexity measures on strings (or other<space/><link><target>data structure</target><trail>s</trail></link>).</paragraph><paragraph>The concept and theory of Kolmogorov Complexity is based on a crucial theorem first discovered by<space/><link><target>Ray Solomonoff</target></link>, who published it in 1960, describing it in &quot;A Preliminary Report on a General Theory of Inductive Inference&quot;<extension extension_name='ref'><template><target>cite journal</target><arg name="authorlink">Ray Solomonoff<space/></arg><arg name="last">Solomonoff<space/></arg><arg name="first"><space/>Ray<space/></arg><arg name="url">http://world.std.com/~rjs/rayfeb60.pdf<space/></arg><arg name="format">PDF<space/></arg><arg name="title">A Preliminary Report on a General Theory of Inductive Inference<space/></arg><arg name="journal"><space/>Report V-131<space/></arg><arg name="publisher"><space/>Zator Co.<space/></arg><arg name="location"><space/>Cambridge, Ma.<space/></arg><arg name="date"><space/>February 4, 1960<space/></arg></template><space/><link type='external' href='http://world.std.com/~rjs/z138.pdf'>revision</link>, Nov., 1960.</extension><space/>as part of his invention of<space/><link><target>algorithmic probability</target></link>. He gave a more complete description in his 1964 publications, &quot;A Formal Theory of Inductive Inference,&quot; Part 1 and Part 2 in<space/><italics>Information and Control</italics>.<extension extension_name='ref'><template><target>cite journal</target><arg name="doi">10.1016/S0019-9958(64)90223-2 
</arg><arg name="last">Solomonoff 
</arg><arg name="first"><space/>Ray 
</arg><arg name="title">A Formal Theory of Inductive Inference Part I 
</arg><arg name="journal"><space/>Information and Control 
</arg><arg name="url">http://world.std.com/~rjs/1964pt1.pdf 
</arg><arg name="volume">7 
</arg><arg name="issue"><space/>1 
</arg><arg name="pages"><space/>1&ndash;22 
</arg><arg name="date">March 1964
</arg></template></extension><extension extension_name='ref'><template><target>cite journal</target><arg name="doi">10.1016/S0019-9958(64)90131-7 
</arg><arg name="last">Solomonoff 
</arg><arg name="first"><space/>Ray 
</arg><arg name="title">A Formal Theory of Inductive Inference Part II 
</arg><arg name="journal"><space/>Information and Control 
</arg><arg name="url">http://world.std.com/~rjs/1964pt2.pdf 
</arg><arg name="volume">7 
</arg><arg name="issue"><space/>2 
</arg><arg name="pages"><space/>224&ndash;254 
</arg><arg name="date">June 1964
</arg></template></extension></paragraph><paragraph>Andrey Kolmogorov later<space/><link><target>multiple discovery</target><part>independently published</part></link><space/>this theorem in<space/><italics>Problems Inform. Transmission</italics><extension extension_name='ref'><template><target>cite journal</target><arg name="volume"><space/>1</arg><arg name="issue">1<space/></arg><arg name="year">1965<space/></arg><arg name="pages"><space/>1–7<space/></arg><arg name="title">Three Approaches to the Quantitative Definition of Information<space/></arg><arg name="url">http://www.ece.umd.edu/~abarg/ppi/contents/1-65-abstracts.html#1-65.2<space/></arg><arg name="journal"><space/>Problems Inform. Transmission<space/></arg><arg name="first">A.N.<space/></arg><arg name="last">Kolmogorov<space/></arg></template></extension><space/>in 1965. Gregory Chaitin also presents this theorem in<space/><italics>J. ACM</italics><space/>Chaitin's paper was submitted October 1966 and revised in December 1968, and cites both Solomonoff's and Kolmogorov's papers.<extension extension_name='ref'><template><target>cite journal</target><arg name="last1"><space/>Chaitin<space/></arg><arg name="first1"><space/>Gregory J.<space/></arg><arg name="title"><space/>On the Simplicity and Speed of Programs for Computing Infinite Sets of Natural Numbers</arg><arg name="url">http://www.cs.auckland.ac.nz/~chaitin/acm69b.pdf</arg><arg name="journal"><space/>Journal of the ACM<space/></arg><arg name="volume"><space/>16<space/></arg><arg name="pages"><space/>407–422</arg><arg name="year"><space/>1969<space/></arg><arg name="doi"><space/>10.1145/321526.321530<space/></arg><arg name="issue"><space/>3</arg></template></extension></paragraph><paragraph>The theorem says that, among algorithms that decode strings from their descriptions (codes), there exists an optimal one. This algorithm, for all strings, allows codes as short as allowed by any other algorithm up to an additive constant that depends on the algorithms, but not on the strings themselves. Solomonoff used this algorithm, and the code lengths it allows, to define a &quot;universal probability&quot; of a string on which inductive inference of the subsequent digits of the string can be based. Kolmogorov used this theorem to define several functions of strings, including complexity, randomness, and information.</paragraph><paragraph>When Kolmogorov became aware of Solomonoff's work, he acknowledged Solomonoff's priority.<extension extension_name='ref'><template><target>cite journal</target><arg name="last1">Kolmogorov<space/></arg><arg name="first1">A.<space/></arg><arg name="title">Logical basis for information theory and probability theory<space/></arg><arg name="journal">IEEE Transactions on Information Theory<space/></arg><arg name="volume">14</arg><arg name="issue">5<space/></arg><arg name="pages">662–664<space/></arg><arg name="year">1968<space/></arg><arg name="doi">10.1109/TIT.1968.1054210<space/></arg></template></extension><space/>For several years, Solomonoff's work was better known in the Soviet Union than in the Western World. The general consensus in the scientific community, however, was to associate this type of complexity with Kolmogorov, who was concerned with randomness of a sequence, while Algorithmic Probability became associated with Solomonoff, who focused on prediction using his invention of the universal prior probability distribution. The broader area encompassing descriptional complexity and probability is often called Kolmogorov complexity. The computer scientist Ming Li considers this an example of the<space/><link><target>Matthew effect (sociology)</target><part>Matthew effect</part></link>: &quot;... to everyone who has more will be given ...&quot;<extension extension_name='ref'><template><target>Cite book</target><arg name="edition"><space/>2nd
</arg><arg name="publisher"><space/>Springer
</arg><arg name="isbn"><space/>0-387-94868-6
</arg><arg name="last"><space/>Li
</arg><arg name="first"><space/>Ming
</arg><arg name="author2"><space/>Paul Vitanyi
</arg><arg name="title"><space/>An Introduction to Kolmogorov Complexity and Its Applications
</arg><arg name="page">90
</arg><arg name="date"><space/>1997-02-27
</arg></template></extension></paragraph><paragraph>There are several other variants of Kolmogorov complexity or algorithmic information. The most widely used one is based on<space/><link><target>self-delimiting program</target><trail>s</trail></link>, and is mainly due to<space/><link><target>Leonid Levin</target></link><space/>(1974).</paragraph><paragraph>An axiomatic approach to Kolmogorov complexity based on<space/><link><target>Blum axioms</target></link><space/>(Blum 1967) was introduced by Mark Burgin in the paper presented for publication by Andrey Kolmogorov (Burgin 1982).</paragraph><heading level='2'>Basic results</heading><paragraph>In the following discussion, let<space/><italics>K</italics>(<italics>s</italics>) be the complexity of the string<space/><italics>s</italics>.</paragraph><paragraph>It is not hard to see that the minimal description of a string cannot be too much larger than the string itself - the program<space/><xhtml:code>GenerateFixedString</xhtml:code><space/>above that outputs<space/><italics>s</italics><space/>is a fixed amount larger than<space/><italics>s</italics>.</paragraph><paragraph><bold>Theorem</bold>: There is a constant<space/><italics>c</italics><space/>such that</paragraph><list type='ident'><listitem><italics>s</italics>.<space/><italics>K</italics>(<italics>s</italics>) |<italics>s</italics>| +<space/><italics>c</italics>.</listitem></list><heading level='3'>Uncomputability of Kolmogorov complexity</heading><paragraph><bold>Theorem</bold>: There exist strings of arbitrarily large Kolmogorov complexity. Formally: for each<space/><italics>n</italics><space/>, there is a string<space/><italics>s</italics><space/>with<space/><italics>K</italics>(<italics>s</italics>)<space/><italics>n</italics>.<extension extension_name='ref' group="note">However, an<space/><italics>s</italics><space/>with<space/><italics>K</italics>(<italics>s</italics>) =<space/><italics>n</italics><space/>needn't exist for every<space/><italics>n</italics>. For example, if<space/><italics>n</italics><space/>isn't a multiple of 7 bits, no<space/><link><target>ASCII</target></link><space/>program can have a length of exactly<space/><italics>n</italics><space/>bits.</extension></paragraph><paragraph><bold>Proof:</bold><space/>Otherwise all infinitely many possible strings could be generated by the finitely many<extension extension_name='ref' group="note">There are 1 + 2 + 2<xhtml:sup>2</xhtml:sup><space/>+ 2<xhtml:sup>3</xhtml:sup><space/>+ ... + 2<xhtml:sup><italics>n</italics></xhtml:sup><space/>= 2<xhtml:sup><italics>n</italics>+1</xhtml:sup><space/>&amp;minus; 1 different program texts of length up to<space/><italics>n</italics><space/>bits; cf.<space/><link><target>geometric series</target></link>. If program lengths are to be multiples of 7 bits, even fewer program texts exist.</extension><space/>programs with a complexity below<space/><italics>n</italics><space/>bits.</paragraph><paragraph><bold>Theorem</bold>:<space/><italics>K</italics><space/>is not a<space/><link><target>computable function</target></link>. In other words, there is no program which takes a string<space/><italics>s</italics><space/>as input and produces the integer<space/><italics>K</italics>(<italics>s</italics>) as output.</paragraph><paragraph>The following<space/><link><target>indirect proof</target><part>indirect '''proof'''</part></link><space/>uses a simple<space/><link><target>Pascal (programming language)</target><part>Pascal</part></link>-like language to denote programs; for sake of proof simplicity assume its description (i.e. an<space/><link><target>interpreter (computing)</target><part>interpreter</part></link>) to have a length of<space/><template><target>val</target><arg>1400000</arg></template><space/>bits.Assume for contradiction there is a program</paragraph><preblock><preline><space/><space/><bold>function</bold><space/>KolmogorovComplexity(<bold>string</bold><space/>s)</preline></preblock><paragraph>which takes as input a string<space/><italics>s</italics><space/>and returns<space/><italics>K</italics>(<italics>s</italics>); for sake of proof simplicity, assume its length to be<space/><template><target>val</target><arg>7000000000</arg></template><space/>bits.Now, consider the following program of length<space/><template><target>val</target><arg>1288</arg></template><space/>bits:</paragraph><preblock><preline><space/><space/><bold>function</bold><space/>GenerateComplexString()</preline><preline><space/><space/><space/><space/><space/><bold>for</bold><space/>i = 1<space/><bold>to</bold><space/>infinity:</preline><preline><space/><space/><space/><space/><space/><space/><space/><space/><bold>for each</bold><space/>string s<space/><bold>of</bold><space/>length exactly i</preline><preline><space/><space/><space/><space/><space/><space/><space/><space/><space/><space/><space/><bold>if</bold><space/>KolmogorovComplexity(s) &gt;= 8000000000</preline><preline><space/><space/><space/><space/><space/><space/><space/><space/><space/><space/><space/><space/><space/><space/><bold>return</bold><space/>s</preline></preblock><paragraph>Using<space/><xhtml:code>KolmogorovComplexity</xhtml:code><space/>as a subroutine, the program tries every string, starting with the shortest, until it returns a string with Kolmogorov complexity at least<space/><template><target>val</target><arg>8000000000</arg></template><space/>bits,<extension extension_name='ref' group="note">By the previous theorem, such a string exists, hence the<space/><xhtml:code>for</xhtml:code><space/>loop will eventually terminate.</extension><space/>i.e. a string that cannot be produced by any program shorter than<space/><template><target>val</target><arg>8000000000</arg></template><space/>bits. However, the overall length of the above program that produced<space/><italics>s</italics><space/>is only<space/><template><target>val</target><arg>7001401288</arg></template><space/>bits,<extension extension_name='ref' group="note">including the language interpreter and the subroutine code for<space/><xhtml:code>KolmogorovComplexity</xhtml:code></extension><space/>which is a contradiction. (If the code of<space/><xhtml:code>KolmogorovComplexity</xhtml:code><space/>is shorter, the contradiction remains. If it is longer, the constant used in<space/><xhtml:code>GenerateComplexString</xhtml:code><space/>can always be changed appropriately.)<extension extension_name='ref' group="note">If<space/><xhtml:code>KolmogorovComplexity</xhtml:code><space/>has length<space/><italics>n</italics><space/>bits, the constant<space/><italics>m</italics><space/>used in<space/><xhtml:code>GenerateComplexString</xhtml:code><space/>needs to be adapted to satisfy<space/><template><target>nobreak</target><arg>''n'' + {{val|1400000}} + {{val|1218}} + 7·log<sub>10</sub>(''m'')<space/>< ''m''</arg></template>, which is always possible since<space/><italics>m</italics><space/>grows faster than log<xhtml:sub>10</xhtml:sub>(<italics>m</italics>).</extension></paragraph><paragraph>The above proof used a contradiction similar to that of the<space/><link><target>Berry paradox</target></link>: &quot;<xhtml:sub><template><target>color</target><arg>#8080ff</arg><arg>1</arg></template></xhtml:sub>The<space/><xhtml:sub><template><target>color</target><arg>#8080ff</arg><arg>2</arg></template></xhtml:sub>smallest<space/><xhtml:sub><template><target>color</target><arg>#8080ff</arg><arg>3</arg></template></xhtml:sub>positive<space/><xhtml:sub><template><target>color</target><arg>#8080ff</arg><arg>4</arg></template></xhtml:sub>integer<space/><xhtml:sub><template><target>color</target><arg>#8080ff</arg><arg>5</arg></template></xhtml:sub>that<space/><xhtml:sub><template><target>color</target><arg>#8080ff</arg><arg>6</arg></template></xhtml:sub>cannot<space/><xhtml:sub><template><target>color</target><arg>#8080ff</arg><arg>7</arg></template></xhtml:sub>be<space/><xhtml:sub><template><target>color</target><arg>#8080ff</arg><arg>8</arg></template></xhtml:sub>defined<space/><xhtml:sub><template><target>color</target><arg>#8080ff</arg><arg>9</arg></template></xhtml:sub>in<space/><xhtml:sub><template><target>color</target><arg>#8080ff</arg><arg>10</arg></template></xhtml:sub>fewer<space/><xhtml:sub><template><target>color</target><arg>#8080ff</arg><arg>11</arg></template></xhtml:sub>than<space/><xhtml:sub><template><target>color</target><arg>#8080ff</arg><arg>12</arg></template></xhtml:sub>twenty<space/><xhtml:sub><template><target>color</target><arg>#8080ff</arg><arg>13</arg></template></xhtml:sub>English<space/><xhtml:sub><template><target>color</target><arg>#8080ff</arg><arg>14</arg></template></xhtml:sub>words&quot;. It is also possible to show the non-computability of<space/><italics>K</italics><space/>by reduction from the non-computability of the halting problem<space/><italics>H</italics>, since<space/><italics>K</italics><space/>and<space/><italics>H</italics><space/>are<space/><link><target>turing degree#Turing equivalence</target><part>Turing-equivalent</part></link>.<extension extension_name='ref'>Stated without proof in:<space/><link type='external' href='http://www.daimi.au.dk/~bromille/DC05/Kolmogorov.pdf'>&quot;''Course notes for Data Compression - Kolmogorov complexity''&quot;</link>, 2005, P.B. Miltersen, p.7</extension></paragraph><paragraph>There is a corollary, humorously called the &quot;<link><target>full employment theorem</target></link>&quot; in the programming language community, stating that there is no perfect size-optimizing compiler.</paragraph><heading level='3'>Chain rule for Kolmogorov complexity</heading><paragraph><template><target>Main</target><arg><space/>Chain rule for Kolmogorov complexity</arg></template>The chain rule<extension extension_name='ref'><template><target>cite news</target><arg name="first"><space/>A. 
<space/></arg><arg name="last"><space/>Zvonkin 
<space/></arg><arg name="author2">L. Levin 
<space/></arg><arg name="title"><space/>The complexity of finite objects and the development of the concepts of information and randomness by means of the theory of algorithms. 
<space/></arg><arg name="journal"><space/>Russian Mathematical Surveys 
<space/></arg><arg name="volume"><space/>25
<space/></arg><arg name="number"><space/>6
<space/></arg><arg name="pages"><space/>83–124 
<space/></arg><arg name="year"><space/>1970</arg></template></extension><space/>for Kolmogorov complexity states that</paragraph><list type='ident'><listitem><italics>K</italics>(<italics>X</italics>,<italics>Y</italics>) =<space/><italics>K</italics>(<italics>X</italics>) +<space/><italics>K</italics>(<italics>Y</italics>|<italics>X</italics>) +<space/><italics>O</italics>(log(<italics>K</italics>(<italics>X</italics>,<italics>Y</italics>))).</listitem></list><paragraph>It states that the shortest program that reproduces<space/><italics>X</italics><space/>and<space/><italics>Y</italics><space/>is<space/><link><target>Big-O notation</target><part>no more</part></link><space/>than a logarithmic term larger than a program to reproduce<space/><italics>X</italics><space/>and a program to reproduce<space/><italics>Y</italics><space/>given<space/><italics>X</italics>. Using this statement, one can define<space/><link><target>Mutual information#Absolute mutual information</target><part>an analogue of mutual information for Kolmogorov complexity</part></link>.</paragraph><heading level='2'>Compression</heading><paragraph>It is straightforward to compute upper bounds for<space/><italics>K</italics>(<italics>s</italics>) simply<space/><link><target>data compression</target><part>compress</part></link><space/>the string<space/><italics>s</italics><space/>with some method, implement the corresponding decompressor in the chosen language, concatenate the decompressor to the compressed string, and measure the length of the resulting string concretely, the size of a<space/><link><target>self-extracting archive</target></link><space/>in the given language.</paragraph><paragraph>A string<space/><italics>s</italics><space/>is compressible by a number<space/><italics>c</italics><space/>if it has a description whose length does not exceed |<italics>s</italics>|&amp;minus;<italics>c</italics><space/>bits. This is equivalent to saying that<space/><italics>K</italics>(<italics>s</italics>) |<italics>s</italics>|-<italics>c</italics>. Otherwise,<space/><italics>s</italics><space/>is incompressible by<space/><italics>c</italics>. A string incompressible by 1 is said to be simply<space/><italics>incompressible</italics><space/>by the<space/><link><target>pigeonhole principle</target></link>, which applies because every compressed string maps to only one uncompressed string,<space/><link><target>incompressible string</target><trail>s</trail></link><space/>must exist, since there are 2<xhtml:sup><italics>n</italics></xhtml:sup><space/>bit strings of length<space/><italics>n</italics>, but only 2<xhtml:sup><italics>n</italics></xhtml:sup><space/>- 1 shorter strings, that is, strings of length less than<space/><italics>n</italics>, (i.e. with length 0,1,...,''n&amp;nbsp;&amp;minus;&amp;nbsp;1).<extension extension_name='ref' group="note">As there are<space/><template><target>nobr</target><arg name="1">''N''<sub>''L''</sub><space/></arg></template><space/>strings of length<space/><italics>L</italics>, the number of strings of lengths<space/><template><target>nowrap</target><arg name="1">''L''<space/></arg></template><space/>is<space/><template><target>nobr</target><arg>''N''<sub>0</sub><space/>+ ''N''<sub>1</sub><space/>+ ... + ''N''<sub>''n''−1</sub></arg></template><space/>=<space/><template><target>nobr</target><arg>2<sup>0</sup><space/>+ 2<sup>1</sup><space/>+ ... + 2<sup>''n''−1</sup></arg></template>, which is a finite<space/><link><target>geometric series</target></link><space/>with sum<space/><template><target>nobr</target><arg>2<sup>0</sup><space/>+ 2<sup>1</sup><space/>+ ... + 2<sup>''n''−1</sup></arg></template><space/>=<space/><template><target>nobr</target><arg name="1"><space/>2<sup>0</sup><space/>× (1 − 2<sup>''n''</sup>) / (1 − 2)<space/></arg></template>.</extension></paragraph><paragraph>For the same reason, most strings are complex in the sense that they cannot be significantly compressed their<space/><italics>K</italics>(<italics>s</italics>) is not much smaller than |<italics>s</italics>|, the length of<space/><italics>s</italics><space/>in bits. To make this precise, fix a value of<space/><italics>n</italics>. There are 2<xhtml:sup><italics>n</italics></xhtml:sup><space/>bitstrings of length<space/><italics>n</italics>. The<space/><link><target>Uniform distribution (discrete)</target><part>uniform</part></link><space/><link><target>probability</target></link><space/>distribution on the space of these bitstrings assigns exactly equal weight 2<xhtml:sup><italics>n</italics></xhtml:sup><space/>to each string of length<space/><italics>n</italics>.</paragraph><paragraph><bold>Theorem</bold>: With the uniform probability distribution on the space of bitstrings of length<space/><italics>n</italics>, the probability that a string is incompressible by<space/><italics>c</italics><space/>is at least 1 - 2<xhtml:sup><italics>c</italics>+1</xhtml:sup><space/>+ 2<xhtml:sup><italics>n</italics></xhtml:sup>.</paragraph><paragraph>To prove the theorem, note that the number of descriptions of length not exceeding<space/><italics>n</italics>-<italics>c</italics><space/>is given by the geometric series:</paragraph><list type='ident'><listitem>1 + 2 + 2<xhtml:sup>2</xhtml:sup><space/>+ ... + 2<xhtml:sup><italics>n</italics>-<italics>c</italics></xhtml:sup><space/>= 2<xhtml:sup><italics>n</italics>-<italics>c</italics>+1</xhtml:sup><space/>- 1.</listitem></list><paragraph>There remain at least</paragraph><list type='ident'><listitem>2<xhtml:sup><italics>n</italics></xhtml:sup><space/>- 2<xhtml:sup><italics>n</italics>-<italics>c</italics>+1</xhtml:sup><space/>+ 1</listitem></list><paragraph>bitstrings of length<space/><italics>n</italics><space/>that are incompressible by<space/><italics>c</italics>. To determine the probability, divide by 2<xhtml:sup><italics>n</italics></xhtml:sup>.</paragraph><heading level='2'>Chaitin's incompleteness theorem</heading><paragraph><link><target>File:Kolmogorov complexity and computable lower bounds.gif</target><part>thumb</part><part>right</part><part>500px</part><part>Kolmogorov complexity<space/><template><target>color</target><arg>#800000</arg><arg>''K''(''s'')</arg></template>, and two computable lower bound functions<space/><template><target>color</target><arg>#808000</arg><arg><code>prog1(s)</code></arg></template>,<space/><template><target>color</target><arg>#008000</arg><arg><code>prog2(s)</code></arg></template>. The horizontal axis (<link><target>logarithmic scale</target></link>) enumerates all<space/><link><target>string (computer science)</target><part>strings</part></link><space/>''s'', ordered by length; the vertical axis (<link><target>linear scale</target></link>) measures string length in<space/><link><target>bit</target><trail>s</trail></link>. Most strings are incompressible, i.e. their Kolmogorov complexity exceeds their length by a constant amount. 17 compressible strings are shown in the picture, appearing as almost vertical slopes. Due to Chaitin's incompleteness theorem, the output of any program computing a lower bound of the Kolmogorov complexity cannot exceed some fixed limit, which is independent of the input string ''s''.</part></link>We know that, in the set of all possible strings, most strings are complex in the sense that they cannot be described in any significantly &quot;compressed&quot; way. However, it turns out that the fact that a specific string is complex cannot be formally proven, if the complexity of the string is above a certain threshold. The precise formalization is as follows. First, fix a particular<space/><link><target>axiomatic system</target></link><space/><bold>S</bold><space/>for the<space/><link><target>natural number</target><trail>s</trail></link>. The axiomatic system has to be powerful enough so that, to certain assertions<space/><bold>A</bold><space/>about complexity of strings, one can associate a formula<space/><bold>F</bold><xhtml:sub><bold>A</bold></xhtml:sub><space/>in<space/><bold>S</bold>. This association must have the following property:</paragraph><paragraph>If<space/><bold>F</bold><xhtml:sub><bold>A</bold></xhtml:sub><space/>is provable from the axioms of<space/><bold>S</bold>, then the corresponding assertion<space/><bold>A</bold><space/>must be true. This &quot;formalization&quot; can be achieved, either by an artificial encoding such as a<space/><link><target>Gdel numbering</target></link>, or by a formalization which more clearly respects the intended interpretation of<space/><bold>S</bold>.</paragraph><paragraph><bold>Theorem</bold>: There exists a constant<space/><italics>L</italics><space/>(which only depends on the particular axiomatic system and the choice of description language) such that there does not exist a string<space/><italics>s</italics><space/>for which the statement</paragraph><list type='ident'><listitem><italics>K</italics>(<italics>s</italics>)<space/><italics>L</italics><space/>(as formalized in<space/><bold>S</bold>)</listitem></list><paragraph>can be proven within the axiomatic system<space/><bold>S</bold>.</paragraph><paragraph>Note that, by the abundance of nearly incompressible strings, the vast majority of those statements must be true.</paragraph><paragraph>The proof of this result is modeled on a self-referential construction used in<space/><link><target>Berry's paradox</target></link>. The proof is by contradiction. If the theorem were false, then</paragraph><list type='ident'><listitem><bold>Assumption (X)</bold>: For any integer<space/><italics>n</italics><space/>there exists a string<space/><italics>s</italics><space/>for which there is a proof in<space/><bold>S</bold><space/>of the formula &quot;<italics>K</italics>(<italics>s</italics>)&amp;nbsp;&amp;nbsp;<italics>n</italics>&quot; (which we assume can be formalized in<space/><bold>S</bold>).</listitem></list><paragraph>We can find an effective enumeration of all the formal proofs in<space/><bold>S</bold><space/>by some procedure</paragraph><preblock><preline><space/><space/><bold>function</bold><space/>NthProof(<bold>int</bold><space/><italics>n</italics>)</preline></preblock><paragraph>which takes as input<space/><italics>n</italics><space/>and outputs some proof. This function enumerates all proofs. Some of these are proofs for formulas we do not care about here, since every possible proof in the language of<space/><bold>S</bold><space/>is produced for some<space/><italics>n</italics>. Some of these are complexity formulas of the form<space/><italics>K</italics>(<italics>s</italics>)&amp;nbsp;&amp;nbsp;<italics>n</italics><space/>where<space/><italics>s</italics><space/>and<space/><italics>n</italics><space/>are constants in the language of<space/><bold>S</bold>. There is a program</paragraph><preblock><preline><space/><space/><bold>function</bold><space/>NthProofProvesComplexityFormula(<bold>int</bold><space/><italics>n</italics>)</preline></preblock><paragraph>which determines whether the<space/><italics>n</italics>th proof actually proves a complexity formula<space/><italics>K</italics>(<italics>s</italics>)&amp;nbsp;&amp;nbsp;<italics>L</italics>. The strings<space/><italics>s</italics>, and the integer<space/><italics>L</italics><space/>in turn, are computable by programs:</paragraph><preblock><preline><space/><space/><bold>function</bold><space/>StringNthProof(<bold>int</bold><space/><italics>n</italics>)</preline></preblock><preblock><preline><space/><space/><bold>function</bold><space/>ComplexityLowerBoundNthProof(<bold>int</bold><space/><italics>n</italics>)</preline></preblock><paragraph>Consider the following program</paragraph><preblock><preline><space/><space/><bold>function</bold><space/>GenerateProvablyComplexString(<bold>int</bold><space/><italics>n</italics>)</preline><preline><space/><space/><space/><space/><space/><bold>for</bold><space/>i = 1 to infinity:</preline><preline><space/><space/><space/><space/><space/><space/><space/><space/><bold>if</bold><space/>NthProofProvesComplexityFormula(i)<space/><bold>and</bold><space/>ComplexityLowerBoundNthProof(i)<space/><italics>n</italics></preline><preline><space/><space/><space/><space/><space/><space/><space/><space/><space/><space/><space/><bold>return</bold><space/>StringNthProof(<italics>i</italics>)</preline></preblock><paragraph>Given an<space/><italics>n</italics>, this program tries every proof until it finds a string and a proof in the<space/><link><target>formal system</target></link><space/><bold>S</bold><space/>of the formula<space/><italics>K</italics>(<italics>s</italics>)&amp;nbsp;&amp;nbsp;<italics>L</italics><space/>for some<space/><italics>L</italics>&amp;nbsp;&amp;nbsp;<italics>n</italics>. The program terminates by our<space/><bold>Assumption (X)</bold>. Now, this program has a length<space/><italics>U</italics>. There is an integer<space/><italics>n</italics><xhtml:sub>0</xhtml:sub><space/>such that<space/><italics>U</italics>&amp;nbsp;+&amp;nbsp;log<xhtml:sub>2</xhtml:sub>(<italics>n</italics><xhtml:sub>0</xhtml:sub>)&amp;nbsp;+&amp;nbsp;<italics>C</italics>&amp;nbsp;&lt;&amp;nbsp;<italics>n</italics><xhtml:sub>0</xhtml:sub>, where<space/><italics>C</italics><space/>is the overhead cost of</paragraph><preblock><preline><space/><space/><space/><bold>function</bold><space/>GenerateProvablyParadoxicalString()</preline><preline><space/><space/><space/><space/><space/><space/><bold>return</bold><space/>GenerateProvablyComplexString(<italics>n</italics><xhtml:sub>0</xhtml:sub>)</preline></preblock><paragraph>(note that<space/><italics>n</italics><xhtml:sub>0</xhtml:sub><space/>is hard-coded into the above function, and the summand log<xhtml:sub>2</xhtml:sub>(<italics>n</italics><xhtml:sub>0</xhtml:sub>) already allows for its encoding). The program GenerateProvablyParadoxicalString outputs a string<space/><italics>s</italics><space/>for which there exists an<space/><italics>L</italics><space/>such that<space/><italics>K</italics>(<italics>s</italics>)&amp;nbsp;&amp;nbsp;<italics>L</italics><space/>can be formally proved in<space/><bold>S</bold><space/>with<space/><italics>L</italics>&amp;nbsp;&amp;nbsp;<italics>n</italics><xhtml:sub>0</xhtml:sub>. In particular,<space/><italics>K</italics>(<italics>s</italics>)&amp;nbsp;&amp;nbsp;<italics>n</italics><xhtml:sub>0</xhtml:sub><space/>is true. However,<space/><italics>s</italics><space/>is also described by a program of length<space/><italics>U</italics>&amp;nbsp;+&amp;nbsp;log<xhtml:sub>2</xhtml:sub>(<italics>n</italics><xhtml:sub>0</xhtml:sub>)&amp;nbsp;+&amp;nbsp;<italics>C</italics>, so its complexity is less than<space/><italics>n</italics><xhtml:sub>0</xhtml:sub>. This contradiction proves<space/><bold>Assumption (X)</bold><space/>cannot hold.</paragraph><paragraph>Similar ideas are used to prove the properties of<space/><link><target>Chaitin's constant</target></link>.</paragraph><heading level='2'>Minimum message length</heading><paragraph>The<space/><link><target>minimum message length</target></link><space/>principle of statistical and inductive inference and machine learning was developed by<space/><link><target>Chris Wallace (computer scientist)</target><part>C.S. Wallace</part></link><space/>and D.M. Boulton in 1968. MML is<space/><link><target>Bayesian probability</target><part>Bayesian</part></link><space/>(i.e. it incorporates prior beliefs) and information-theoretic. It has the desirable properties of statistical invariance (i.e. the inference transforms with a re-parametrisation, such as from polar coordinates to Cartesian coordinates), statistical consistency (i.e. even for very hard problems, MML will converge to any underlying model) and efficiency (i.e. the MML model will converge to any true underlying model about as quickly as is possible). C.S. Wallace and D.L. Dowe (1999) showed a formal connection between MML and algorithmic information theory (or Kolmogorov complexity).</paragraph><heading level='2'>Kolmogorov randomness</heading><paragraph><italics>Kolmogorov randomness</italics><space/>also called<space/><italics>algorithmic randomness</italics><space/>defines a string (usually of<space/><link><target>bit</target><trail>s</trail></link>) as being<space/><link><target>randomness</target><part>random</part></link><space/>if and only if it is shorter than any<space/><link><target>computer program</target></link><space/>that can produce that string. To make this precise, a<space/><link><target>universal computer</target></link><space/>(or universal Turing machine) must be specified, so that &quot;program&quot; means a program for this universal machine. A random string in this sense is &quot;incompressible&quot; in that it is impossible to &quot;compress&quot; the string into a program whose length is shorter than the length of the string itself. A<space/><link><target>counting argument</target></link><space/>is used to show that, for any universal computer, there is at least one algorithmically random string of each length. Whether any particular string is random, however, depends on the specific universal computer that is chosen.</paragraph><paragraph>This definition can be extended to define a notion of randomness for<space/><italics>infinite</italics><space/>sequences from a finite alphabet. These<space/><link><target>algorithmically random sequence</target><trail>s</trail></link><space/>can be defined in three equivalent ways. One way uses an effective analogue of<space/><link><target>measure theory</target></link>; another uses effective<space/><link><target>Martingale (probability theory)</target><part>martingales</part></link>. The third way defines an infinite sequence to be random if the prefix-free Kolmogorov complexity of its initial segments grows quickly enough - there must be a constant<space/><italics>c</italics><space/>such that the complexity of an initial segment of length<space/><italics>n</italics><space/>is always at least<space/><italics>n</italics>&amp;minus;<italics>c</italics>. This definition, unlike the definition of randomness for a finite string, is not affected by which universal machine is used to define prefix-free Kolmogorov complexity.<extension extension_name='ref'><template><target>Cite journal</target><arg name="last1"><space/>Martin-Löf<space/></arg><arg name="first1"><space/>P.<space/></arg><arg name="last2"><space/></arg><arg name="first2"><space/></arg><arg name="year"><space/>1966<space/></arg><arg name="title"><space/>The definition of random sequences<space/></arg><arg name="journal"><space/>Information and Control<space/></arg><arg name="volume"><space/>9<space/></arg><arg name="issue"><space/>6<space/></arg><arg name="pages"><space/>602–619<space/></arg><arg name="publisher"><space/></arg><arg name="jstor"><space/></arg><arg name="url"><space/>http://statweb.stanford.edu/~cgates/PERSI/Courses/Phil166-266/Martin-Lof.pdf<space/></arg><arg name="format"><space/></arg><arg name="accessdate"><space/></arg><arg name="doi">10.1016/s0019-9958(66)80018-9</arg></template></extension></paragraph><heading level='2'>Relation to entropy</heading><paragraph>For dynamical systems, entropy rate and algorithmic complexity of the trajectories are related by a theorem of Brudno, that the equality K(x;T) = h(T) holds for almost all x.<extension extension_name='ref'><template><target>cite journal</target><arg name="authors">Stefano Galatolo, Mathieu Hoyrup, Cristóbal Rojas<space/></arg><arg name="title">Effective symbolic dynamics, random points, statistical behavior, complexity and entropy<space/></arg><arg name="journal">Information and Computation<space/></arg><arg name="volume">208<space/></arg><arg name="pages">23–41<space/></arg><arg name="year">2010</arg><arg name="url">http://www.loria.fr/~hoyrup/random_ergodic.pdf<space/></arg><arg name="doi">10.1016/j.ic.2009.05.001</arg></template></extension></paragraph><paragraph>It can be shown<extension extension_name='ref'><template><target>cite journal</target><arg name="author">Alexei Kaltchenko<space/></arg><arg name="title">Algorithms for Estimating Information Distance with Application to Bioinformatics and Linguistics<space/></arg><arg name="journal">CoRR<space/></arg><arg name="volume">cs.CC/0404039<space/></arg><arg name="year">2004<space/></arg><arg name="url">http://arxiv.org/pdf/cs.CC/0404039<space/></arg><arg name="url">http://arxiv.org/abs/cs.CC/0404039</arg></template></extension><space/>that for the output of<space/><link><target>Markov information source</target><trail>s</trail></link>, Kolmogorov complexity is related to the<space/><link><target>Entropy (information theory)</target><part>entropy</part></link><space/>of the information source. More precisely, the Kolmogorov complexity of the output of a Markov information source, normalized by the length of the output, converges almost surely (as the length of the output goes to infinity) to the<space/><link><target>Entropy (information theory)</target><part>entropy</part></link><space/>of the source.</paragraph><heading level='2'>Conditional versions</heading><paragraph><template><target>expand section</target><arg name="date">July 2014</arg></template>The conditional [Kolmogorov] complexity of two strings<space/><italics>K(x|y)</italics><space/>is, roughly speaking, defined as the Kolmogorov complexity of<space/><italics>x</italics><space/>given<space/><italics>y</italics><space/>as an auxiliary input to the procedure.<extension extension_name='ref' name="Rissanen2007"><template><target>cite book</target><arg name="author">Jorma Rissanen</arg><arg name="title">Information and Complexity in Statistical Modeling</arg><arg name="year">2007</arg><arg name="publisher">Springer Science & Business Media</arg><arg name="isbn">978-0-387-68812-1</arg><arg name="page">53</arg></template></extension><extension extension_name='ref'><template><target>cite book</target><arg name="author1">Ming Li</arg><arg name="author2">Paul M.B. Vitányi</arg><arg name="title">An Introduction to Kolmogorov Complexity and Its Applications</arg><arg name="year">2009</arg><arg name="publisher">Springer Science & Business Media</arg><arg name="isbn">978-0-387-49820-1</arg><arg name="pages">105–106</arg></template></extension></paragraph><paragraph>There is also a length-conditional complexity<space/><italics>K(x|l(x))</italics>, which is the complexity of<space/><italics>x</italics><space/>given the length of<space/><italics>x</italics><space/>as known/input.<extension extension_name='ref'><template><target>cite book</target><arg name="author1">Ming Li</arg><arg name="author2">Paul M.B. Vitányi</arg><arg name="title">An Introduction to Kolmogorov Complexity and Its Applications</arg><arg name="year">2009</arg><arg name="publisher">Springer Science & Business Media</arg><arg name="isbn">978-0-387-49820-1</arg><arg name="page">119</arg></template></extension></paragraph><heading level='2'>See also</heading><list type='bullet'><listitem><link><target>Berry paradox</target></link></listitem><listitem><link><target>Data compression</target></link></listitem><listitem><link><target>Inductive inference</target></link></listitem><listitem><link><target>Kolmogorov structure function</target></link></listitem><listitem><link><target>List of important publications in theoretical computer science#Algorithmic information theory</target><part>Important publications in algorithmic information theory</part></link></listitem><listitem><link><target>Solomonoff's theory of inductive inference</target></link></listitem><listitem><link><target>Levenshtein distance</target></link></listitem><listitem><link><target>Grammar induction</target></link></listitem></list><heading level='2'>Notes</heading><paragraph><template><target>Reflist</target><arg name="group">note</arg></template></paragraph><heading level='2'>References</heading><paragraph><template><target>Reflist</target><arg name="colwidth">30em</arg></template></paragraph><list type='bullet'><listitem><template><target>cite journal</target><arg name="authorlink">Manuel Blum</arg><arg name="last">Blum<space/></arg><arg name="title">On the size of machines<space/></arg><arg name="journal">Information and Control<space/></arg><arg name="first"><space/>M.<space/></arg><arg name="volume">11<space/></arg><arg name="issue">3<space/></arg><arg name="pages">257<space/></arg><arg name="year">1967<space/></arg><arg name="doi"><space/>10.1016/S0019-9958(67)90546-3<space/></arg></template></listitem><listitem>Brudno, A. Entropy and the complexity of the trajectories of a dynamical system., Transactions of the Moscow Mathematical Society, 2:127{151, 1983.</listitem><listitem>Burgin, M. (1982), &quot;Generalized Kolmogorov complexity and duality in theory of computations&quot;,<space/><italics>Notices of the Russian Academy of Sciences</italics>, v.25, No. 3, pp.&amp;nbsp;19&amp;ndash;23.</listitem><listitem>Cover, Thomas M. and Thomas, Joy A.,<space/><italics>Elements of information theory</italics>, 1st Edition. New York: Wiley-Interscience, 1991. ISBN 0-471-06259-6. 2nd Edition. New York: Wiley-Interscience, 2006. ISBN 0-471-24195-4.</listitem><listitem>Lajos, Rnyai and Gbor, Ivanyos and Rka, Szab,<space/><italics>Algoritmusok</italics>. TypoTeX, 1999. ISBN 963-279-014-6</listitem><listitem><template><target>cite book</target><arg name="author">Li, Ming and Vitányi, Paul</arg><arg name="title">An Introduction to Kolmogorov Complexity and Its Applications</arg><arg name="publisher">Springer</arg><arg name="year">1997<space/></arg><arg name="isbn"><space/>978-0387339986</arg></template><space/><link type='external' href='http://citeseer.ist.psu.edu/li97introduction.html'>First chapter on citeseer</link></listitem><listitem>Yu Manin,<space/><italics>A Course in Mathematical Logic</italics>, Springer-Verlag, 1977. ISBN 978-0-7204-2844-5</listitem><listitem>Sipser, Michael,<space/><italics>Introduction to the Theory of Computation</italics>, PWS Publishing Company, 1997. ISBN 0-534-95097-3.</listitem><listitem><link><target>Chris Wallace (computer scientist)</target><part>Wallace, C. S</part></link>. and Dowe, D. L.,<space/><link type='external' href='http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.321'>Minimum Message Length and Kolmogorov Complexity</link>, Computer Journal, Vol. 42, No. 4, 1999).</listitem></list><heading level='2'>External links</heading><list type='bullet'><listitem><link type='external' href='http://www.kolmogorov.com/'>The Legacy of Andrei Nikolaevich Kolmogorov</link></listitem><listitem><link type='external' href='http://www.cs.umaine.edu/~chaitin/'>Chaitin's online publications</link></listitem><listitem><link type='external' href='http://www.idsia.ch/~juergen/ray.html'>Solomonoff's IDSIA page</link></listitem><listitem><link type='external' href='http://www.idsia.ch/~juergen/kolmogorov.html'>Generalizations of algorithmic information</link><space/>by<space/><link><target>Juergen Schmidhuber</target><part>J. Schmidhuber</part></link></listitem><listitem><link type='external' href='http://homepages.cwi.nl/~paulv/kolmogorov.html'>Ming Li and Paul Vitanyi, An Introduction to Kolmogorov Complexity and Its Applications, 2nd Edition, Springer Verlag, 1997.</link></listitem><listitem><link type='external' href='http://tromp.github.io/cl/cl.html'>Tromp's lambda calculus computer model offers a concrete definition of K()</link></listitem><listitem>Universal AI based on Kolmogorov Complexity ISBN 3-540-22139-5 by<space/><link><target>Marcus Hutter</target><part>M. Hutter</part></link>: ISBN 3-540-22139-5</listitem><listitem><link type='external' href='http://www.csse.monash.edu.au/~dld'>David Dowe</link>'s<space/><link type='external' href='http://www.csse.monash.edu.au/~dld/MML.html'>Minimum Message Length (MML)</link><space/>and<space/><link type='external' href='http://www.csse.monash.edu.au/~dld/Occam.html'>Occam's razor</link><space/>pages.</listitem><listitem>P. Grunwald, M. A. Pitt and I. J. Myung (ed.),<space/><link type='external' href='http://mitpress.mit.edu/catalog/item/default.asp?sid=4C100C6F-2255-40FF-A2ED-02FC49FEBE7C&amp;amp;ttype=2&amp;amp;tid=10478'>Advances in Minimum Description Length: Theory and Applications</link>, M.I.T. Press, April 2005, ISBN 0-262-07262-9.</listitem></list><paragraph><template><target>Compression Methods</target></template></paragraph><paragraph><template><target>DEFAULTSORT:Kolmogorov Complexity</target></template><link><target>Category:Algorithmic information theory</target><part>*</part></link><link><target>Category:Information theory</target><part>*</part></link><link><target>Category:Computability theory</target></link><link><target>Category:Descriptive complexity</target></link><link><target>Category:Measures of complexity</target></link></paragraph></article>