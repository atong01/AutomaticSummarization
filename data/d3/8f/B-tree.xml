<article title='B-tree'><paragraph><template><target>distinguish</target><arg>Binary tree</arg></template><template><target>Infobox data structure</target><arg name="name">B-tree
</arg><arg name="type">tree
</arg><arg name="invented_by">[[Rudolf Bayer]], [[Edward M. McCreight]]
</arg><arg name="invented_year">1972
</arg><arg>
</arg><arg name="space_avg"><space/>O(''n'')
</arg><arg name="space_worst"><space/>O(''n'')
</arg><arg name="search_avg"><space/>O(log ''n'')
</arg><arg name="search_worst"><space/>O(log ''n'')
</arg><arg name="insert_avg"><space/>O(log ''n'')
</arg><arg name="insert_worst"><space/>O(log ''n'')
</arg><arg name="delete_avg"><space/>O(log ''n'')
</arg><arg name="delete_worst"><space/>O(log ''n'')
</arg></template></paragraph><paragraph>In<space/><link><target>computer science</target></link>, a<space/><bold>B-tree</bold><space/>is a self-balancing<space/><link><target>tree data structure</target></link><space/>that keeps data sorted and allows searches, sequential access, insertions, and deletions in<space/><link><target>logarithmic time</target></link>. The B-tree is a generalization of a<space/><link><target>binary search tree</target></link><space/>in that a node can have more than two children<space/><template><target>Harv</target><arg>Comer</arg><arg>1979</arg><arg name="p">123</arg></template>. Unlike<space/><link><target>self-balancing binary search tree</target><trail>s</trail></link>, the B-tree is optimized for systems that read and write large blocks of data. B-trees are a good example of a data structure for external memory. It is commonly used in<space/><link><target>database</target><trail>s</trail></link><space/>and<space/><link><target>filesystem</target><trail>s</trail></link>.</paragraph><heading level='2'>Overview</heading><paragraph><link><target>File:B-tree.svg</target><part>thumb</part><part>400px</part><part>right</part><part>A B-tree of order 2<space/><template><target>Harv</target><arg>Bayer</arg><arg>McCreight</arg><arg>1972</arg></template><space/>or order 5<space/><template><target>Harv</target><arg>Knuth</arg><arg>1998</arg></template>.</part></link>In B-trees, internal (<link><target>Leaf node</target><part>non-leaf</part></link>) nodes can have a variable number of child nodes within some pre-defined range. When data is inserted or removed from a node, its number of child nodes changes. In order to maintain the pre-defined range, internal nodes may be joined or split. Because a range of child nodes is permitted, B-trees do not need re-balancing as frequently as other self-balancing search trees, but may waste some space, since nodes are not entirely full. The lower and upper bounds on the number of child nodes are typically fixed for a particular implementation. For example, in a<space/><link><target>2-3 tree</target><part>2-3 B-tree</part></link><space/>(often simply referred to as a<space/><bold>2-3 tree</bold>), each internal node may have only 2 or 3 child nodes.</paragraph><paragraph>Each internal node of a B-tree will contain a number of<space/><link><target>Unique key</target><part>keys</part></link>. The keys act as separation values which divide its<space/><link><target>subtree</target><trail>s</trail></link>. For example, if an internal node has 3 child nodes (or subtrees) then it must have 2 keys:<space/><italics>a</italics><xhtml:sub>1</xhtml:sub><space/>and<space/><italics>a</italics><xhtml:sub>2</xhtml:sub>. All values in the leftmost subtree will be less than<space/><italics>a</italics><xhtml:sub>1</xhtml:sub>, all values in the middle subtree will be between<space/><italics>a</italics><xhtml:sub>1</xhtml:sub><space/>and<space/><italics>a</italics><xhtml:sub>2</xhtml:sub>, and all values in the rightmost subtree will be greater than<space/><italics>a</italics><xhtml:sub>2</xhtml:sub>.</paragraph><paragraph>Usually, the number of keys is chosen to vary between<space/><extension extension_name='math'>d</extension><space/>and<space/><extension extension_name='math'>2d</extension>, where<space/><extension extension_name='math'>d</extension><space/>is the minimum number of keys, and<space/><extension extension_name='math'>d+1</extension><space/>is the minimum<space/><link><target>Outdegree#Indegree and outdegree</target><part>degree</part></link><space/>or<space/><link><target>branching factor</target></link><space/>of the tree. In practice, the keys take up the most space in a node. The factor of 2 will guarantee that nodes can be split or combined. If an internal node has<space/><extension extension_name='math'>2d</extension><space/>keys, then adding a key to that node can be accomplished by splitting the<space/><extension extension_name='math'>2d</extension><space/>key node into two<space/><extension extension_name='math'>d</extension><space/>key nodes and adding the key to the parent node. Each split node has the required minimum number of keys. Similarly, if an internal node and its neighbor each have<space/><extension extension_name='math'>d</extension><space/>keys, then a key may be deleted from the internal node by combining with its neighbor. Deleting the key would make the internal node have<space/><extension extension_name='math'>d-1</extension><space/>keys; joining the neighbor would add<space/><extension extension_name='math'>d</extension><space/>keys plus one more key brought down from the neighbor's parent. The result is an entirely full node of<space/><extension extension_name='math'>2d</extension><space/>keys.</paragraph><paragraph>The number of branches (or child nodes) from a node will be one more than the number of keys stored in the node. In a 2-3 B-tree, the internal nodes will store either one key (with two child nodes) or two keys (with three child nodes). A B-tree is sometimes described with the parameters<space/><extension extension_name='math'>(d+1)</extension><space/><extension extension_name='math'>(2d+1)</extension><space/>or simply with the highest branching order,<space/><extension extension_name='math'>(2d+1)</extension>.</paragraph><paragraph>A B-tree is kept balanced by requiring that all leaf nodes be at the same depth. This depth will increase slowly as elements are added to the tree, but an increase in the overall depth is infrequent, and results in all leaf nodes being one more node farther away from the root.</paragraph><paragraph>B-trees have substantial advantages over alternative implementations when the time to access the data of a node greatly exceeds the time spent processing that data, because then the cost of accessing the node may be amortized over multiple operations within the node. This usually occurs when the node data are in<space/><link><target>secondary storage</target></link><space/>such as<space/><link><target>hard drive</target><part>disk drives</part></link>. By maximizing the number of keys within each<space/><link><target>internal node</target></link>, the height of the tree decreases and the number of expensive node accesses is reduced. In addition, rebalancing of the tree occurs less often. The maximum number of child nodes depends on the information that must be stored for each child node and the size of a full<space/><link><target>Block (data storage)</target><part>disk block</part></link><space/>or an analogous size in secondary storage. While 2-3 B-trees are easier to explain, practical B-trees using secondary storage need a large number of child nodes to improve performance.</paragraph><heading level='3'>Variants</heading><paragraph>The term<space/><bold>B-tree</bold><space/>may refer to a specific design or it may refer to a general class of designs. In the narrow sense, a B-tree stores keys in its internal nodes but need not store those keys in the records at the leaves. The general class includes variations such as the<space/><link><target>B+ tree</target></link><space/>and the B<xhtml:sup><list type='bullet'><listitem></listitem></list></xhtml:sup><space/>tree.</paragraph><list type='bullet'><listitem>In the<space/><link><target>B+ tree</target></link>, copies of the keys are stored in the internal nodes; the keys and records are stored in leaves; in addition, a leaf node may include a pointer to the next leaf node to speed sequential access<space/><template><target>Harv</target><arg>Comer</arg><arg>1979</arg><arg name="p">129</arg></template>.</listitem><listitem>The B<xhtml:sup><list type='bullet'><listitem></listitem></list></xhtml:sup><space/>tree balances more neighboring internal nodes to keep the internal nodes more densely packed<space/><template><target>Harv</target><arg>Comer</arg><arg>1979</arg><arg name="p">129</arg></template>. This variant requires non-root nodes to be at least 2/3 full instead of 1/2<space/><template><target>Harv</target><arg>Knuth</arg><arg>1998</arg><arg name="p">488</arg></template>. To maintain this, instead of immediately splitting up a node when it gets full, its keys are shared with a node next to it. When both nodes are full, then the two nodes are split into three. Deleting nodes is somewhat more complex than inserting however.</listitem><listitem>B-trees can be turned into<space/><link><target>order statistic tree</target><trail>s</trail></link><space/>to allow rapid searches for the Nth record in key order, or counting the number of records between any two records, and various other related operations.<extension extension_name='ref'><link type='external' href='http://www.chiark.greenend.org.uk/~sgtatham/algorithms/cbtree.html'>Counted B-Trees</link>, retrieved 2010-01-25</extension></listitem></list><heading level='3'>Etymology</heading><paragraph><link><target>Rudolf Bayer</target></link><space/>and<space/><link><target>Edward M. McCreight</target><part>Ed McCreight</part></link><space/>invented the B-tree while working at<space/><link><target>Boeing</target><part>Boeing Research Labs</part></link><space/>in 1971<space/><template><target>Harv</target><arg>Bayer</arg><arg>McCreight</arg><arg>1972</arg></template>, but they did not explain what, if anything, the<space/><italics>B</italics><space/>stands for.<space/><link><target>Douglas Comer</target></link><space/>explains:<extension extension_name='blockquote'>The origin of &quot;B-tree&quot; has never been explained by the authors. As we shall see, &quot;balanced,&quot; &quot;broad,&quot; or &quot;bushy&quot; might apply. Others suggest that the &quot;B&quot; stands for Boeing. Because of his contributions, however, it seems appropriate to think of B-trees as &quot;Bayer&quot;-trees.<space/><template><target>Harv</target><arg>Comer</arg><arg>1979</arg><arg name="p">123 footnote 1</arg></template></extension></paragraph><paragraph><link><target>Donald Knuth</target></link><space/>speculates on the etymology of B-trees in his May, 1980 lecture on the topic &quot;CS144C classroom lecture about disk storage and B-trees&quot;, suggesting the &quot;B&quot; may have originated from Boeing or from Bayer's name.<extension extension_name='ref'><link type='external' href='http://scpd.stanford.edu/knuth/index.jsp'>Knuth's video lectures from Stanford</link></extension></paragraph><paragraph>After a talk at CPM 2013 (24th Annual Symposium on Combinatorial Pattern Matching, Bad Herrenalb, Germany, June 1719, 2013),<space/><link><target>Edward M. McCreight</target><part>Ed McCreight</part></link><space/>answered a question on B-tree's name by<space/><link><target>Martin Farach-Colton</target></link><space/>saying: &quot;Bayer and I were in a lunch time where we get to think a name. And we were, so, B, we were thinking B is, you know We were working for Boeing at the time, we couldn't use the name without talking to lawyers. So, there is a B. It has to do with balance, another B. Bayer was the senior author, who did have several years older than I am and had many more publications than I did. So there is another B. And so, at the lunch table we never did resolve whether there was one of those that made more sense than the rest. What really lives to say is: the more you think about what the B in B-trees means, the better you understand B-trees.&quot;<extension extension_name='ref'><link type='external' href='http://vimeo.com/73481096'>Talk's video</link>, retrieved 2014-01-17</extension></paragraph><heading level='2'>B-tree usage in databases</heading><heading level='3'>Time to search a sorted file</heading><paragraph>Usually, sorting and searching algorithms have been characterized by the number of comparison operations that must be performed using<space/><link><target>Big O notation</target><part>order notation</part></link>. A<space/><link><target>binary search</target></link><space/>of a sorted table with<space/><extension extension_name='math'>N</extension><space/>records, for example, can be done in roughly<space/><extension extension_name='math'>\lceil \log_2 N \rceil</extension><space/>comparisons. If the table had 1,000,000 records, then a specific record could be located with at most 20 comparisons:<space/><extension extension_name='math'>\lceil \log_2 (1,000,000) \rceil = 20<space/></extension>.</paragraph><paragraph>Large databases have historically been kept on disk drives. The time to read a record on a disk drive far exceeds the time needed to compare keys once the record is available. The time to read a record from a disk drive involves a<space/><link><target>seek time</target></link><space/>and a rotational delay. The seek time may be 0 to 20 or more milliseconds, and the rotational delay averages about half the rotation period. For a 7200 RPM drive, the rotation period is 8.33 milliseconds. For a drive such as the Seagate ST3500320NS, the track-to-track seek time is 0.8 milliseconds and the average reading seek time is 8.5 milliseconds.<extension extension_name='ref'>Seagate Technology LLC, Product Manual: Barracuda ES.2 Serial ATA, Rev. F., publication 100468393, 2008<space/><link type='external' href='http://www.seagate.com/staticfiles/support/disc/manuals/NL35%20Series%20&amp;amp;%20BC%20ES%20Series/Barracuda%20ES.2%20Series/100468393f.pdf'></link>, page 6</extension><space/>For simplicity, assume reading from disk takes about 10 milliseconds.</paragraph><paragraph>Naively, then, the time to locate one record out of a million would take 20 disk reads times 10 milliseconds per disk read, which is 0.2 seconds.</paragraph><paragraph>The time won't be that bad because individual records are grouped together in a disk<space/><bold>block</bold>. A disk block might be 16 kilobytes. If each record is 160 bytes, then 100 records could be stored in each block. The disk read time above was actually for an entire block. Once the disk head is in position, one or more disk blocks can be read with little delay. With 100 records per block, the last 6 or so comparisons don't need to do any disk readsthe comparisons are all within the last disk block read.</paragraph><paragraph>To speed the search further, the first 13 to 14 comparisons (which each required a disk access) must be sped up.</paragraph><heading level='3'>An index speeds the search</heading><paragraph>A significant improvement can be made with an<space/><link><target>Index (database)</target><part>index</part></link>. In the example above, initial disk reads narrowed the search range by a factor of two. That can be improved substantially by creating an auxiliary index that contains the first record in each disk block (sometimes called a<space/><link><target>Index (database)#Sparse index</target><part>sparse index</part></link>). This auxiliary index would be 1% of the size of the original database, but it can be searched more quickly. Finding an entry in the auxiliary index would tell us which block to search in the main database; after searching the auxiliary index, we would have to search only that one block of the main databaseat a cost of one more disk read. The index would hold 10,000 entries, so it would take at most 14 comparisons. Like the main database, the last 6 or so comparisons in the aux index would be on the same disk block. The index could be searched in about 8 disk reads, and the desired record could be accessed in 9 disk reads.</paragraph><paragraph>The trick of creating an auxiliary index can be repeated to make an auxiliary index to the auxiliary index. That would make an aux-aux index that would need only 100 entries and would fit in one disk block.</paragraph><paragraph>Instead of reading 14 disk blocks to find the desired record, we only need to read 3 blocks. Reading and searching the first (and only) block of the aux-aux index identifies the relevant block in aux-index. Reading and searching that aux-index block identifies the relevant block in the main database. Instead of 150 milliseconds, we need only 30 milliseconds to get the record.</paragraph><paragraph>The auxiliary indices have turned the search problem from a binary search requiring roughly<space/><extension extension_name='math'>\log_2 N</extension><space/>disk reads to one requiring only<space/><extension extension_name='math'>\log_b N</extension><space/>disk reads where<space/><extension extension_name='math'>b</extension><space/>is the blocking factor (the number of entries per block:<space/><extension extension_name='math'>b = 100</extension><space/>entries per block;<space/><extension extension_name='math'>\log_b 1,000,000 = 3</extension><space/>reads).</paragraph><paragraph>In practice, if the main database is being frequently searched, the aux-aux index and much of the aux index may reside in a<space/><link><target>page cache</target><part>disk cache</part></link>, so they would not incur a disk read.</paragraph><heading level='3'>Insertions and deletions</heading><paragraph>If the database does not change, then compiling the index is simple to do, and the index need never be changed. If there are changes, then managing the database and its index becomes more complicated.</paragraph><paragraph>Deleting records from a database is relatively easy. The index can stay the same, and the record can just be marked as deleted. The database stays in sorted order. If there is a large number of deletions, then the searching and storage become less efficient.</paragraph><paragraph>Insertions can be very slow in a sorted sequential file because room for the inserted record must be made. Inserting a record before the first record in the file requires shifting all of the records down one. Such an operation is just too expensive to be practical. One solution is to leave some space available to be used for insertions. Instead of densely storing all the records in a block, the block can have some free space to allow for subsequent insertions. Those records would be marked as if they were &quot;deleted&quot; records.</paragraph><paragraph>Both insertions and deletions are fast as long as space is available on a block. If an insertion won't fit on the block, then some free space on some nearby block must be found and the auxiliary indices adjusted. The hope is that enough space is nearby such that a lot of blocks do not need to be reorganized. Alternatively, some out-of-sequence disk blocks may be used.</paragraph><heading level='3'>Advantages of B-tree usage for databases</heading><paragraph>The B-tree uses all of the ideas described above. In particular, a B-tree:</paragraph><list type='bullet'><listitem>keeps keys in sorted order for sequential traversing</listitem><listitem>uses a hierarchical index to minimize the number of disk reads</listitem><listitem>uses partially full blocks to speed insertions and deletions</listitem><listitem>keeps the index balanced with an elegant recursive algorithm</listitem></list><paragraph>In addition, a B-tree minimizes waste by making sure the interior nodes are at least half full. A B-tree can handle an arbitrary number of insertions and deletions.</paragraph><heading level='2'>Technical description</heading><heading level='3'>Terminology</heading><paragraph>Unfortunately, the literature on B-trees is not uniform in its terminology<space/><template><target>Harv</target><arg>Folk</arg><arg>Zoellick</arg><arg>1992</arg><arg name="p">362</arg></template>.</paragraph><paragraph><template><target>Harvtxt</target><arg>Bayer</arg><arg>McCreight</arg><arg>1972</arg></template>,<space/><template><target>Harvtxt</target><arg>Comer</arg><arg>1979</arg></template>, and others define the<space/><bold>order</bold><space/>of B-tree as the minimum number of keys in a non-root node.<space/><template><target>Harvtxt</target><arg>Folk</arg><arg>Zoellick</arg><arg>1992</arg></template><space/>points out that terminology is ambiguous because the maximum number of keys is not clear. An order 3 B-tree might hold a maximum of 6 keys or a maximum of 7 keys.<space/><template><target>Harvtxt</target><arg>Knuth</arg><arg>1998</arg><arg name="p">483</arg></template><space/>avoids the problem by defining the<space/><bold>order</bold><space/>to be maximum number of children (which is one more than the maximum number of keys).</paragraph><paragraph>The term<space/><bold>leaf</bold><space/>is also inconsistent.<space/><template><target>Harvtxt</target><arg>Bayer</arg><arg>McCreight</arg><arg>1972</arg></template><space/>considered the leaf level to be the lowest level of keys, but Knuth considered the leaf level to be one level below the lowest keys<space/><template><target>Harv</target><arg>Folk</arg><arg>Zoellick</arg><arg>1992</arg><arg name="p">363</arg></template>. There are many possible implementation choices. In some designs, the leaves may hold the entire data record; in other designs, the leaves may only hold pointers to the data record. Those choices are not fundamental to the idea of a B-tree.<extension extension_name='ref'><template><target>Harvtxt</target><arg>Bayer</arg><arg>McCreight</arg><arg>1972</arg></template><space/>avoided the issue by saying an index element is a (physically adjacent) pair of (<italics>x</italics>,&amp;nbsp;<italics>a</italics>) where<space/><italics>x</italics><space/>is the key, and<space/><italics>a</italics><space/>is some associated information. The associated information might be a pointer to a record or records in a random access, but what it was didn't really matter.<space/><template><target>Harvtxt</target><arg>Bayer</arg><arg>McCreight</arg><arg>1972</arg></template><space/>states, &quot;For this paper the associated information is of no further interest.&quot;</extension></paragraph><paragraph>There are also unfortunate choices like using the variable<space/><italics>k</italics><space/>to represent the number of children when<space/><italics>k</italics><space/>could be confused with the number of keys.</paragraph><paragraph>For simplicity, most authors assume there are a fixed number of keys that fit in a node. The basic assumption is the key size is fixed and the node size is fixed. In practice, variable length keys may be employed<space/><template><target>Harv</target><arg>Folk</arg><arg>Zoellick</arg><arg>1992</arg><arg name="p">379</arg></template>.</paragraph><heading level='3'>Definition</heading><paragraph>According to Knuth's definition, a B-tree of order<space/><italics>m</italics><space/>is a tree which satisfies the following properties:</paragraph><list type='numbered'><listitem>Every node has at most<space/><italics>m</italics><space/>children.</listitem><listitem>Every non-leaf node (except root) has at least &amp;lceil;<template><target>frac</target><arg>''m''</arg><arg>2</arg></template>&amp;rceil; children.</listitem><listitem>The root has at least two children if it is not a leaf node.</listitem><listitem>A non-leaf node with<space/><italics>k</italics><space/>children contains<space/><italics>k</italics>1 keys.</listitem><listitem>All leaves appear in the same level</listitem></list><paragraph>Each internal nodes keys act as separation values which divide its subtrees. For example, if an internal node has 3 child nodes (or subtrees) then it must have 2 keys:<space/><italics>a</italics><xhtml:sub>1</xhtml:sub><space/>and<space/><italics>a</italics><xhtml:sub>2</xhtml:sub>. All values in the leftmost subtree will be less than<space/><italics>a</italics><xhtml:sub>1</xhtml:sub>, all values in the middle subtree will be between<space/><italics>a</italics><xhtml:sub>1</xhtml:sub><space/>and<space/><italics>a</italics><xhtml:sub>2</xhtml:sub>, and all values in the rightmost subtree will be greater than<space/><italics>a</italics><xhtml:sub>2</xhtml:sub>.</paragraph><list type='def'><listitem><defkey><bold>Internal nodes</bold></defkey></listitem></list><list type='ident'><listitem>Internal nodes are all nodes except for leaf nodes and the root node. They are usually represented as an ordered set of elements and child pointers. Every internal node contains a<space/><bold>maximum</bold><space/>of<space/><italics>U</italics><space/>children and a<space/><bold>minimum</bold><space/>of<space/><italics>L</italics><space/>children. Thus, the number of elements is always 1 less than the number of child pointers (the number of elements is between<space/><italics>L</italics>1 and<space/><italics>U</italics>1).<space/><italics>U</italics><space/>must be either 2<italics>L</italics><space/>or 2<italics>L</italics>1; therefore each internal node is at least half full. The relationship between<space/><italics>U</italics><space/>and<space/><italics>L</italics><space/>implies that two half-full nodes can be joined to make a legal node, and one full node can be split into two legal nodes (if theres room to push one element up into the parent). These properties make it possible to delete and insert new values into a B-tree and adjust the tree to preserve the B-tree properties.</listitem></list><list type='def'><listitem><defkey><bold>The root node</bold></defkey></listitem></list><list type='ident'><listitem>The root nodes number of children has the same upper limit as internal nodes, but has no lower limit. For example, when there are fewer than<space/><italics>L</italics>1 elements in the entire tree, the root will be the only node in the tree, with no children at all.</listitem></list><list type='def'><listitem><defkey><bold>Leaf nodes</bold></defkey></listitem></list><list type='ident'><listitem>Leaf nodes have the same restriction on the number of elements, but have no children, and no child pointers.</listitem></list><paragraph>A B-tree of depth<space/><italics>n</italics>+1 can hold about<space/><italics>U</italics><space/>times as many items as a B-tree of depth<space/><italics>n</italics>, but the cost of search, insert, and delete operations grows with the depth of the tree. As with any balanced tree, the cost grows much more slowly than the number of elements.</paragraph><paragraph>Some balanced trees store values only at leaf nodes, and use different kinds of nodes for leaf nodes and internal nodes. B-trees keep values in every node in the tree, and may use the same structure for all nodes. However, since leaf nodes never have children, the B-trees benefit from improved performance if they use a specialized structure.</paragraph><heading level='2'>Best case and worst case heights</heading><paragraph>Let<space/><italics>h</italics><space/>be the height of the classic B-tree. Let<space/><italics>n</italics><space/>&gt; 0 be the number of entries in the tree.<extension extension_name='ref'>If<space/><italics>n</italics><space/>is zero, then no root node is needed, so the height of an empty tree is not well defined.</extension><space/>Let<space/><italics>m</italics><space/>be the maximum number of children a node can have. Each node can have at most<space/><italics>m</italics>1 keys.</paragraph><paragraph>It can be shown (by induction for example) that a B-tree of height<space/><italics>h</italics><space/>with all its nodes completely filled has<space/><italics>n</italics>=<space/><italics>m</italics><xhtml:sup><italics>h</italics></xhtml:sup>&amp;minus;1 entries. Hence, the best case height of a B-tree is:</paragraph><list type='ident'><listitem><extension extension_name='math'>\lceil \log_{m} (n+1) \rceil .</extension></listitem></list><paragraph>Let<space/><italics>d</italics><space/>be the minimum number of children an internal (non-root) node can have. For an ordinary B-tree,<space/><italics>d</italics>=<italics>m</italics>/2.</paragraph><paragraph><template><target>Harvtxt</target><arg>Comer</arg><arg>1979</arg><arg name="p">127</arg></template><space/>and<space/><template><target>Harvtxt</target><arg>Cormen</arg><arg>Leiserson</arg><arg>Rivest</arg><arg>Stein</arg><arg>2001</arg><arg name="pp">383â€“384</arg></template><space/>give the worst case height of a B-tree (where the root node is considered to have height 0) as</paragraph><list type='ident'><listitem><extension extension_name='math'>h \le \left\lfloor \log_{d}\left(\frac{n+1}{2}\right) \right\rfloor .</extension></listitem></list><heading level='2'>Algorithms</heading><paragraph><template><target>confusing</target><arg name="reason">the discussion below uses "element", "value", "key", "separator", and "separation value" to mean essentially the same thing. The terms are not clearly defined. There are some subtle issues at the root and leaves
</arg><arg name="date">February 2012</arg></template></paragraph><heading level='3'>Search</heading><paragraph>Searching is similar to searching a binary search tree. Starting at the root, the tree is recursively traversed from top to bottom. At each level, the search chooses the child pointer (subtree) whose separation values are on either side of the search value.</paragraph><paragraph>Binary search is typically (but not necessarily) used within nodes to find the separation values and child tree of interest.</paragraph><heading level='3'>Insertion</heading><paragraph><link><target>Image:B tree insertion example.png</target><part>thumb</part><part>A B Tree insertion example with each iteration. The nodes of this B tree have at most 3 children (Knuth order 3).</part></link></paragraph><paragraph>All insertions start at a leaf node. To insert a new element, search the tree to find the leaf node where the new element should be added. Insert the new element into that node with the following steps:</paragraph><list type='numbered'><listitem>If the node contains fewer than the maximum legal number of elements, then there is room for the new element. Insert the new element in the node, keeping the node's elements ordered.</listitem><listitem>Otherwise the node is full, evenly split it into two nodes so:<list type='numbered'><listitem>A single median is chosen from among the leaf's elements and the new element.</listitem><listitem>Values less than the median are put in the new left node and values greater than the median are put in the new right node, with the median acting as a separation value.</listitem><listitem>The separation value is inserted in the node's parent, which may cause it to be split, and so on. If the node has no parent (i.e., the node was the root), create a new root above this node (increasing the height of the tree).</listitem></list></listitem></list><paragraph>If the splitting goes all the way up to the root, it creates a new root with a single separator value and two children, which is why the lower bound on the size of internal nodes does not apply to the root. The maximum number of elements per node is<space/><italics>U</italics>1. When a node is split, one element moves to the parent, but one element is added. So, it must be possible to divide the maximum number<space/><italics>U</italics>1 of elements into two legal nodes. If this number is odd, then<space/><italics>U</italics>=2<italics>L</italics><space/>and one of the new nodes contains (<italics>U</italics>2)/2 =<space/><italics>L</italics>1 elements, and hence is a legal node, and the other contains one more element, and hence it is legal too. If<space/><italics>U</italics>1 is even, then<space/><italics>U</italics>=2<italics>L</italics>1, so there are 2<italics>L</italics>2 elements in the node. Half of this number is<space/><italics>L</italics>1, which is the minimum number of elements allowed per node.</paragraph><paragraph>An improved algorithm<space/><template><target>Citation Needed</target><arg name="date">August 2015</arg></template><space/>supports a single pass down the tree from the root to the node where the insertion will take place, splitting any full nodes encountered on the way. This prevents the need to recall the parent nodes into memory, which may be expensive if the nodes are on secondary storage. However, to use this improved algorithm, we must be able to send one element to the parent and split the remaining<space/><italics>U</italics>2 elements into two legal nodes, without adding a new element. This requires<space/><italics>U</italics><space/>= 2<italics>L</italics><space/>rather than<space/><italics>U</italics><space/>= 2<italics>L</italics>1, which accounts for why some textbooks impose this requirement in defining B-trees.</paragraph><heading level='3'>Deletion</heading><paragraph>There are two popular strategies for deletion from a B-tree.</paragraph><list type='numbered'><listitem>Locate and delete the item, then restructure the tree to regain its invariants,<space/><bold>OR</bold></listitem><listitem>Do a single pass down the tree, but before entering (visiting) a node, restructure the tree so that once the key to be deleted is encountered, it can be deleted without triggering the need for any further restructuring</listitem></list><paragraph>The algorithm below uses the former strategy.</paragraph><paragraph>There are two special cases to consider when deleting an element:</paragraph><list type='numbered'><listitem>The element in an internal node is a separator for its child nodes</listitem><listitem>Deleting an element may put its node under the minimum number of elements and children</listitem></list><paragraph>The procedures for these cases are in order below.</paragraph><heading level='4'>Deletion from a leaf node</heading><list type='numbered'><listitem>Search for the value to delete.</listitem><listitem>If the value is in a leaf node, simply delete it from the node.</listitem><listitem>If underflow happens, rebalance the tree as described in section &quot;Rebalancing after deletion&quot; below.</listitem></list><heading level='4'>Deletion from an internal node</heading><paragraph>Each element in an internal node acts as a separation value for two subtrees, therefore we need to find a replacement for separation. Note that the largest element in the left subtree is still less than the separator. Likewise, the smallest element in the right subtree is still greater than the separator. Both of those elements are in leaf nodes, and either one can be the new separator for the two subtrees. Algorithmically described below:</paragraph><list type='numbered'><listitem>Choose a new separator (either the largest element in the left subtree or the smallest element in the right subtree), remove it from the leaf node it is in, and replace the element to be deleted with the new separator.</listitem><listitem>The previous step deleted an element (the new separator) from a leaf node. If that leaf node is now deficient (has fewer than the required number of nodes), then rebalance the tree starting from the leaf node.</listitem></list><heading level='4'>Rebalancing after deletion</heading><paragraph>Rebalancing starts from a leaf and proceeds toward the root until the tree is balanced. If deleting an element from a node has brought it under the minimum size, then some elements must be redistributed to bring all nodes up to the minimum. Usually, the redistribution involves moving an element from a sibling node that has more than the minimum number of nodes. That redistribution operation is called a<space/><bold>rotation</bold>. If no sibling can spare an element, then the deficient node must be<space/><bold>merged</bold><space/>with a sibling. The merge causes the parent to lose a separator element, so the parent may become deficient and need rebalancing. The merging and rebalancing may continue all the way to the root. Since the minimum element count doesn't apply to the root, making the root be the only deficient node is not a problem. The algorithm to rebalance the tree is as follows:<template><target>Citation needed</target><arg name="date">July 2008</arg></template></paragraph><list type='bullet'><listitem>If the deficient node's right sibling exists and has more than the minimum number of elements, then rotate left<list type='numbered'><listitem>Copy the separator from the parent to the end of the deficient node (the separator moves down; the deficient node now has the minimum number of elements)</listitem><listitem>Replace the separator in the parent with the first element of the right sibling (right sibling loses one node but still has at least the minimum number of elements)</listitem><listitem>The tree is now balanced</listitem></list></listitem><listitem>Otherwise, if the deficient node's left sibling exists and has more than the minimum number of elements, then rotate right<list type='numbered'><listitem>Copy the separator from the parent to the start of the deficient node (the separator moves down; deficient node now has the minimum number of elements)</listitem><listitem>Replace the separator in the parent with the last element of the left sibling (left sibling loses one node but still has at least the minimum number of elements)</listitem><listitem>The tree is now balanced</listitem></list></listitem><listitem>Otherwise, if both immediate siblings have only the minimum number of elements, then merge with a sibling sandwiching their separator taken off from their parent<list type='numbered'><listitem>Copy the separator to the end of the left node (the left node may be the deficient node or it may be the sibling with the minimum number of elements)</listitem><listitem>Move all elements from the right node to the left node (the left node now has the maximum number of elements, and the right node empty)</listitem><listitem>Remove the separator from the parent along with its empty right child (the parent loses an element)<list type='bullet'><listitem>If the parent is the root and now has no elements, then free it and make the merged node the new root (tree becomes shallower)</listitem><listitem>Otherwise, if the parent has fewer than the required number of elements, then rebalance the parent</listitem></list></listitem></list></listitem></list><list type='ident'><listitem><xhtml:small><bold>Note</bold>: The rebalancing operations are different for B+ trees (e.g., rotation is different because parent has copy of the key) and B*-tree (e.g., three siblings are merged into two siblings).</xhtml:small></listitem></list><heading level='3'>Sequential access</heading><paragraph>While freshly loaded databases tend to have good sequential behavior, this behavior becomes increasingly difficult to maintain as a database grows, resulting in more random I/O and performance challenges.<extension extension_name='ref'><template><target>cite web</target><arg name="url">http://www.cs.sunysb.edu/~bender/pub/cache-oblivious-btree.ps<space/></arg><arg name="title">Cache Oblivious B-trees<space/></arg><arg name="publisher">State University of New York (SUNY) at Stony Brook<space/></arg><arg name="accessdate">2011-01-17</arg></template></extension></paragraph><heading level='3'>Initial construction</heading><paragraph>In applications, it is frequently useful to build a B-tree to represent a large existing collection of data and then update it incrementally using standard B-tree operations. In this case, the most efficient way to construct the initial B-tree is not to insert every element in the initial collection successively, but instead to construct the initial set of leaf nodes directly from the input, then build the internal nodes from these. This approach to B-tree construction is called<space/><link><target>bulkloading</target></link>. Initially, every leaf but the last one has one extra element, which will be used to build the internal nodes.<template><target>Citation needed</target><arg name="date">July 2008</arg></template></paragraph><paragraph>For example, if the leaf nodes have maximum size 4 and the initial collection is the integers 1 through 24, we would initially construct 4 leaf nodes containing 5 values each and 1 which contains 4 values:</paragraph><table><tablerow><tablecell><table class="wikitable"><tablerow><tablecell>1</tablecell><tablecell>2</tablecell><tablecell>3</tablecell><tablecell>4</tablecell><tablecell>5</tablecell></tablerow></table></tablecell><tablecell><paragraph>|</paragraph><table class="wikitable"><tablerow><tablecell>6</tablecell><tablecell>7</tablecell><tablecell>8</tablecell><tablecell>9</tablecell><tablecell>10</tablecell></tablerow></table></tablecell><tablecell><paragraph>|</paragraph><table class="wikitable"><tablerow><tablecell>11</tablecell><tablecell>12</tablecell><tablecell>13</tablecell><tablecell>14</tablecell><tablecell>15</tablecell></tablerow></table></tablecell><tablecell><paragraph>|</paragraph><table class="wikitable"><tablerow><tablecell>16</tablecell><tablecell>17</tablecell><tablecell>18</tablecell><tablecell>19</tablecell><tablecell>20</tablecell></tablerow></table></tablecell><tablecell><paragraph>|</paragraph><table class="wikitable"><tablerow><tablecell>21</tablecell><tablecell>22</tablecell><tablecell>23</tablecell><tablecell>24</tablecell></tablerow></table></tablecell></tablerow></table><paragraph>We build the next level up from the leaves by taking the last element from each leaf node except the last one. Again, each node except the last will contain one extra value. In the example, suppose the internal nodes contain at most 2 values (3 child pointers). Then the next level up of internal nodes would be:</paragraph><table><tablerow><tablecell colspan="3"><table class="wikitable" align="center"><tablerow><tablecell>5</tablecell><tablecell>10</tablecell><tablecell>15</tablecell></tablerow></table></tablecell><tablecell>&amp;nbsp;</tablecell><tablecell colspan="2"><table class="wikitable" align="center"><tablerow><tablecell>20</tablecell></tablerow></table></tablecell></tablerow><tablerow><tablecell><table class="wikitable"><tablerow><tablecell>1</tablecell><tablecell>2</tablecell><tablecell>3</tablecell><tablecell>4</tablecell></tablerow></table></tablecell><tablecell><paragraph>|</paragraph><table class="wikitable"><tablerow><tablecell>6</tablecell><tablecell>7</tablecell><tablecell>8</tablecell><tablecell>9</tablecell></tablerow></table></tablecell><tablecell><paragraph>|</paragraph><table class="wikitable"><tablerow><tablecell>11</tablecell><tablecell>12</tablecell><tablecell>13</tablecell><tablecell>14</tablecell></tablerow></table></tablecell><tablecell>&amp;nbsp;</tablecell><tablecell><table class="wikitable"><tablerow><tablecell>16</tablecell><tablecell>17</tablecell><tablecell>18</tablecell><tablecell>19</tablecell></tablerow></table></tablecell><tablecell><paragraph>|</paragraph><table class="wikitable"><tablerow><tablecell>21</tablecell><tablecell>22</tablecell><tablecell>23</tablecell><tablecell>24</tablecell></tablerow></table></tablecell></tablerow></table><paragraph>This process is continued until we reach a level with only one node and it is not overfilled. In the example only the root level remains:</paragraph><table><tablerow><tablecell colspan="6"><table class="wikitable" align="center"><tablerow><tablecell>15</tablecell></tablerow></table></tablecell></tablerow><tablerow><tablecell colspan="3"><table class="wikitable" align="center"><tablerow><tablecell>5</tablecell><tablecell>10</tablecell></tablerow></table></tablecell><tablecell>&amp;nbsp;</tablecell><tablecell colspan="2"><table class="wikitable" align="center"><tablerow><tablecell>20</tablecell></tablerow></table></tablecell></tablerow><tablerow><tablecell><table class="wikitable"><tablerow><tablecell>1</tablecell><tablecell>2</tablecell><tablecell>3</tablecell><tablecell>4</tablecell></tablerow></table></tablecell><tablecell><paragraph>|</paragraph><table class="wikitable"><tablerow><tablecell>6</tablecell><tablecell>7</tablecell><tablecell>8</tablecell><tablecell>9</tablecell></tablerow></table></tablecell><tablecell><paragraph>|</paragraph><table class="wikitable"><tablerow><tablecell>11</tablecell><tablecell>12</tablecell><tablecell>13</tablecell><tablecell>14</tablecell></tablerow></table></tablecell><tablecell>&amp;nbsp;</tablecell><tablecell><table class="wikitable"><tablerow><tablecell>16</tablecell><tablecell>17</tablecell><tablecell>18</tablecell><tablecell>19</tablecell></tablerow></table></tablecell><tablecell><paragraph>|</paragraph><table class="wikitable"><tablerow><tablecell>21</tablecell><tablecell>22</tablecell><tablecell>23</tablecell><tablecell>24</tablecell></tablerow></table></tablecell></tablerow></table><heading level='2'>In filesystems</heading><paragraph>In addition to its use in databases, the B-tree is also used in filesystems to allow quick random access to an arbitrary block in a particular file. The basic problem is turning the file block<space/><extension extension_name='math'>i</extension><space/>address into a disk block (or perhaps to a<space/><link><target>cylinder-head-sector</target></link>) address.</paragraph><paragraph>Some operating systems require the user to allocate the maximum size of the file when the file is created. The file can then be allocated as contiguous disk blocks. Converting to a disk block: the operating system just adds the file block address to the starting disk block of the file. The scheme is simple, but the file cannot exceed its created size.</paragraph><paragraph>Other operating systems allow a file to grow. The resulting disk blocks may not be contiguous, so mapping logical blocks to physical blocks is more involved.</paragraph><paragraph><link><target>MS-DOS</target></link>, for example, used a simple<space/><link><target>File Allocation Table</target></link><space/>(FAT). The FAT has an entry for each disk block,<extension extension_name='ref' group="note">For FAT, what is called a &quot;disk block&quot; here is what the FAT documentation calls a &quot;cluster&quot;, which is fixed-size group of one or more contiguous whole physical disk<space/><link><target>Cylinder-head-sector</target><part>sectors</part></link>. For the purposes of this discussion, a cluster has no significant difference from a physical sector.</extension><space/>and that entry identifies whether its block is used by a file and if so, which block (if any) is the next disk block of the same file. So, the allocation of each file is represented as a<space/><link><target>linked list</target></link><space/>in the table. In order to find the disk address of file block<space/><extension extension_name='math'>i</extension>, the operating system (or disk utility) must sequentially follow the file's linked list in the FAT. Worse, to find a free disk block, it must sequentially scan the FAT. For MS-DOS, that was not a huge penalty because the disks and files were small and the FAT had few entries and relatively short file chains. In the<space/><link><target>FAT12</target></link><space/>filesystem (used on floppy disks and early hard disks), there were no more than 4,080<space/><extension extension_name='ref' group="note">Two of these were reserved for special purposes, so only 4078 could actually represent disk blocks (clusters).</extension><space/>entries, and the FAT would usually be resident in memory. As disks got bigger, the FAT architecture began to confront penalties. On a large disk using FAT, it may be necessary to perform disk reads to learn the disk location of a file block to be read or written.</paragraph><paragraph><link><target>TOPS-20</target></link><space/>(and possibly<space/><link><target>TOPS-20#TENEX</target><part>TENEX</part></link>) used a 0 to 2 level tree that has similarities to a B-tree<template><target>Citation needed</target><arg name="date">October 2009</arg></template>. A disk block was 512 36-bit words. If the file fit in a 512 (2<xhtml:sup>9</xhtml:sup>) word block, then the file directory would point to that physical disk block. If the file fit in 2<xhtml:sup>18</xhtml:sup><space/>words, then the directory would point to an aux index; the 512 words of that index would either be NULL (the block isn't allocated) or point to the physical address of the block. If the file fit in 2<xhtml:sup>27</xhtml:sup><space/>words, then the directory would point to a block holding an aux-aux index; each entry would either be NULL or point to an aux index. Consequently, the physical disk block for a 2<xhtml:sup>27</xhtml:sup><space/>word file could be located in two disk reads and read on the third.</paragraph><paragraph>Apple's filesystem<space/><link><target>HFS+</target></link>, Microsoft's<space/><link><target>NTFS</target></link>,<extension extension_name='ref' name="insidewin2kntfs"><template><target>cite web</target><arg name="url"><space/>http://msdn2.microsoft.com/en-us/library/ms995846.aspx
<space/></arg><arg name="title"><space/>Inside Win2K NTFS, Part 1
<space/></arg><arg name="author"><space/>[[Mark Russinovich]]
<space/></arg><arg name="publisher"><space/>[[MSDN|Microsoft Developer Network]]
<space/></arg><arg name="accessdate"><space/>2008-04-18
</arg><arg name="archiveurl"><space/>http://web.archive.org/web/20080413181940/http://msdn2.microsoft.com/en-us/library/ms995846.aspx</arg><arg name="archivedate"><space/>13 April 2008<space/></arg><arg name="deadurl"><space/>no</arg></template></extension><space/>AIX (jfs2) and some<space/><link><target>Linux</target></link><space/>filesystems, such as<space/><link><target>btrfs</target></link><space/>and<space/><link><target>Ext4</target></link>, use B-trees.</paragraph><paragraph>B*-trees are used in the<space/><link><target>Hierarchical File System</target><part>HFS</part></link><space/>and<space/><link><target>Reiser4</target></link><space/><link><target>file system</target><trail>s</trail></link>.</paragraph><heading level='2'>Variations</heading><heading level='3'>Access concurrency</heading><paragraph>Lehman and Yao<extension extension_name='ref'><template><target>cite web</target><arg name="url">http://portal.acm.org/citation.cfm?id</arg><arg name="title">Efficient locking for concurrent operations on B-trees<space/></arg><arg name="doi">10.1145/319628.319663<space/></arg><arg name="publisher">Portal.acm.org<space/></arg><arg name="date"><space/></arg><arg name="accessdate">2012-06-28</arg></template></extension><space/>showed that all read locks could be avoided (and thus concurrent access greatly improved) by linking the tree blocks at each level together with a &quot;next&quot; pointer. This results in a tree structure where both insertion and search operations descend from the root to the leaf. Write locks are only required as a tree block is modified. This maximizes access concurrency by multiple users, an important consideration for databases and/or other B-tree based<space/><link><target>ISAM</target></link><space/>storage methods. The cost associated with this improvement is that empty pages cannot be removed from the btree during normal operations. (However, see<space/><extension extension_name='ref'>http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA232287&amp;Location=U2&amp;doc=GetTRDoc.pdf</extension><space/>for various strategies to implement node merging, and source code at.<extension extension_name='ref'><template><target>cite web</target><arg name="url">https://github.com/malbrain/Btree-source-code</arg><arg name="title">Downloads - high-concurrency-btree - High Concurrency B-Tree code in C - GitHub Project Hosting<space/></arg><arg name="date"><space/></arg><arg name="accessdate">2014-01-27</arg></template></extension>)</paragraph><paragraph>United States Patent 5283894, granted in 1994, appears to show a way to use a 'Meta Access Method'<space/><extension extension_name='ref'><link type='external' href='http://www.freepatentsonline.com/5283894.html'>Lockless Concurrent B+Tree</link></extension><space/>to allow concurrent B+ tree access and modification without locks. The technique accesses the tree 'upwards' for both searches and updates by means of additional in-memory indexes that point at the blocks in each level in the block cache. No reorganization for deletes is needed and there are no 'next' pointers in each block as in Lehman and Yao.</paragraph><heading level='2'>See also</heading><list type='bullet'><listitem><link><target>B+tree</target></link></listitem><listitem><link><target>R-tree</target></link></listitem><listitem><link><target>23 tree</target></link></listitem><listitem><link><target>234 tree</target></link></listitem></list><heading level='2'>Notes</heading><paragraph><template><target>reflist</target><arg>30em</arg><arg name="group">note</arg></template></paragraph><heading level='2'>References</heading><paragraph><template><target>reflist</target><arg>30em</arg></template></paragraph><list type='def'><listitem><defkey>General</defkey></listitem></list><list type='bullet'><listitem><template><target>Citation</target><arg name="last"><space/>Bayer
<space/></arg><arg name="first"><space/>R.
<space/></arg><arg name="author-link"><space/>Rudolf Bayer
<space/></arg><arg name="last2"><space/>McCreight
<space/></arg><arg name="first2"><space/>E.
<space/></arg><arg name="author2-link"><space/>Edward M. McCreight
<space/></arg><arg name="title"><space/>Organization and Maintenance of Large Ordered Indexes
<space/></arg><arg name="journal"><space/>Acta Informatica
<space/></arg><arg name="volume"><space/>1
<space/></arg><arg name="issue"><space/>3
<space/></arg><arg name="pages"><space/>173â€“189
<space/></arg><arg name="date">
<space/></arg><arg name="year"><space/>1972
<space/></arg><arg name="url"><space/>http://www.minet.uni-jena.de/dbis/lehre/ws2005/dbs1/Bayer_hist.pdf
<space/></arg><arg name="doi">10.1007/bf00288683
<space/></arg><arg name="ref"><space/>harv</arg></template></listitem><listitem><template><target>Citation</target><arg name="last"><space/>Comer
<space/></arg><arg name="first"><space/>Douglas
<space/></arg><arg name="author-link"><space/>Douglas Comer
<space/></arg><arg name="title"><space/>The Ubiquitous B-Tree
<space/></arg><arg name="journal"><space/>Computing Surveys
<space/></arg><arg name="volume"><space/>11
<space/></arg><arg name="issue"><space/>2
<space/></arg><arg name="pages"><space/>123â€“137
<space/></arg><arg name="date"><space/>June 1979
<space/></arg><arg name="issn"><space/>0360-0300
<space/></arg><arg name="url">
<space/></arg><arg name="doi"><space/>10.1145/356770.356776
<space/></arg><arg name="ref"><space/>harv</arg></template>.</listitem><listitem><template><target>Citation</target><arg name="last"><space/>Cormen
<space/></arg><arg name="first"><space/>Thomas
<space/></arg><arg name="author-link"><space/>Thomas H. Cormen
<space/></arg><arg name="last2"><space/>Leiserson
<space/></arg><arg name="first2"><space/>Charles
<space/></arg><arg name="author2-link"><space/>Charles E. Leiserson
<space/></arg><arg name="last3"><space/>Rivest
<space/></arg><arg name="first3"><space/>Ronald
<space/></arg><arg name="author3-link"><space/>Ronald L. Rivest
<space/></arg><arg name="last4"><space/>Stein
<space/></arg><arg name="first4"><space/>Clifford
<space/></arg><arg name="author4-link"><space/>Clifford Stein
<space/></arg><arg name="title"><space/>[[Introduction to Algorithms]]
<space/></arg><arg name="place">
<space/></arg><arg name="publisher"><space/>MIT Press and McGraw-Hill
<space/></arg><arg name="year"><space/>2001
<space/></arg><arg name="volume">
<space/></arg><arg name="edition"><space/>Second
<space/></arg><arg name="page">
<space/></arg><arg name="pages"><space/>434â€“454
<space/></arg><arg name="url">
<space/></arg><arg name="doi">
<space/></arg><arg name="isbn"><space/>0-262-03293-7</arg></template>. Chapter 18: B-Trees.</listitem><listitem><template><target>Citation</target><arg name="last">Folk
<space/></arg><arg name="first">Michael J.
<space/></arg><arg name="author-link">
<space/></arg><arg name="last2">Zoellick
<space/></arg><arg name="first2">Bill
<space/></arg><arg name="author2-link">
<space/></arg><arg name="title">File Structures
<space/></arg><arg name="place">
<space/></arg><arg name="publisher">Addison-Wesley
<space/></arg><arg name="year"><space/>1992
<space/></arg><arg name="volume">
<space/></arg><arg name="edition">2nd
<space/></arg><arg name="page">
<space/></arg><arg name="pages">
<space/></arg><arg name="url">
<space/></arg><arg name="doi">
<space/></arg><arg name="isbn"><space/>0-201-55713-4
<space/></arg><arg name="ref">harv</arg></template></listitem><listitem><template><target>Citation</target><arg name="last"><space/>Knuth
<space/></arg><arg name="first"><space/>Donald
<space/></arg><arg name="author-link"><space/>Donald Knuth
<space/></arg><arg name="series"><space/>[[The Art of Computer Programming]]
<space/></arg><arg name="title"><space/>Sorting and Searching
<space/></arg><arg name="place">
<space/></arg><arg name="publisher"><space/>Addison-Wesley
<space/></arg><arg name="year"><space/>1998
<space/></arg><arg name="volume"><space/>Volume 3
<space/></arg><arg name="edition"><space/>Second
<space/></arg><arg name="page">
<space/></arg><arg name="pages">
<space/></arg><arg name="url">
<space/></arg><arg name="doi">
<space/></arg><arg name="isbn"><space/>0-201-89685-0<space/></arg></template>. Section 6.2.4: Multiway Trees, pp.&amp;nbsp;481491. Also, pp.&amp;nbsp;476477 of section 6.2.3 (Balanced Trees) discusses 2-3 trees.</listitem></list><heading level='3'>Original papers</heading><list type='bullet'><listitem><template><target>Citation</target><arg name="last"><space/>Bayer
<space/></arg><arg name="first"><space/>Rudolf
<space/></arg><arg name="author-link"><space/>Rudolf Bayer
<space/></arg><arg name="last2"><space/>McCreight
<space/></arg><arg name="first2"><space/>E.
<space/></arg><arg name="author2-link"><space/>Edward M. McCreight
<space/></arg><arg name="title"><space/>Organization and Maintenance of Large Ordered Indices
<space/></arg><arg name="place">
<space/></arg><arg name="publisher"><space/>Boeing Scientific Research Laboratories
 
<space/></arg><arg name="volume"><space/>Mathematical and Information Sciences Report No. 20
<space/></arg><arg name="edition">
<space/></arg><arg name="page">
<space/></arg><arg name="pages">
<space/></arg><arg name="url">
<space/></arg><arg name="doi">
<space/></arg><arg name="date"><space/>July 1970
<space/></arg><arg name="isbn"><space/></arg></template>.</listitem><listitem><template><target>Citation</target><arg name="last"><space/>Bayer
<space/></arg><arg name="first"><space/>Rudolf
<space/></arg><arg name="author-link"><space/>Rudolf Bayer
<space/></arg><arg name="title"><space/>Binary B-Trees for Virtual Memory
<space/></arg><arg name="series"><space/>Proceedings of 1971 ACM-SIGFIDET Workshop on Data Description, Access and Control
<space/></arg><arg name="year"><space/>1971
<space/></arg><arg name="pages">
<space/></arg><arg name="place"><space/>San Diego, California
<space/></arg><arg name="publisher">
<space/></arg><arg name="url">
<space/></arg><arg name="doi">
</arg></template>.</listitem></list><heading level='2'>External links</heading><paragraph><template><target>externallinks</target><arg name="date">October 2015</arg></template></paragraph><list type='bullet'><listitem><link type='external' href='http://www.youtube.com/watch?v=I22wEC1tTGo'>B-tree lecture</link><space/>by David Scot Taylor, SJSU</listitem><listitem><link type='external' href='http://slady.net/java/bt/view.php'>B-Tree animation applet</link><space/>by slady</listitem><listitem><link type='external' href='http://www.scholarpedia.org/article/B-tree_and_UB-tree'>B-tree and UB-tree on Scholarpedia</link><space/>Curator: Dr Rudolf Bayer</listitem><listitem><link type='external' href='http://www.bluerwhite.org/btree'>B-Trees: Balanced Tree Data Structures</link></listitem><listitem><link type='external' href='http://www.nist.gov/dads/HTML/btree.html'>NIST's Dictionary of Algorithms and Data Structures: B-tree</link></listitem><listitem><link type='external' href='http://cis.stvincent.edu/html/tutorials/swd/btree/btree.html'>B-Tree Tutorial</link></listitem><listitem><link type='external' href='http://www.boilerbay.com/infinitydb/TheDesignOfTheInfinityDatabaseEngine.htm'>The InfinityDB BTree implementation</link></listitem><listitem><link type='external' href='http://supertech.csail.mit.edu/cacheObliviousBTree.html'>Cache Oblivious B(+)-trees</link></listitem><listitem><link type='external' href='http://www.nist.gov/dads/HTML/bstartree.html'>Dictionary of Algorithms and Data Structures entry for B*-tree</link></listitem><listitem><link type='external' href='http://opendatastructures.org/versions/edition-0.1g/ods-python/14_2_B_Trees.html'>Open Data Structures - Section 14.2 - B-Trees</link></listitem><listitem><link type='external' href='http://www.chiark.greenend.org.uk/~sgtatham/algorithms/cbtree.html'>Counted B-Trees</link></listitem><listitem><link type='external' href='http://sop.codeplex.com'>B-Tree .Net, a modern, virtualized RAM &amp; Disk implementation</link></listitem></list><paragraph><template><target>CS-Trees</target></template><template><target>Data structures</target></template></paragraph><paragraph><template><target>DEFAULTSORT:B-Tree</target></template><link><target>Category:1971 introductions</target></link><link><target>Category:B-tree</target></link><link><target>Category:Database index techniques</target></link></paragraph></article>