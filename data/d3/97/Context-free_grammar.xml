<article title='Context-free_grammar'><paragraph><template><target>refimprove</target><arg name="date">February 2012</arg></template>In<space/><link><target>formal language</target></link><space/>theory, a<space/><bold>context-free grammar</bold><space/>(<bold>CFG</bold>)is a<space/><link><target>formal grammar</target></link><space/>in which every<space/><link><target>Production (computer science)</target><part>production rule</part></link><space/>is of the form</paragraph><list type='ident'><listitem><italics>V</italics><space/>&amp;rarr;<space/><italics>w</italics></listitem></list><paragraph>where<space/><italics>V</italics><space/>is a<space/><italics>single</italics><space/><link><target>nonterminal</target></link><space/>symbol, and<space/><italics>w</italics><space/>is a string of<space/><link><target>Terminal and nonterminal symbols</target><part>terminal</part><trail>s</trail></link><space/>and/or nonterminals (<italics>w</italics><space/>can be empty). A formal grammar is considered &quot;context free&quot; when its production rules can be applied regardless of the context of a nonterminal. No matter which symbols surround it, the single nonterminal on the left hand side can always be replaced by the right hand side. This is what distinguishes it from a<space/><link><target>context-sensitive grammar</target></link>.</paragraph><paragraph>Such a grammar has long lists of words, and also rules on what types of words can be added in what order. Higher rules combine several lower rules to make a sentence. Such sentences will be grammatically correct, but may not have any meaning.Each rule has its own symbol, which can be replaced with symbols representing lower rules, which can be replaced with words.<space/></paragraph><paragraph>This can also be done in reverse to check if a sentence is grammatically correct.</paragraph><paragraph><link><target>formal language</target><part>Language</part><trail>s</trail></link><space/>generated by context-free grammars are known as<space/><link><target>context-free language</target><trail>s</trail></link><space/>(CFL). Different context-free grammars can generate the same context-free language. It is important to distinguish properties of the language (intrinsic properties) from properties of a particular grammar (extrinsic properties). The<space/><link><target>#Language equality</target><part>language equality</part></link><space/>question (do two given context-free grammars generate the same language?) is<space/><link><target>Decidability (logic)</target><part>undecidable</part></link>.</paragraph><paragraph>Context-free grammars arise in<space/><link><target>linguistics</target></link><space/>where they are used to describe the structure of sentences and words in<space/><link><target>natural language</target></link>, and they were in fact invented by the linguist<space/><link><target>Noam Chomsky</target></link><space/>for this purpose, but have not really lived up to their original expectation. By contrast, in<space/><link><target>computer science</target></link>, as the use of recursively defined concepts increased, they were used more and more. In an early application, grammars are used to describe the structure of<space/><link><target>programming language</target><trail>s</trail></link>. In a newer application, they are used in an essential part of the<space/><link><target>Extensible Markup Language</target></link><space/>(XML) called the<space/><italics><link><target>Document Type Definition</target></link></italics>.<extension extension_name='ref'><italics>Introduction to Automata Theory, Languages, and Computation</italics>, John E. Hopcroft, Rajeen Motwani, Jeffrey D. Ullman, Addison Wesley, 2001, p.191</extension></paragraph><paragraph>In<space/><link><target>linguistics</target></link>, some authors use the term<space/><bold><link><target>phrase structure grammar</target></link></bold><space/>to refer to context-free grammars, whereby phrase structure grammars are distinct from<space/><link><target>dependency grammar</target><trail>s</trail></link>. In<space/><link><target>computer science</target></link>, a popular notation for context-free grammars is<space/><link><target>BackusNaur Form</target></link>, or<space/><italics>BNF</italics>.</paragraph><heading level='2'>Background</heading><paragraph>Since the time of<space/><link><target>Pini</target></link>, at least, linguists have described the<space/><link><target>grammar</target><trail>s</trail></link><space/>of languages in terms of their block structure, and described how sentences are<space/><link><target>recursion</target><part>recursively</part></link><space/>built up from smaller phrases, and eventually individual words or word elements. An essential property of these block structures is that logical units never overlap. For example, the sentence:</paragraph><list type='ident'><listitem>John, whose blue car was in the garage, walked to the grocery store.</listitem></list><paragraph>can be logically parenthesized as follows:</paragraph><list type='ident'><listitem>(John, ((whose blue car) (was (in the garage))), (walked (to (the grocery store)))).</listitem></list><paragraph>A context-free grammar provides a simple and mathematically precise mechanism for describing the methods by which phrases in some natural language are built from smaller blocks, capturing the &quot;block structure&quot; of sentences in a natural way. Its simplicity makes the formalism amenable to rigorous mathematical study. Important features of natural language syntax such as<space/><link><target>agreement (linguistics)</target><part>agreement</part></link><space/>and<space/><link><target>reference</target></link><space/>are not part of the context-free grammar, but the basic recursive structure of sentences, the way in which clauses nest inside other clauses, and the way in which lists of adjectives and adverbs are swallowed by nouns and verbs, is described exactly.</paragraph><paragraph>The formalism of context-free grammars was developed in the mid-1950s by<space/><link><target>Noam Chomsky</target></link>,<extension extension_name='ref' name="hu106"><template><target>harvtxt</target><arg>Hopcroft</arg><arg>Ullman</arg><arg>1979</arg></template>, p.&amp;nbsp;106.</extension><space/>and also their<space/><link><target>Chomsky hierarchy</target><part>classification as a special type</part></link><space/>of<space/><link><target>formal grammar</target></link><space/>(which he called<space/><link><target>phrase-structure grammar</target><trail>s</trail></link>).<extension extension_name='ref' name="chomsky1956"><template><target>citation</target><arg name="last"><space/>Chomsky<space/></arg><arg name="first"><space/>Noam<space/></arg><arg name="authorlink">
<space/></arg><arg name="title"><space/>Three models for the description of language
<space/></arg><arg name="journal"><space/>Information Theory, IEEE Transactions
<space/></arg><arg name="volume"><space/>2<space/></arg><arg name="issue"><space/>3<space/></arg><arg name="pages"><space/>113â€“124
<space/></arg><arg name="publisher"><space/></arg><arg name="date"><space/>Sep 1956
<space/></arg><arg name="url"><space/>http://ieeexplore.ieee.org/iel5/18/22738/01056813.pdf?isnumber</arg><arg name="archiveurl"><space/>http://www.chomsky.info/articles/195609--.pdf
<space/></arg><arg name="archivedate"><space/>2013-10-18
<space/></arg><arg name="doi"><space/>10.1109/TIT.1956.1056813</arg><arg name="id"><space/></arg><arg name="accessdate"><space/>2007-06-18</arg></template></extension><space/>What Chomsky called a phrase structure grammar is also known now as a constituency grammar, whereby constituency grammars stand in contrast to<space/><link><target>dependency grammar</target><trail>s</trail></link>. In Chomsky's<space/><link><target>generative grammar</target></link><space/>framework, the syntax of natural language was described by context-free rules combined with transformation rules.</paragraph><paragraph>Block structure was introduced into computer<space/><link><target>programming language</target><trail>s</trail></link><space/>by the<space/><link><target>Algol (programming language)</target><part>Algol</part></link><space/>project (19571960), which, as a consequence, also featured a context-free grammar to describe the resulting Algol syntax. This became a standard feature of computer languages, and the notation for grammars used in concrete descriptions of computer languages came to be known as<space/><link><target>Backus-Naur Form</target></link>, after two members of the Algol language design committee.<extension extension_name='ref' name="hu106"></extension><space/>The &quot;block structure&quot; aspect that context-free grammars capture is so fundamental to grammar that the terms syntax and grammar are often identified with context-free grammar rules, especially in computer science. Formal constraints not captured by the grammar are then considered to be part of the &quot;semantics&quot; of the language.</paragraph><paragraph>Context-free grammars are simple enough to allow the construction of efficient<space/><link><target>list of algorithms#Parsing</target><part>parsing algorithm</part><trail>s</trail></link><space/>which, for a given string, determine whether and how it can be generated from the grammar. An<space/><link><target>Earley parser</target></link><space/>is an example of such an algorithm, while the widely used<space/><link><target>LR parser</target><part>LR</part></link><space/>and<space/><link><target>LL parser</target><trail>s</trail></link><space/>are simpler algorithms that deal only with more restrictive subsets of context-free grammars.</paragraph><heading level='2'>Formal definitions</heading><paragraph>A context-free grammar<space/><italics>G</italics><space/>is defined by the 4-<link><target>tuple</target></link>:<extension extension_name='ref'>The notation here is that of<space/><template><target>harvtxt</target><arg>Sipser</arg><arg>1997</arg></template>, p.&amp;nbsp;94.<space/><template><target>harvtxt</target><arg>Hopcroft</arg><arg>Ullman</arg><arg>1979</arg></template><space/>(p.&amp;nbsp;79) define context-free grammars as 4-tuples in the same way, but with different variable names.</extension></paragraph><paragraph><extension extension_name='math'>G = (V\,, \Sigma\,, R\,, S\,)</extension>where</paragraph><list type='numbered'><listitem><extension extension_name='math'>V\,<space/></extension><space/>is a finite set; each element<space/><extension extension_name='math'><space/>v\in V</extension><space/>is called<space/><italics>a non-terminal character</italics><space/>or a<space/><italics>variable</italics>. Each variable represents a different type of phrase or clause in the sentence. Variables are also sometimes called syntactic categories. Each variable defines a sub-language of the language defined by<space/><extension extension_name='math'>G\,<space/></extension>.</listitem><listitem><extension extension_name='math'>\Sigma\,</extension><space/>is a finite set of<space/><italics>terminal</italics>s, disjoint from<space/><extension extension_name='math'>V\,</extension>, which make up the actual content of the sentence. The set of terminals is the alphabet of the language defined by the grammar<space/><extension extension_name='math'>G\,<space/></extension>.</listitem><listitem><extension extension_name='math'>R\,</extension><space/>is a finite relation from<space/><extension extension_name='math'>V\,</extension><space/>to<space/><extension extension_name='math'>(V\cup\Sigma)^{*}</extension>, where the asterisk represents the<space/><link><target>Kleene star</target></link><space/>operation. The members of<space/><extension extension_name='math'>R\,</extension><space/>are called the<space/><italics>(rewrite) rule</italics>s or<space/><italics>production</italics>s of the grammar. (also commonly symbolized by a<space/><extension extension_name='math'>P\,</extension>)</listitem><listitem><extension extension_name='math'>S\,</extension><space/>is the start variable (or start symbol), used to represent the whole sentence (or program). It must be an element of<space/><extension extension_name='math'>V\,</extension>.</listitem></list><heading level='3'>Production rule notation</heading><paragraph>A<space/><link><target>Formal grammar#The syntax of grammars</target><part>production rule</part></link><space/>in<space/><extension extension_name='math'>R\,</extension><space/>is formalized mathematically as a pair<space/><extension extension_name='math'>(\alpha, \beta)\in R</extension>, where<space/><extension extension_name='math'>\alpha \in V</extension><space/>is a non-terminal and<space/><extension extension_name='math'>\beta \in (V\cup\Sigma)^{*}</extension><space/>is a<space/><link><target>string (computer science)</target><part>string</part></link><space/>of variables and/or terminals; rather than using<space/><link><target>ordered pair</target></link><space/>notation, production rules are usually written using an arrow operator with<space/><extension extension_name='math'>\alpha</extension><space/>as its left hand side and<space/><extension extension_name='math'>\beta</extension><space/>as its right hand side:<extension extension_name='math'>\alpha\rightarrow\beta</extension>.</paragraph><paragraph>It is allowed for<space/><extension extension_name='math'>\beta</extension><space/>to be the<space/><link><target>empty string</target></link>, and in this case it is customary to denote it by . The form<space/><extension extension_name='math'>\alpha\rightarrow\varepsilon</extension><space/>is called an -production.<extension extension_name='ref'><template><target>harvtxt</target><arg>Hopcroft</arg><arg>Ullman</arg><arg>1979</arg></template>, pp. 9092.</extension></paragraph><paragraph>It is common to list all right-hand sides for the same left-hand side on the same line, using | (the<space/><link><target>pipe symbol</target></link>) to separate them. Rules<space/><extension extension_name='math'>\alpha\rightarrow \beta_1</extension><space/>and<space/><extension extension_name='math'>\alpha\rightarrow\beta_2</extension><space/>can hence be written as<space/><extension extension_name='math'>\alpha\rightarrow\beta_1\mid\beta_2</extension>. In this case,<space/><extension extension_name='math'>\beta_1</extension><space/>and<space/><extension extension_name='math'>\beta_2</extension><space/>is called the first and second alternative, respectively.</paragraph><heading level='3'>Rule application</heading><paragraph>For any strings<space/><extension extension_name='math'>u, v\in (V\cup\Sigma)^{*}</extension>, we say<space/><extension extension_name='math'>u\,</extension><space/>directly yields<space/><extension extension_name='math'>v\,</extension>, written as<space/><extension extension_name='math'>u\Rightarrow v\,</extension>, if<space/><extension extension_name='math'>\exists (\alpha, \beta)\in R</extension><space/>with<space/><extension extension_name='math'>\alpha \in V</extension><space/>and<space/><extension extension_name='math'>u_{1}, u_{2}\in (V\cup\Sigma)^{*}</extension><space/>such that<space/><extension extension_name='math'>u\,=u_{1}\alpha u_{2}</extension><space/>and<space/><extension extension_name='math'>v\,=u_{1}\beta u_{2}</extension>. Thus,<space/><extension extension_name='math'>\! v</extension><space/>is a result of applying the rule<space/><extension extension_name='math'>\! (\alpha, \beta)</extension><space/>to<space/><extension extension_name='math'>\! u</extension>.</paragraph><heading level='3'>Repetitive rule application</heading><paragraph>For any strings<space/><extension extension_name='math'>u, v\in (V\cup\Sigma)^{*},<space/></extension><space/>we say<space/><extension extension_name='math'>u</extension><space/><bold>yields</bold><space/><extension extension_name='math'>v</extension>, written as<space/><extension extension_name='math'>u\stackrel{*}{\Rightarrow} v</extension><space/>(or<space/><extension extension_name='math'>u\Rightarrow\Rightarrow v\,</extension><space/>in some textbooks), if<space/><extension extension_name='math'>\exists k\geq 1\, \exists \, u_{1}, \cdots, u_{k}\in (V\cup\Sigma)^{*}</extension><space/>such that<space/><extension extension_name='math'>u = \, u_{1} \Rightarrow u_{2} \Rightarrow \cdots \Rightarrow u_{k} \, = v</extension>. In this case, if<space/><extension extension_name='math'>k\geq 2</extension><space/>(i.e.,<space/><extension extension_name='math'>u \neq v</extension>), the relation<space/><extension extension_name='math'>u\stackrel{+}{\Rightarrow} v</extension><space/>holds. In other words,<space/><extension extension_name='math'>(\stackrel{*}{\Rightarrow})</extension><space/>and<space/><extension extension_name='math'>(\stackrel{+}{\Rightarrow})</extension><space/>are the<space/><link><target>reflexive transitive closure</target></link><space/>(allowing a word to yield itself) and the<space/><link><target>transitive closure</target></link><space/>(requiring at least one step) of<space/><extension extension_name='math'>(\Rightarrow)</extension>, respectively.</paragraph><heading level='3'>Context-free language</heading><paragraph>The language of a grammar<space/><extension extension_name='math'>G = (V\,, \Sigma\,, R\,, S\,)</extension><space/>is the set</paragraph><list type='ident'><listitem><extension extension_name='math'>L(G) = \{ w\in\Sigma^{*} : S\stackrel{*}{\Rightarrow} w\}</extension></listitem></list><paragraph>A language<space/><extension extension_name='math'>L\,</extension><space/>is said to be a context-free language (CFL), if there exists a CFG<space/><extension extension_name='math'>G\,</extension>, such that<space/><extension extension_name='math'>L\,=\,L(G)</extension>.</paragraph><heading level='3'>Proper CFGs</heading><paragraph>A context-free grammar is said to be<space/><italics>proper</italics>,<extension extension_name='ref'><template><target>citation</target><arg name="last"><space/>Nijholt<space/></arg><arg name="first"><space/>Anton
<space/></arg><arg name="isbn"><space/>3-540-10245-0
<space/></arg><arg name="mr"><space/>590047
<space/></arg><arg name="page"><space/>8
<space/></arg><arg name="publisher"><space/>Springer
<space/></arg><arg name="series"><space/>Lecture Notes in Computer Science
<space/></arg><arg name="title"><space/>Context-free grammars: covers, normal forms, and parsing
<space/></arg><arg name="volume"><space/>93
<space/></arg><arg name="year"><space/>1980</arg></template>.</extension><space/>if it has</paragraph><list type='bullet'><listitem>no<space/><link><target>unreachable symbol</target><trail>s</trail></link>:<space/><extension extension_name='math'>\forall N \in V: \exists \alpha,\beta \in (V\cup\Sigma)^*: S \stackrel{*}{\Rightarrow} \alpha{N}\beta</extension></listitem><listitem>no<space/><link><target>unproductive symbol</target><trail>s</trail></link>:<space/><extension extension_name='math'>\forall N \in V: \exists w \in \Sigma^*: N \stackrel{*}{\Rightarrow} w</extension></listitem><listitem>no -productions:<space/><extension extension_name='math'>\neg\exists N \in V: (N, \varepsilon) \in R</extension></listitem><listitem>no cycles:<space/><extension extension_name='math'>\neg\exists N \in V: N \stackrel{+}{\Rightarrow} N</extension></listitem></list><paragraph>Every context-free grammar can be effectively transformed into a<space/><link><target>weak equivalence (formal languages)</target><part>weakly equivalent</part></link><space/>one without unreachable symbols,<extension extension_name='ref'>Hopcroft &amp; Ullman (1979), p.88, Lemma 4.1</extension><space/>a weakly equivalent one without unproductive symbols,<extension extension_name='ref'>Hopcroft &amp; Ullman (1979), p.89, Lemma 4.2</extension><space/>and a weakly equivalent one without cycles.<extension extension_name='ref'>This is a consequence of the unit-production elimination theorem in Hopcroft &amp; Ullman (1979), p.91, Theorem 4.4</extension>Every context-free grammar not producing can be effectively transformed into a weakly equivalent one without -productions;<extension extension_name='ref'>Hopcroft &amp; Ullman (1979), p.91, Theorem 4.4</extension><space/>altogether, every such grammar can be effectively transformed into a weakly equivalent proper CFG.</paragraph><heading level='3'>Example</heading><paragraph>The grammar<space/><extension extension_name='math'>G = (\{S\} , \{a, b\}, P, S)</extension>, with productions</paragraph><preblock><preline></preline></preblock><list type='ident'><listitem>S &amp;rarr; aSa,</listitem><listitem>S &amp;rarr; bSb,</listitem><listitem>S &amp;rarr; ,</listitem></list><preblock><preline></preline></preblock><paragraph>is context-free. It is not proper since it includes an -production. A typical derivation in this grammar is</paragraph><list type='ident'><listitem>S &amp;rarr; aSa &amp;rarr; aaSaa &amp;rarr; aabSbaa &amp;rarr; aabbaa.</listitem></list><paragraph>This makes it clear that<space/><extension extension_name='math'>L(G) = \{ww^R:w\in\{a,b\}^*\}</extension>. The language is context-free, however it can be proved that it is not<space/><link><target>Regular language</target><part>regular</part></link>.</paragraph><heading level='2'>Examples</heading><paragraph><template><target>unreferenced section</target><arg name="date">June 2013</arg></template></paragraph><heading level='3'>Well-formed parentheses</heading><paragraph>The canonical example of a context free grammar is parenthesis matching, which is representative of the general case. There are two terminal symbols &quot;(&quot; and &quot;)&quot; and one nonterminal symbol S. The production rules are</paragraph><list type='ident'><listitem>S &amp;rarr; SS</listitem><listitem>S &amp;rarr; (S)</listitem><listitem>S &amp;rarr; ()</listitem></list><paragraph>The first rule allows the S symbol to multiply; the second rule allows the S symbol to become enclosed by matching parentheses; and the third rule terminates the recursion.</paragraph><heading level='3'>Well-formed nested parentheses and square brackets</heading><paragraph>A second canonical example is two different kinds of matching nested parentheses, described by the productions:</paragraph><list type='ident'><listitem>S &amp;rarr; SS</listitem><listitem>S &amp;rarr; ()</listitem><listitem>S &amp;rarr; (S)</listitem><listitem>S &amp;rarr; []</listitem><listitem>S &amp;rarr; [S]</listitem></list><paragraph>with terminal symbols [ ] ( ) and nonterminal S.</paragraph><paragraph>The following sequence can be derived in that grammar:</paragraph><list type='ident'><listitem>([ [ [ ()() [ ][ ] ] ]([ ]) ])</listitem></list><paragraph>However, there is no context-free grammar for generating all sequences of two different types of parentheses, each separately balanced disregarding the other, but where the two types need not nest inside one another, for example:</paragraph><list type='ident'><listitem>[ ( ] )</listitem></list><paragraph>or</paragraph><list type='ident'><listitem>[ [ [ [(((( ] ] ] ]))))(([ ))(([ ))([ )( ])( ])( ])</listitem></list><heading level='3'>A regular grammar</heading><paragraph>Every regular grammar is context-free, but not all context-free grammars are regular. The following context-free grammar, however, is also regular.</paragraph><list type='ident'><listitem>S &amp;rarr; a</listitem><listitem>S &amp;rarr; aS</listitem><listitem>S &amp;rarr; bS</listitem></list><paragraph>The terminals here are<space/><italics>a</italics><space/>and<space/><italics>b</italics>, while the only non-terminal is S.The language described is all nonempty strings of<space/><extension extension_name='math'>a</extension>s and<space/><extension extension_name='math'>b</extension>s that end in<space/><extension extension_name='math'>a</extension>.</paragraph><paragraph>This grammar is<space/><link><target>regular grammar</target><part>regular</part></link>: no rule has more than one nonterminal in its right-hand side, and each of these nonterminals is at the same end of the right-hand side.</paragraph><paragraph>Every regular grammar corresponds directly to a<space/><link><target>nondeterministic finite automaton</target></link>, so we know that this is a<space/><link><target>regular language</target></link>.</paragraph><paragraph>Using pipe symbols, the grammar above can be described more tersely as follows:</paragraph><list type='ident'><listitem>S &amp;rarr; a | aS | bS</listitem></list><heading level='3'>Matching pairs</heading><paragraph>In a context-free grammar, we can pair up characters the way we do with<space/><link><target>bracket</target><trail>s</trail></link>. The simplest example:</paragraph><list type='ident'><listitem>S &amp;rarr; aSb</listitem><listitem>S &amp;rarr; ab</listitem></list><paragraph>This grammar generates the language<space/><extension extension_name='math'><space/>\{ a^n b^n : n \ge 1 \}<space/></extension>, which is not<space/><link><target>regular language</target><part>regular</part></link><space/>(according to the<space/><link><target>pumping lemma for regular languages</target></link>).</paragraph><paragraph>The special character stands for the empty string. By changing the above grammar to</paragraph><list type='ident'><listitem>S &amp;rarr; aSb |<space/></listitem></list><paragraph>we obtain a grammar generating the language<space/><extension extension_name='math'><space/>\{ a^n b^n : n \ge 0 \}<space/></extension><space/>instead. This differs only in that it contains the empty string while the original grammar did not.</paragraph><heading level='3'>Algebraic expressions</heading><paragraph>Here is a context-free grammar for syntactically correct<space/><link><target>Infix notation</target><part>infix</part></link><space/>algebraic expressions in the variables x, y and z:</paragraph><list type='numbered'><listitem>S x</listitem><listitem>S y</listitem><listitem>S z</listitem><listitem>S S + S</listitem><listitem>S S - S</listitem><listitem>S S * S</listitem><listitem>S S / S</listitem><listitem>S ( S )</listitem></list><paragraph>This grammar can, for example, generate the string</paragraph><list type='ident'><listitem>( x + y ) * x - z * y / ( x + x )</listitem></list><paragraph>as follows:</paragraph><list type='ident'><listitem>S (the start symbol)</listitem><listitem>&amp;rarr; S - S (by rule 5)</listitem><listitem>&amp;rarr; S * S - S (by rule 6, applied to the leftmost S)</listitem><listitem>&amp;rarr; S * S - S / S (by rule 7, applied to the rightmost S)</listitem><listitem>&amp;rarr; ( S ) * S - S / S (by rule 8, applied to the leftmost S)</listitem><listitem>&amp;rarr; ( S ) * S - S / ( S ) (by rule 8, applied to the rightmost S)</listitem><listitem>&amp;rarr; ( S + S ) * S - S / ( S ) (etc.)</listitem><listitem>&amp;rarr; ( S + S ) * S - S * S / ( S )</listitem><listitem>&amp;rarr; ( S + S ) * S - S * S / ( S + S )</listitem><listitem>&amp;rarr; ( x + S ) * S - S * S / ( S + S )</listitem><listitem>&amp;rarr; ( x + y ) * S - S * S / ( S + S )</listitem><listitem>&amp;rarr; ( x + y ) * x - S * y / ( S + S )</listitem><listitem>&amp;rarr; ( x + y ) * x - S * y / ( x + S )</listitem><listitem>&amp;rarr; ( x + y ) * x - z * y / ( x + S )</listitem><listitem>&amp;rarr; ( x + y ) * x - z * y / ( x + x )</listitem></list><paragraph>Note that many choices were made underway as to which rewrite was going to be performed next.These choices look quite arbitrary. As a matter of fact, they are, in the sense that the string finally generated is always the same. For example, the second and third rewrites</paragraph><list type='ident'><listitem>&amp;rarr; S * S - S (by rule 6, applied to the leftmost S)</listitem><listitem>&amp;rarr; S * S - S / S (by rule 7, applied to the rightmost S)</listitem></list><paragraph>could be done in the opposite order:</paragraph><list type='ident'><listitem>&amp;rarr; S - S / S (by rule 7, applied to the rightmost S)</listitem><listitem>&amp;rarr; S * S - S / S (by rule 6, applied to the leftmost S)</listitem></list><paragraph>Also, many choices were made on which rule to apply to each selected<space/><xhtml:code>S</xhtml:code>.Changing the choices made and not only the order they were made in usually affects which terminal string comes out at the end.</paragraph><paragraph>Let's look at this in more detail. Consider the<space/><link><target>parse tree</target></link><space/>of this derivation:</paragraph><preblock><preline></preline><preline><space/><space/><space/><space/><space/><space/><space/><space/><space/><space/><space/>S</preline><preline><space/><space/><space/><space/><space/><space/><space/><space/><space/><space/><space/>|</preline><preline><space/><space/><space/><space/><space/><space/><space/><space/><space/><space/>/|\</preline><preline><space/><space/><space/><space/><space/><space/><space/><space/><space/>S - S</preline><preline><space/><space/><space/><space/><space/><space/><space/><space/>/ \</preline><preline><space/><space/><space/><space/><space/><space/><space/>/|\ /|\</preline><preline><space/><space/><space/><space/><space/><space/>S * S S / S</preline><preline><space/><space/><space/><space/><space/>/ | | \</preline><preline><space/><space/><space/><space/>/|\ x /|\ /|\</preline><preline><space/><space/><space/>( S ) S * S ( S )</preline><preline><space/><space/><space/><space/>/ | | \<space/></preline><preline><space/><space/><space/>/|\ z y /|\</preline><preline><space/><space/>S + S S + S</preline><preline><space/><space/>| | | |</preline><preline><space/><space/>x y x x</preline></preblock><paragraph>Starting at the top, step by step, an S in the tree is expanded, until no more unexpanded<space/><xhtml:code>S</xhtml:code>es (non-terminals) remain.Picking a different order of expansion will produce a different derivation, but the same parse tree.The parse tree will only change if we pick a different rule to apply at some position in the tree.</paragraph><paragraph>But can a different parse tree still produce the same terminal string,which is<space/><xhtml:code>( x + y ) * x - z * y / ( x + x )</xhtml:code><space/>in this case?Yes, for this particular grammar, this is possible.Grammars with this property are called<space/><link><target>ambiguous grammar</target><part>ambiguous</part></link>.</paragraph><paragraph>For example,<space/><xhtml:code>x + y * z</xhtml:code><space/>can be produced with these two different parse trees:</paragraph><preblock><preline></preline><preline><space/><space/><space/><space/><space/><space/><space/><space/><space/>S S</preline><preline><space/><space/><space/><space/><space/><space/><space/><space/><space/>| |</preline><preline><space/><space/><space/><space/><space/><space/><space/><space/>/|\ /|\</preline><preline><space/><space/><space/><space/><space/><space/><space/>S * S S + S<space/></preline><preline><space/><space/><space/><space/><space/><space/>/ \ / \</preline><preline><space/><space/><space/><space/><space/>/|\ z x /|\</preline><preline><space/><space/><space/><space/>S + S S * S<space/></preline><preline><space/><space/><space/><space/>| | | |</preline><preline><space/><space/><space/><space/>x y y z</preline></preblock><paragraph>However, the<space/><italics>language</italics><space/>described by this grammar is not inherently ambiguous:an alternative, unambiguous grammar can be given for the language, for example:</paragraph><list type='ident'><listitem>T &amp;rarr; x</listitem><listitem>T &amp;rarr; y</listitem><listitem>T &amp;rarr; z</listitem><listitem>S &amp;rarr; S + T</listitem><listitem>S &amp;rarr; S - T</listitem><listitem>S &amp;rarr; S * T</listitem><listitem>S &amp;rarr; S / T</listitem><listitem>T &amp;rarr; ( S )</listitem><listitem>S &amp;rarr; T</listitem></list><paragraph>(once again picking<space/><xhtml:code>S</xhtml:code><space/>as the start symbol). This alternative grammar will produce<space/><xhtml:code>x + y * z</xhtml:code><space/>with a parse tree similar to the left one above, i.e. implicitly assuming the association<space/><xhtml:code>(x + y) * z</xhtml:code>, which is not according to standard operator precedence. More elaborate, unambiguous and context-free grammars can be constructed that produce parse trees that obey all desired<space/><link><target>Operator-precedence parser</target><part>operator precedence</part></link><space/>and associativity rules.</paragraph><heading level='3'>Further examples</heading><heading level='4'>Example 1</heading><paragraph>A context-free grammar for the language consisting of all strings over {a,b} containing an unequal number of a's and b's:</paragraph><list type='ident'><listitem>S &amp;rarr; U | V</listitem><listitem>U &amp;rarr; TaU | TaT | UaT</listitem><listitem>V &amp;rarr; TbV | TbT | VbT</listitem><listitem>T &amp;rarr; aTbT | bTaT | &amp;epsilon;<space/></listitem></list><paragraph>Here, the nonterminal T can generate all strings with the same number of a's as b's, the nonterminal U generates all strings with more a's than b's and the nonterminal V generates all strings with fewer a's than b's. Omitting the third alternative in the rule for U and V doesn't restrict the grammar's language.</paragraph><heading level='4'>Example 2</heading><paragraph>Another example of a non-regular language is<space/><extension extension_name='math'><space/>\{ b^n a^m b^{2n} : n \ge 0, m \ge 0 \}<space/></extension>. It is context-free as it can be generated by the following context-free grammar:</paragraph><list type='ident'><listitem>S &amp;rarr; bSbb | A</listitem><listitem>A &amp;rarr; aA | &amp;epsilon;</listitem></list><heading level='4'>Other examples</heading><paragraph>The<space/><link><target>First-order logic#Formation rules</target><part>formation rules</part></link><space/>for the terms and formulas of formal logic fit the definition of context-free grammar, except that the set of symbols may be infinite and there may be more than one start symbol.</paragraph><heading level='3'>Derivations and syntax trees</heading><paragraph>A<space/><italics>derivation</italics><space/>of a string for a grammar is a sequence of grammar rule applications that transforms the start symbol into the string.A derivation proves that the string belongs to the grammar's language.</paragraph><paragraph>A derivation is fully determined by giving, for each step:</paragraph><list type='bullet'><listitem>the rule applied in that step</listitem><listitem>the occurrence of its left hand side to which it is applied</listitem></list><paragraph>For clarity, the intermediate string is usually given as well.</paragraph><paragraph>For instance, with the grammar:</paragraph><preblock><preline><space/>(1) S S + S</preline><preline><space/>(2) S 1</preline><preline><space/>(3) S a</preline></preblock><paragraph>the string</paragraph><preblock><preline>1 + 1 + a</preline></preblock><paragraph>can be derived with the derivation:</paragraph><preblock><preline>S</preline><preline><space/><space/><space/><space/><space/>(rule 1 on first S)</preline><preline><space/>S+S</preline><preline><space/><space/><space/><space/><space/>(rule 1 on second S)</preline><preline><space/>S+S+S</preline><preline><space/><space/><space/><space/><space/>(rule 2 on second S)</preline><preline><space/>S+1+S</preline><preline><space/><space/><space/><space/><space/>(rule 3 on third S)</preline><preline><space/>S+1+a</preline><preline><space/><space/><space/><space/><space/>(rule 2 on first S)</preline><preline><space/>1+1+a</preline></preblock><paragraph>Often, a strategy is followed that deterministically determines the next nonterminal to rewrite:</paragraph><list type='bullet'><listitem>in a<space/><italics>leftmost derivation</italics>, it is always the leftmost nonterminal;</listitem><listitem>in a<space/><italics>rightmost derivation</italics>, it is always the rightmost nonterminal.</listitem></list><paragraph>Given such a strategy, a derivation is completely determined by the sequence of rules applied. For instance, the leftmost derivation</paragraph><preblock><preline>S</preline><preline><space/><space/><space/><space/><space/>(rule 1 on first S)</preline><preline><space/>S+S</preline><preline><space/><space/><space/><space/><space/>(rule 2 on first S)</preline><preline><space/>1+S</preline><preline><space/><space/><space/><space/><space/>(rule 1 on first S)</preline><preline><space/>1+S+S</preline><preline><space/><space/><space/><space/><space/>(rule 2 on first S)</preline><preline><space/>1+1+S</preline><preline><space/><space/><space/><space/><space/>(rule 3 on first S)</preline><preline><space/>1+1+a</preline></preblock><paragraph>can be summarized as</paragraph><preblock><preline>rule 1, rule 2, rule 1, rule 2, rule 3</preline></preblock><paragraph>The distinction between leftmost derivation and rightmost derivation is important because in most<space/><link><target>parsing</target><part>parser</part><trail>s</trail></link><space/>the transformation of the input is defined by giving a piece of code for every grammar rule that is executed whenever the rule is applied. Therefore it is important to know whether the parser determines a leftmost or a rightmost derivation because this determines the order in which the pieces of code will be executed. See for an example<space/><link><target>LL parser</target><trail>s</trail></link><space/>and<space/><link><target>LR parser</target><trail>s</trail></link>.</paragraph><paragraph>A derivation also imposes in some sense a hierarchical structure on the string that is derived. For example, if the string &quot;1 + 1 + a&quot; is derived according to the leftmost derivation:</paragraph><list type='ident'><listitem>S &amp;rarr; S + S (1)</listitem><listitem>&amp;nbsp;&amp;nbsp; &amp;rarr; 1 + S (2)</listitem><listitem>&amp;nbsp;&amp;nbsp; &amp;rarr; 1 + S + S (1)</listitem><listitem>&amp;nbsp;&amp;nbsp; &amp;rarr; 1 + 1 + S (2)</listitem><listitem>&amp;nbsp;&amp;nbsp; &amp;rarr; 1 + 1 + a (3)</listitem></list><paragraph>the structure of the string would be:</paragraph><list type='ident'><listitem>{ { 1 }<xhtml:sub>S</xhtml:sub><space/>+ { { 1 }<xhtml:sub>S</xhtml:sub><space/>+ { a }<xhtml:sub>S</xhtml:sub><space/>}<xhtml:sub>S</xhtml:sub><space/>}<xhtml:sub>S</xhtml:sub></listitem></list><paragraph>where { ... }<xhtml:sub>S</xhtml:sub><space/>indicates a substring recognized as belonging to S. This hierarchy can also be seen as a tree:</paragraph><preblock><preline><space/><space/><space/><space/><space/><space/><space/><space/><space/><space/><space/>S</preline><preline><space/><space/><space/><space/><space/><space/><space/><space/><space/><space/>/|\</preline><preline><space/><space/><space/><space/><space/><space/><space/><space/><space/>/ | \</preline><preline><space/><space/><space/><space/><space/><space/><space/><space/>/ | \</preline><preline><space/><space/><space/><space/><space/><space/><space/>S '+' S</preline><preline><space/><space/><space/><space/><space/><space/><space/>| /|\</preline><preline><space/><space/><space/><space/><space/><space/><space/>| / | \</preline><preline><space/><space/><space/><space/><space/><space/>'1' S '+' S</preline><preline><space/><space/><space/><space/><space/><space/><space/><space/><space/><space/><space/><space/>| |</preline><preline><space/><space/><space/><space/><space/><space/><space/><space/><space/><space/><space/>'1' 'a'</preline></preblock><paragraph>This tree is called a<space/><italics><link><target>parse tree</target></link></italics><space/>or &quot;concrete syntax tree&quot; of the string, by contrast with the<space/><link><target>abstract syntax tree</target></link>. In this case the presented leftmost and the rightmost derivations define the same parse tree; however, there is another (rightmost) derivation of the same string</paragraph><list type='ident'><listitem>S &amp;rarr; S + S (1)</listitem><listitem>&amp;nbsp;&amp;nbsp; &amp;rarr; S + a (3)</listitem><listitem>&amp;nbsp;&amp;nbsp; &amp;rarr; S + S + a (1)</listitem><listitem>&amp;nbsp;&amp;nbsp; &amp;rarr; S + 1 + a (2)</listitem><listitem>&amp;nbsp;&amp;nbsp; &amp;rarr; 1 + 1 + a (2)</listitem></list><paragraph>and this defines the following parse tree:</paragraph><preblock><preline><space/><space/><space/><space/><space/><space/><space/><space/><space/><space/><space/>S<space/></preline><preline><space/><space/><space/><space/><space/><space/><space/><space/><space/><space/>/|\</preline><preline><space/><space/><space/><space/><space/><space/><space/><space/><space/>/ | \</preline><preline><space/><space/><space/><space/><space/><space/><space/><space/>/ | \</preline><preline><space/><space/><space/><space/><space/><space/><space/>S '+' S</preline><preline><space/><space/><space/><space/><space/><space/>/|\ |</preline><preline><space/><space/><space/><space/><space/>/ | \ |</preline><preline><space/><space/><space/><space/>S '+' S 'a'</preline><preline><space/><space/><space/><space/>| |</preline><preline><space/><space/><space/>'1' '1'</preline></preblock><paragraph>If, for certain strings in the language of the grammar, there is more than one parsing tree, then the grammar is said to be an<space/><italics><link><target>ambiguous grammar</target></link></italics>. Such grammars are usually hard to parse because the parser cannot always decide which grammar rule it has to apply. Usually, ambiguity is a feature of the grammar, not the language, and an unambiguous grammar can be found that generates the same context-free language. However, there are certain languages that can only be generated by ambiguous grammars; such languages are called<space/><italics><link><target>inherently ambiguous language</target><trail>s</trail></link></italics>.</paragraph><heading level='2'>Normal forms</heading><paragraph>Every context-free grammar that does not generate the empty string can be transformed into one in which there is no -production (that is, a rule that has the empty string as a product). If a grammar does generate the empty string, it will be necessary to include the rule<space/><extension extension_name='math'>S \rarr \epsilon</extension>, but there need be no other -rule. Every context-free grammar with no -production has an equivalent grammar in<space/><link><target>Chomsky normal form</target></link><space/>or<space/><link><target>Greibach normal form</target></link>. &quot;Equivalent&quot; here means that the two grammars generate the same language.</paragraph><paragraph>The especially simple form of production rules in Chomsky Normal Form grammars has both theoretical and practical implications. For instance, given a context-free grammar, one can use the Chomsky Normal Form to construct a<space/><link><target>polynomial-time</target></link><space/>algorithm that decides whether a given string is in the language represented by that grammar or not (the<space/><link><target>CYK algorithm</target></link>).</paragraph><heading level='2'>Closure properties</heading><paragraph>Context-free languages are<space/><link><target>Closure (mathematics)</target><part>closed</part></link><space/>under<space/><link><target>set union</target><part>union</part></link>,<space/><link><target>string concatenation#Concatenation of sets of strings</target><part>concatenation</part></link>,<space/><link><target>Kleene star</target></link>,<extension extension_name='ref'>Hopcroft &amp; Ullman (1979), p.131, Theorem 6.1</extension><link><target>string substitution</target><part>substitution</part></link><space/>(in particular<space/><link><target>string homomorphism</target><part>homomorphism</part></link>),<extension extension_name='ref'>Hopcroft &amp; Ullman (1979), p.131-132, Theorem 6.2</extension><space/><link><target>string homomorphism</target><part>inverse homomorphism</part></link>,<extension extension_name='ref'>Hopcroft &amp; Ullman (1979), p.132-134, Theorem 6.3</extension>and<space/><link><target>set intersection</target><part>intersection</part></link><space/>with a<space/><link><target>regular language</target></link>.<extension extension_name='ref'>Hopcroft &amp; Ullman (1979), p.135-136, Theorem 6.5</extension>They are not closed under general intersection (hence neither under<space/><link><target>set complement</target><part>complementation</part></link>) and set difference.<extension extension_name='ref'>Hopcroft &amp; Ullman (1979), p.134-135, Theorem 6.4</extension></paragraph><heading level='2'>Decidable problems</heading><paragraph>There are algorithms to decide whether a context-free language is empty, and whether it is finite.<extension extension_name='ref'>Hopcroft &amp; Ullman (1979), p.137-138, Theorem 6.6</extension></paragraph><heading level='2'>Undecidable problems</heading><paragraph>Some questions that are undecidable for wider classes of grammars become decidable for context-free grammars; e.g. the emptiness problem (whether the grammar generates any terminal strings at all), is undecidable for<space/><link><target>context-sensitive grammar</target><trail>s</trail></link>, but decidable for context-free grammars.</paragraph><paragraph>However, many problems are<space/><link><target>Undecidable problem</target><part>undecidable</part></link><space/>even for context-free grammars. Examples are:</paragraph><heading level='3'>Universality</heading><paragraph>Given a CFG, does it generate the language of all strings over the alphabet of terminal symbols used in its rules?<extension extension_name='ref' name="sipser"><template><target>harvtxt</target><arg>Sipser</arg><arg>1997</arg></template>, Theorem 5.10, p. 181.</extension><extension extension_name='ref' name="hu281"></extension></paragraph><paragraph>A reduction can be demonstrated to this problem from the well-known undecidable problem of determining whether a<space/><link><target>Turing machine</target></link><space/>accepts a particular input (the<space/><link><target>halting problem</target></link>). The reduction uses the concept of a<space/><italics><link><target>computation history</target></link></italics>, a string describing an entire computation of a<space/><link><target>Turing machine</target></link>. A CFG can be constructed that generates all strings that are not accepting computation histories for a particular Turing machine on a particular input, and thus it will accept all strings only if the machine doesn't accept that input.</paragraph><heading level='3'>Language equality</heading><paragraph>Given two CFGs, do they generate the same language?<extension extension_name='ref' name="hu281"><template><target>harvtxt</target><arg>Hopcroft</arg><arg>Ullman</arg><arg>1979</arg></template>, p. 281.</extension><extension extension_name='ref' name="eom"></extension></paragraph><paragraph>The undecidability of this problem is a direct consequence of the previous: it is impossible to even decide whether a CFG is equivalent to the trivial CFG defining the language of all strings.</paragraph><heading level='3'>Language inclusion</heading><paragraph>Given two CFGs, can the first one generate all strings that the second one can generate?<extension extension_name='ref' name="hu281"></extension><extension extension_name='ref' name="eom"></extension></paragraph><paragraph>If this problem was decidable, then language equality could be decided too: two CFGs G1 and G2 generate the same language if L(G1) is a subset of L(G2) and L(G2) is a subset of L(G1).</paragraph><heading level='3'>Being in a lower or higher level of the Chomsky hierarchy</heading><paragraph>Using<space/><link><target>Greibach's theorem</target></link>, it can be shown that the two following problems are undecidable:</paragraph><list type='bullet'><listitem>Given a<space/><link><target>context-sensitive grammar</target></link>, does it describe a context-free language?</listitem><listitem>Given a context-free grammar, does it describe a<space/><link><target>regular language</target></link>?<extension extension_name='ref' name="hu281"></extension><extension extension_name='ref' name="eom"><template><target>citation</target><arg name="title">Encyclopaedia of mathematics: an updated and annotated translation of the Soviet "Mathematical Encyclopaedia"</arg><arg name="first">Michiel</arg><arg name="last">Hazewinkel</arg><arg name="publisher">Springer</arg><arg name="year">1994</arg><arg name="isbn">978-1-55608-003-6</arg><arg name="at">Vol. IV, p. 56</arg><arg name="url">http://books.google.com/books?id</arg></template>.</extension></listitem></list><heading level='3'>Grammar ambiguity</heading><paragraph>Given a CFG, is it<space/><link><target>Ambiguous grammar</target><part>ambiguous</part></link>?</paragraph><paragraph>The undecidability of this problem follows from the fact that if an algorithm to determine ambiguity existed, the<space/><link><target>Post correspondence problem</target></link><space/>could be decided, which is known to be undecidable.</paragraph><heading level='3'>Language disjointness</heading><paragraph>Given two CFGs, is there any string derivable from both grammars?</paragraph><paragraph>If this problem was decidable, the undecidable<space/><link><target>Post correspondence problem</target></link><space/>could be decided, too: given strings<space/><extension extension_name='math'>\alpha_1, \ldots, \alpha_N, \beta_1, \ldots, \beta_N</extension><space/>over some alphabet<space/><extension extension_name='math'>\{a_1, \ldots, a_k\}</extension>, let the grammar G1 consist of the rule</paragraph><list type='ident'><listitem>S &amp;rarr;<space/><extension extension_name='math'>\alpha_1</extension><space/>S<space/><extension extension_name='math'>\beta_1^{rev}</extension><space/>| ... |<space/><extension extension_name='math'>\alpha_N</extension><space/>S<space/><extension extension_name='math'>\beta_N^{rev}</extension><space/>|<space/><extension extension_name='math'>b</extension>;</listitem></list><paragraph>where<space/><extension extension_name='math'>\beta_i^{rev}</extension><space/>denotes the<space/><link><target>String (computer science)#Reversal</target><part>reversed</part></link><space/>string<space/><extension extension_name='math'>\beta_i</extension><space/>and<space/><extension extension_name='math'>b</extension><space/>doesn't occur among the<space/><extension extension_name='math'>a_i</extension>; and let grammar G2 consist of the rule</paragraph><list type='ident'><listitem>T &amp;rarr;<space/><extension extension_name='math'>a_1</extension><space/>T<space/><extension extension_name='math'>a_1</extension><space/>| ... |<space/><extension extension_name='math'>a_k</extension><space/>T<space/><extension extension_name='math'>a_k</extension><space/>|<space/><extension extension_name='math'>b</extension>;</listitem></list><paragraph>Then the Post problem given by<space/><extension extension_name='math'>\alpha_1, \ldots, \alpha_N, \beta_1, \ldots, \beta_N</extension><space/>has a solution if and only if L(G1) and L(G2) share a derivable string.</paragraph><heading level='2'>Extensions</heading><paragraph>An obvious way to extend the context-free grammar formalism is to allow nonterminals to have arguments, the values of which are passed along within the rules. This allows natural language features such as<space/><link><target>Agreement (linguistics)</target><part>agreement</part></link><space/>and<space/><link><target>reference</target></link>, and programming language analogs such as the correct use and definition of identifiers, to be expressed in a natural way. E.g. we can now easily express that in English sentences, the subject and verb must agree in number. In computer science, examples of this approach include<space/><link><target>affix grammar</target><trail>s</trail></link>,<space/><link><target>attribute grammar</target><trail>s</trail></link>,<space/><link><target>indexed grammar</target><trail>s</trail></link>, and Van Wijngaarden<space/><link><target>two-level grammar</target><trail>s</trail></link>. Similar extensions exist in linguistics.</paragraph><paragraph>An<space/><bold>extended context-free grammar</bold><space/>(or<space/><bold>regular right part grammar</bold>) is one in which the right-hand side of the production rules is allowed to be a<space/><link><target>regular expression</target></link><space/>over the grammar's terminals and nonterminals. Extended context-free grammars describe exactly the context-free languages.<extension extension_name='ref'><template><target>cite web</target><arg name="url">http://www.engr.mun.ca/~theo/Courses/fm/pub/context-free.pdf<space/></arg><arg name="title">A Short Introduction to Regular Expressions and Context-Free Grammars<space/></arg><arg name="accessdate">August 24, 2012<space/></arg><arg name="author">Norvell, Theodore<space/></arg><arg name="pages">4</arg></template></extension></paragraph><paragraph>Another extension is to allow additional terminal symbols to appear at the left hand side of rules, constraining their application. This produces the formalism of<space/><link><target>context-sensitive grammar</target><trail>s</trail></link>.</paragraph><heading level='2'>Subclasses</heading><paragraph>There are a number of important subclasses of the context-free grammars:</paragraph><list type='bullet'><listitem><link><target>LR parser</target><part>LR(''k'')</part></link><space/>grammars (also known as<space/><link><target>deterministic context-free grammar</target><trail>s</trail></link>) allow<space/><link><target>parsing</target></link><space/>(string recognition) with<space/><link><target>deterministic pushdown automaton</target><part>deterministic pushdown automata</part></link><space/>(PDA), but they can only describe<space/><link><target>deterministic context-free language</target><trail>s</trail></link>.</listitem></list><list type='bullet'><listitem><link><target>SLR grammar</target><part>Simple LR</part></link>,<space/><link><target>LALR parser</target><part>Look-Ahead LR</part></link><space/>grammars are subclasses that allow further simplification of parsing. SLR and LALR are recognized using the same PDA as LR, but with simpler tables, in most cases.</listitem></list><list type='bullet'><listitem><link><target>LL parser</target><part>LL(''k'') and LL(''*'')</part></link><space/>grammars allow parsing by direct construction of a leftmost derivation as described above, and describe even fewer languages.</listitem></list><list type='bullet'><listitem><link><target>Simple grammar</target><trail>s</trail></link><space/>are a subclass of the LL(1) grammars mostly interesting for its theoretical property that language equality of simple grammars is decidable, while language inclusion is not.</listitem></list><list type='bullet'><listitem><link><target>Bracketed grammar</target><trail>s</trail></link><space/>have the property that the terminal symbols are divided into left and right bracket pairs that always match up in rules.</listitem></list><list type='bullet'><listitem><link><target>Linear grammar</target><trail>s</trail></link><space/>have no rules with more than one nonterminal in the right hand side.</listitem></list><list type='bullet'><listitem><link><target>Regular grammar</target><trail>s</trail></link><space/>are a subclass of the linear grammars and describe the<space/><link><target>regular language</target><part>regular</part></link><space/>languages, i.e. they correspond to<space/><link><target>finite automaton</target><part>finite automata</part></link><space/>and<space/><link><target>regular expression</target><trail>s</trail></link>.</listitem></list><paragraph>LR parsing extends LL parsing to support a larger range of grammars; in turn,<space/><link><target>GLR parser</target><part>generalized LR parsing</part></link><space/>extends LR parsing to support arbitrary context-free grammars. On LL grammars and LR grammars, it essentially performs LL parsing and LR parsing, respectively, while on<space/><link><target>nondeterministic grammar</target><trail>s</trail></link>, it is as efficient as can be expected. Although GLR parsing was developed in the 1980s, many new language definitions and<space/><link><target>parser generator</target><trail>s</trail></link><space/>continue to be based on LL, LALR or LR parsing up to the present day.</paragraph><heading level='2'>Linguistic applications</heading><paragraph><link><target>Noam Chomsky</target><part>Chomsky</part></link><space/>initially hoped to overcome the limitations of context-free grammars by adding<space/><link><target>transformational grammar</target><part>transformation rules</part></link>.<extension extension_name='ref' name="chomsky1956"></extension></paragraph><paragraph>Such rules are another standard device in traditional linguistics; e.g.<space/><link><target>grammatical voice</target><part>passivization</part></link><space/>in English. Much of<space/><link><target>generative grammar</target></link><space/>has been devoted to finding ways of refining the descriptive mechanisms of phrase-structure grammar and transformation rules such that exactly the kinds of things can be expressed that natural language actually allows. Allowing arbitrary transformations doesn't meet that goal: they are much too powerful, being<space/><link><target>Turing complete</target></link><space/>unless significant restrictions are added (e.g. no transformations that introduce and then rewrite symbols in a context-free fashion).</paragraph><paragraph>Chomsky's general position regarding the non-context-freeness of natural language has held up since then,<extension extension_name='ref' name="shieber1985"><template><target>citation</target><arg name="title">Evidence against the context-freeness of natural language<space/></arg><arg name="year">1985<space/></arg><arg name="last">Shieber<space/></arg><arg name="first">Stuart<space/></arg><arg name="journal">Linguistics and Philosophy<space/></arg><arg name="volume">8<space/></arg><arg name="pages">333â€“343<space/></arg><arg name="url">http://www.eecs.harvard.edu/~shieber/Biblio/Papers/shieber85.pdf<space/></arg><arg name="doi">10.1007/BF00630917<space/></arg><arg name="issue">3</arg></template>.</extension><space/>although his specific examples regarding the inadequacy of context-free grammars in terms of their weak generative capacity were later disproved.<extension extension_name='ref' name="pullum-gazdar1982"><template><target>citation</target><arg name="title">Natural languages and context-free languages<space/></arg><arg name="year">1982<space/></arg><arg name="last">Pullum<space/></arg><arg name="first">Geoffrey K.<space/></arg><arg name="coauthors">Gerald Gazdar<space/></arg><arg name="journal">Linguistics and Philosophy<space/></arg><arg name="volume">4<space/></arg><arg name="pages">471â€“504<space/></arg><arg name="doi">10.1007/BF00360802<space/></arg><arg name="issue">4</arg></template>.</extension><link><target>Gerald Gazdar</target></link><space/>and<space/><link><target>Geoffrey Pullum</target></link><space/>have argued that despite a few non-context-free constructions in natural language (such as<space/><link><target>cross-serial dependencies</target></link><space/>in<space/><link><target>Swiss German</target></link><extension extension_name='ref' name="shieber1985"></extension><space/>and<space/><link><target>reduplication</target></link><space/>in<space/><link><target>Bambara language</target><part>Bambara</part></link><extension extension_name='ref' name="culy1985"><template><target>citation</target><arg name="title">The Complexity of the Vocabulary of Bambara<space/></arg><arg name="year">1985<space/></arg><arg name="last">Culy<space/></arg><arg name="first">Christopher<space/></arg><arg name="journal">Linguistics and Philosophy<space/></arg><arg name="volume">8<space/></arg><arg name="pages">345â€“351<space/></arg><arg name="doi">10.1007/BF00630918<space/></arg><arg name="issue">3</arg></template>.</extension>), the vast majority of forms in natural language are indeed context-free.<extension extension_name='ref' name="pullum-gazdar1982"></extension></paragraph><heading level='2'>See also</heading><list type='bullet'><listitem><link><target>Parsing expression grammar</target></link></listitem><listitem><link><target>Stochastic context-free grammar</target></link></listitem><listitem><link><target>Context-free grammar generation algorithms</target><part>Algorithms for context-free grammar generation</part></link></listitem><listitem><link><target>Pumping lemma for context-free languages</target></link></listitem></list><heading level='3'>Parsing algorithms</heading><list type='bullet'><listitem><link><target>CYK algorithm</target></link></listitem><listitem><link><target>GLR parser</target></link></listitem><listitem><link><target>LL parser</target></link></listitem><listitem><link><target>Earley algorithm</target></link></listitem></list><heading level='2'>Notes</heading><paragraph><template><target>Reflist</target><arg name="colwidth">30em</arg></template></paragraph><heading level='2'>References</heading><list type='bullet'><listitem><template><target>citation</target><arg name="first1">John E.</arg><arg name="last1">Hopcroft</arg><arg name="author1-link">John Hopcroft</arg><arg name="first2">Jeffrey D.</arg><arg name="last2">Ullman</arg><arg name="author2-link">Jeffrey Ullman</arg><arg name="title">Introduction to Automata Theory, Languages, and Computation</arg><arg name="publisher">Addison-Wesley</arg><arg name="year">1979</arg></template>. Chapter 4: Context-Free Grammars, pp.&amp;nbsp;77106; Chapter 6: Properties of Context-Free Languages, pp.&amp;nbsp;125137.</listitem><listitem><template><target>citation</target><arg name="authorlink"><space/>Michael Sipser<space/></arg><arg name="first"><space/>Michael<space/></arg><arg name="last"><space/>Sipser<space/></arg><arg name="year"><space/>1997<space/></arg><arg name="title"><space/>Introduction to the Theory of Computation<space/></arg><arg name="publisher"><space/>PWS Publishing<space/></arg><arg name="isbn"><space/>0-534-94728-X</arg></template>. Chapter 2: Context-Free Grammars, pp.&amp;nbsp;91122; Section 4.1.2: Decidable problems concerning context-free languages, pp.&amp;nbsp;156159; Section 5.1.1: Reductions via computation histories: pp.&amp;nbsp;176183.</listitem><listitem><template><target>cite book</target><arg name="author">J. Berstel, L. Boasson</arg><arg name="title">Context-Free Languages</arg><arg name="year">1990</arg><arg name="volume">B</arg><arg name="pages">59â€“102</arg><arg name="publisher">Elsevier</arg><arg name="editor">Jan van Leeuwen</arg><arg name="series">Handbook of Theoretical Computer Science</arg></template></listitem></list><paragraph><template><target>Formal languages and grammars</target></template></paragraph><paragraph><template><target>DEFAULTSORT:Context-Free Grammar</target></template><link><target>Category:1956 in computer science</target></link><link><target>Category:Compiler construction</target></link><link><target>Category:Formal languages</target></link><link><target>Category:Programming language topics</target></link><link><target>Category:Wikipedia articles with ASCII art</target></link></paragraph></article>